[
  {
    "objectID": "gretl/docs/user/publisher.html",
    "href": "gretl/docs/user/publisher.html",
    "title": "Publisher",
    "section": "",
    "text": "GRETL-Task, welcher für Vektordaten die aktuellen Geodaten-Dateien bereitstellt und das Archiv der vorherigen Zeitstände pflegt.\n\n\nIn den heute vorliegenden Publikationsjobs werden häufig die Daten vom relational aufgebauten Edit-Schema mittels SQL-Queries “flachgewalzt” und ins Pub-Schema kopiert. Die Schema-Struktur wird automatisch mittels ili2pg aus Edit- und Pub-Modell generiert.\nFür den Datenbezug wird das build.gradle mit zwei Aufrufen des Publisher-Task ergänzt:\ndefaultTasks 'pubProduct'\n\ntasks.register('pubEdit', Publisher) {\n    dataIdent = \"ch.so.avt.verkehrszaehlstellen.edit\"\n    ...\n}\n\ntasks.register('transferVZS', Db2Db) {\n    dependsOn 'pubEdit'\n    ...\n}\n\ntasks.register('pubProduct', Publisher) {\n    dependsOn 'transferDenkmal'\n    dataIdent = \"ch.so.avt.verkehrszaehlstellen\"\n    ...\n}\nBei Problemen mit der Datenqualität der Originaldaten schlägt der Task “pubEdit” fehl. Der Job bricht mit Fehler ab, bevor die Daten irgendwo landen.\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataIdent\nString\nIdentifikator der Daten z.B. “ch.so.agi.vermessung.edit”.\n nein\n\n\ndatabase\nConnector\nDatenbank mit Quelldaten z.B. [\"uri\",\"user\",\"password\"]. Alternative zu sourcePath.\n ja\n\n\ndataset\nString\nili2db-Datasetname der Quelldaten “dataset” (Das ili2db-Schema muss also immer mit --createBasketCol erstellt werden)\n ja\n\n\ndbSchema\nString\nSchema in der Datenbank z.B. \"av\"\n ja\n\n\nexportModels\nString\nDas Export-Modell, indem die Daten exportiert werden. Der Parameter wird nur bei der Ausdünnung benötigt. Als Export-Modelle sind Basis-Modelle zulässig.\n ja\n\n\ngrooming\nObject\nKonfiguration für die Ausdünnung z.B. \"grooming.json\". Ohne Angabe wird nicht aufgeräumt.\n ja\n\n\nisUserFormats\nBoolean\nBenutzerformat (Geopackage, Shapefile, Dxf) erstellen. Default: false\n ja\n\n\nkgdiService\nEndpoint\nEndpunkt des SIMI-Services für die Rückmeldung des Publikationsdatums und die Erstellung des Beipackzettels, z.B. [\"http://api.kgdi.ch/metadata\",\"user\",\"pwd\"]. Publisher ergänzt die URL fallabhängig mit /pubsignal respektive /doc.\n ja\n\n\nkgdiTokenService\nEndpoint\nEndpunkt des Authentifizierung-Services, z.B. [\"http://api.kgdi.ch/metadata\",\"user\",\"pwd\"]. Publisher ergänzt die URL mit /v2/oauth/token.\n ja\n\n\nmodeldir\nString\nDateipfade, die Modell-Dateien (ili-Dateien) enthalten. Mehrere Pfade können durch Semikolon ; getrennt werden. Es sind auch URLs von Modell-Repositories möglich. Default: %ITF_DIR;http://models.interlis.ch/. %ITF_DIR ist ein Platzhalter für das Verzeichnis mit der ITF-Datei.\n ja\n\n\nmodelsToPublish\nString\nINTERLIS-Modellnamen der Quelldaten in der DB (Nur für “einfache” Modelle, deren ili2db-Schema ohne --createBasketCol erstellt werden kann).\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\npublishedRegions\nListProperty&lt;String&gt;\nListe der effektiv publizierten Regionen.\n nein\n\n\nregion\nString\nMuster (Regular Expression) der Dateinamen oder Datasetnamen, falls die Publikation Regionen-weise erfolgt z.B. \"[0-9][0-9][0-9][0-9]\". Alternative zum Parameter regions,Bei Quelle “Datei” ist die Angabe einer “stellvertretenden” Transferdatei mittels “sourcePath” zwingend. Bsp.: Bei sourcePath file(\"/transferfiles/dummy.xtf\") werden alle im Ordner “transferfiles” enthaltenen Transferdateien mit dem Muster verglichen und bei “match” selektiert und verarbeitet.\n ja\n\n\nregions\nListProperty&lt;String&gt;\nListe der zu publizierenden Regionen (Dateinamen oder Datasetnamen), falls die Publikation Regionen-weise erfolgen soll. Alternative zum Parameter region.\n ja\n\n\nsourcePath\nObject\nQuelldatei z.B. file(\"/path/file.xtf\")\n ja\n\n\ntarget\nEndpoint\nZielverzeichnis z.B. [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ] oder einfach ein Pfad [file(\"/out\")]\n nein\n\n\nvalidationConfig\nObject\nKonfiguration für die Validierung (eine ilivalidator-config-Datei) z.B. “validationConfig.ini”.\n ja\n\n\nversion\nDate\nnull\n ja\n\n\n\n\n\n\nFalls die Daten bereits als XTF-/ITF-Datei vorliegen, muss die Quelldatei angegeben werden.\ntask publishFile(type: Publisher){\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    sourcePath = file(\"/path/file.xtf\")\n}\nDie Daten können alternativ zu SFTP in ein lokales Verzeichnis publiziert werden:\ntask publishFile(type: Publisher){\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [file(\"/out\")]  \n    sourcePath = file(\"/path/file.xtf\")\n}\n\n\n\nFalls die Daten in einer ili2db konformen PostgreSQL Datenbank vorliegen, muss die Datenbank angegeben werden und welche Daten (Parameter dataset, modelsToPublish, regions, region) aus der Datenbank exportiert werden sollen.\ntasks.register('publishFromDb', Publisher) {\n    dataIdent = \"ch.so.agi.vermessung\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    database = [\"uri\",\"user\",\"password\"]\n    dbSchema \"av\"\n    dataset = \"dataset\"\n}\nNur bei einfachen Modellen (falls das DB Schema ohne createBasketCol erstellt worden ist), kann der Export alternativ mit dem Parameter modelsToPublish mit Angabe des INTERLIS-Modellnamens erfolgen:\ntasks.register('publishFromDb', Publisher) {\n    dataIdent = \"ch.so.agi.vermessung\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    database = [\"uri\",\"user\",\"password\"]\n    dbSchema \"av\"\n    modelsToPublish = \"DM01AVCH24LV95D\"\n}\n\n\n\nFalls die Daten bereits als XTF-/ITF-Dateien vorliegen, muss zusätzlich zu einer möglichen Quelldatei (sourcePath) das Dateinamens-Muster (ohne Nameserweiterung (.xtf oder .itf) der Regionen (region) angegeben werden.\ntasks.register('publishFile', Publisher) {\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    sourcePath = file(\"/transferfiles/file.xtf\")\n    region = \"[0-9][0-9][0-9][0-9]\"  // regex; ersetzt den filename im sourcePath\n}\nDer sourcePath ist wie bei der Verarbeitung eines XTF (ohne Regionen) ein Datei- und nicht ein Ordner-Pfad. Mittels Parameter “region” werden aus allem im Ordner “transferfiles” enthaltenen Transfer-Dateien die zu verarbeitenden selektiert.\nFalls die Daten in einer ili2db konformen PostgreSQL Datenbank vorliegen, muss das Muster der Datensatz-Namen (dataset) angegeben werden (= ein Datensatz pro Region (region)).\ntasks.register('publishFromDb', Publisher) {\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    database = [\"uri\",\"user\",\"password\"]\n    dbSchema \"av\"\n    region = \"[0-9][0-9][0-9][0-9]\"  // regex; ersetzt das dataset\n}\nVerarbeitet werden alle Datensätze, deren Dateiname (Quelle Transferdatei) oder dataset Name (Quelle DB) auf das in Parameter “region” definierte Muster (regular Expression) zutreffen.\n\n\nEs können eindeutige Namen oder auch regular expressions verwendet werden.\nExport mit region aus lokaler DB (Nutzungsplanung mit 3 Datasets: 2580, 2581, 2582)\ntasks.register('publishFromDb2', Publisher) {\n    dataIdent = \"ch.so.arp.nutzungsplanung.publishFromDb2\"\n    database = [dbUriPub, dbUserPub, dbPwdPub]\n    dbSchema = \"arp_nutzungsplanung_pub_v1\"\n    target = [project.buildDir]\n\n    region = \"[2][5][8][0]\"         //exportiert Dataset 2580\n    region = \"2580\"                 //exportiert Dataset 2580\n    region = 2580                   //exportiert Dataset 2580\n    region = \"[0-9][0-9][0-9][0-9]\" //exportiert alle 3 Datasets\n    region = \".*\"                   //exportiert alle 3 Datasets\n    userFormats = true\n    kgdiService = [\"http://localhost:8080/app/rest\",\"admin\",\"admin\"]\n}\nVier xtf-Dateien: a2581.xtf, c2582.xtf, b2583.xtf, d2584.xtf, lokal im Job-Verzeichnis\ntasks.register('publishFile2', Publisher) {\n    dataIdent = \"publishFile2\"\n    sourcePath = file(\"a2581.xtf\") //Angabe zum Ablageort eines der zu publizierenden Files\n    target = [project.buildDir]\n    \n    region = \"[a-d][0-9][0-9][0-9][0-9]\"  //alle 4 Files werden publiziert\n    region = \"[2][5][8][4]\"               //d2584.xtf wird publiziert\n    kgdiService = [\"http://localhost:8080/app/rest\",\"admin\",\"admin\"]\n}\n\n\n\n\nDamit nachfolgende Tasks die Liste der tatsächlich publizierten Regionen auswerten können, kann der Parameter publishedRegions des Tasks Publisher verwendet werden.\ntasks.register('publishFile', Publisher) {\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    sourcePath = file(\"/path/file.xtf\")\n    region = \"[0-9][0-9][0-9][0-9]\"\n}\n\ntasks.register('printPublishedRegions', publishFile) {\n    doLast() {\n      println publishFile.publishedRegions\n    }\n}\nDer Publisher lässt sich somit auch über die zu publizierenden Regionen verketten.\ntasks.register('publishFile0' Publisher) {\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [project.buildDir]\n    sourcePath = file(\"../../../../src/test/resources/data/publisher/files/av_test.itf\")\n    modeldir= file(\"../../../../src/test/resources/data/publisher/ili\")\n    region=\"[0-9][0-9][0-9][0-9]\"\n}\n\ntasks.register('publishFile1', Publisher) {\n    dataIdent = \"ch.so.agi.vermessung.pub\"\n    target = [project.buildDir]\n    sourcePath = file(\"../../../../src/test/resources/data/publisher/files/av_test.itf\")\n    modeldir= file(\"../../../../src/test/resources/data/publisher/ili\")\n    regions=publishFile0.publishedRegions\n}\nEnthält das aktuelle Verzeichnis schon Daten (von Regionen), werden diese (wie sonst auch historisiert), und danach mit den neuen Regionen ergänzt. Der Parameter publishedRegions enthält nur die neu publizierten Regionen (und nicht alle publizierten Regionen). Auch an den KGDI-Service werden nur die neu publizierten Regionen notifiziert (und nicht alle publizierten Regionen). Die Dateien im meta Unterverzeichnis werden neu erstellt.\n\n\n\nDie Validierung kann mit einer ilivalidator Konfigurationsdatei konfiguriert werden.\ntasks.register('publishFile', Publisher) {\n    ...\n    validationConfig =  \"validationConfig.ini\"\n}\n\n\n\nOptional können Benutzerformate (Geopackage, Shapefile, Dxf) erstellt werden. Die Daten müssen in einer entsprechend flachen Struktur vorliegen.\ntasks.register('publishFromDb', Publisher) {\n    dataIdent = \"ch.so.agi.vermessung\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    database = [\"uri\",\"user\",\"password\"]\n    dbSchema \"av\"\n    dataset = \"dataset\"\n    userFormats = true\n}\n\ntasks.register('publishFromFile', Publisher) {\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [file(\"/out\")]  \n    sourcePath = file(\"/path/file.xtf\")\n    userFormats = true\n}\nFalls das Datenmodell der Quelldaten DM01AVCH24LV95D ist, wird das DXF automatisch in der Geobau-Struktur erstellt und kein Geopackage und kein Shapefile erstellt.\nDie Benutzerformate werden beim Verschieben in den Archiv-Ordner entfernt.\n\n\n\nDer Service wird benutzt, um:\n\nden Beipackzettel (im Unterordner meta) zu erstellen/beziehen\ndas Publikationsdatum in den Metadaten nachzuführen\n\ntasks.register('publishFile', Publisher) {\n    ...\n    kgdiTokenService = [\"http://api.kgdi.ch/metadata\",\"superuser\",\"superpwd\"]\n    kgdiService = [\"http://api.kgdi.ch/metadata\",\"user\",\"pwd\"]\n}\n\n\nHTTP GET: endpoint+\"/doc?dataident=\"+dataIdent+\"&published=\"+versionTag\n\n\n\nHTTP PUT: endpoint+\"/pubsignal\",request\nDer Request-Body “ohne Regionen”:\n{\n    \"dataIdent\": \"ch.so.afu.gewaesserschutz\",\n    \"published\": \"2021-12-23T14:54:49.050062\",\n    \"publishedBaskets\": [{\n        \"model\": \"SO_AGI_MOpublic_20201009\",\n        \"topic\": \"Bodenbedeckung\",\n        \"basket\": \"oltenBID\"\n    }, {\n        \"model\": \"DM01\",\n        \"topic\": \"Liegenschaften\",\n        \"basket\": \"wangenBID\"\n    }]\n}\nDer Request-Body “mit Regionen”:\n{\n  \"dataIdent\": \"ch.so.afu.gewaesserschutz\",\n  \"published\": \"2021-12-23T14:54:49.050062\",\n  \"publishedRegions\": [{\n      \"region\": \"olten\",\n      \"publishedBaskets\": [{\n          \"model\": \"SO_AGI_MOpublic_20201009\",\n          \"topic\": \"Bodenbedeckung\",\n          \"basket\": \"oltenBID\"\n      }]\n  }, {\n      \"region\": \"wangen\",\n      \"publishedBaskets\": [{\n          \"model\": \"SO_AGI_MOpublic_20201009\",\n          \"topic\": \"Bodenbedeckung\",\n          \"basket\": \"wangenBID\"\n      }]\n  }]\n}\n\n\n\n\ntasks.register('publishFile', Publisher) {\n    ...\n    grooming = \"grooming.json\"\n}\nIn der Datei grooming.json wird konfiguriert, wie ausgedünnt wird.\n{\n    \"grooming\": {\n        \"daily\": {\n            \"from\": 0,\n            \"to\": 1\n        },\n        \"weekly\": {\n            \"from\": 1,\n            \"to\": 4\n        },\n        \"monthly\": {\n            \"from\": 4,\n            \"to\": 52\n        },\n        \"yearly\": {\n            \"from\": 52,\n            \"to\": null\n        }\n    }\n}\nDie to Angabe muss mit der from Angabe der nächsthöheren Stufe identisch sein (also z.B daily.to=weekly.from). Alle from und to Angaben sind in Tagen. Einzelne Stufen können weggelassen werden. Bei der niedrigsten Stufe (i.d.R. daily) muss from=0 sein. Bei der höchsten Stufe (i.d.R. yearly) kann to definiert oder null sein. Falls to definiert ist, wird der älteste Stand beim Erreichen des to Alters gelöscht. Falls bei der höchsten Stufe to=null, wird der älteste Stand nicht gelöscht.\nSiehe Repo publisher_test bzgl. Doku der Grooming “Corner-Cases”, Testfälle und Testskript.\n\n\n\nDer Publisher arbeitet die folgenden Hauptschritte ab:\n\nVerstecktes Verzeichnis für den Datenstand via FTPS erstellen (.yyyy-MM-dd-UUID/). Kein Abbruch, falls das Verzeichnis vorhanden ist.\nXTFs in Verzeichnis ablegen.\n\nFür Datenthemen mit Quelle=Datenbank: XTF-Transferdateien exportieren.\n\nMit ili2pg das xtf erzeugen\nPrüfung des xtf gegen das Modell. Abbruch bei fatalen Fehlern\nPrüf-Bericht (und evtl. Prüf-Konfiguartion) muss auch mit in die ZIP Datei\nZIP-Datei publizieren\n\nFür Datenthemen mit Quelle=XTF: XTF in Verzeichnis kopieren.\n\nPrüfung des xtf gegen das Modell. Abbruch bei fatalen Fehlern\nPrüf-Bericht (und evtl. Prüf-Konfiguartion) muss auch mit in die ZIP Datei\nZIP-Datei publizieren\n\n\n\nAus dem Publikations-xtf die Benutzerformate (Geopackage, Shapefile, Dxf) ableiten und ablegen.\nMetadaten sammeln und im Unterordner meta/ ablegen.\n\nPublikationsdatum\nili-Dateien\nBeipackzettel (HTML via REST-API vom SIMI-Service beziehen)\n\nNeue Ordnernamen setzen.\n\n“aktuell” umbenennen auf Ordnername gemäss Datum in publishdate.json und verschieben in “hist”.\nVerstecktes Verzeichnis umbenennen auf aktuell.\nBenutzerformate in “hist” löschen\n\nPublikationsdatum via REST-API in den KGDI-Metadaten nachführen\nHistorische Stände ausdünnen.\n\n\n\n\n\n\nPublikation in den beiden Datenbereitstellungen ch.so.avt.verkehrszaehlstellen und ch.so.avt.verkehrszaehlstellen.edit\nNamenskonvention für die Dateien: \\[Datenbereitstellungs-Identifier\\].\\[Format-Identifier\\].zip\ndata-Verzeichnis:\n\n\nch.so.avt.verkehrszaehlstellen/\n\naktuell/\n\nch.so.avt.verkehrszaehlstellen.dxf.zip\n\nTabelle1.dxf\nTabelle2.dxf\n….\nvalidation.log\nvalidation.ini\n\nch.so.avt.verkehrszaehlstellen.gpkg.zip\n\nch.so.avt.verkehrszaehlstellen.gpkg\nvalidation.log\nvalidation.ini\n\nch.so.avt.verkehrszaehlstellen.shp.zip\n\nTabelle1.prj\nTabelle1.shp\nTabelle1.shx\nTabelle2.dbf\nTabelle2.prj\nTabelle2.shp\nTabelle2.shx\n….\nvalidation.log\nvalidation.ini\n\nch.so.avt.verkehrszaehlstellen.xtf.zip\n\nch.so.avt.verkehrszaehlstellen.xtf\nvalidation.log\nvalidation.ini\n\nmeta/\n\nSO_AVT_Verkehrszaehlstellen_Publikation_20190206.ili\npublishdate.json\n\ndatenbeschreibung.html\n\n\nhist/\n\n2021-04-12/ – intern identisch aufgebaut wie Ordner aktuell/ aber ohne Benutzerformate\n\nch.so.avt.verkehrszaehlstellen.xtf.zip\n\nch.so.avt.verkehrszaehlstellen.xtf\nvalidation.log\nvalidation.ini\n\nmeta/\n\nSO_AVT_Verkehrszaehlstellen_Publikation_20190206.ili\npublishdate.json\n\ndatenbeschreibung.html\n\n\n2021-03-14/\n…\n\n\nch.so.avt.verkehrszaehlstellen.edit/\n\naktuell/\n\nch.so.avt.verkehrszaehlstellen.edit.xtf.zip\n\nch.so.avt.verkehrszaehlstellen.edit.xtf\nvalidation.log\nvalidation.ini\n\nmeta/\n\nSO_AVT_Verkehrszaehlstellen_20190206.ili\n\npublishdate.json\n\ndatenbeschreibung.html\n\n\nhist/\n\n…\n\n\n\n\n\n\n\nDie Regionen werden als Präfix der Dateien abgebildet. Die Ordnerstruktur bleibt gleich. Aufbau Dateiname:\n\\[Regionen-Identifier\\].\\[Datenbereitstellungs-Identifier\\].\\[Format-Identifier\\].zip\nBeispiel AV (Regionen-Identifier ist die BFS-NR):\ndata-Verzeichnis:\n\n\nch.so.agi.av.mopublic/\n\naktuell/\n\n2501.ch.so.agi.av.mopublic.dxf.zip\n2501.ch.so.agi.av.mopublic.gpkg.zip\n2501.ch.so.agi.av.mopublic.shp.zip\n2501.ch.so.agi.av.mopublic.xtf.zip\n2502.ch.so.agi.av.mopublic.dxf.zip\n2502.ch.so.agi.av.mopublic.gpkg.zip\n2502.ch.so.agi.av.mopublic.shp.zip\n2502.ch.so.agi.av.mopublic.xtf.zip\n…\nmeta/\n\n…\n\n\n\nhist/\n\n…"
  },
  {
    "objectID": "gretl/docs/user/publisher.html#einbindung-in-einen-typischen-gretl-publikationsjob",
    "href": "gretl/docs/user/publisher.html#einbindung-in-einen-typischen-gretl-publikationsjob",
    "title": "Publisher",
    "section": "",
    "text": "In den heute vorliegenden Publikationsjobs werden häufig die Daten vom relational aufgebauten Edit-Schema mittels SQL-Queries “flachgewalzt” und ins Pub-Schema kopiert. Die Schema-Struktur wird automatisch mittels ili2pg aus Edit- und Pub-Modell generiert.\nFür den Datenbezug wird das build.gradle mit zwei Aufrufen des Publisher-Task ergänzt:\ndefaultTasks 'pubProduct'\n\ntasks.register('pubEdit', Publisher) {\n    dataIdent = \"ch.so.avt.verkehrszaehlstellen.edit\"\n    ...\n}\n\ntasks.register('transferVZS', Db2Db) {\n    dependsOn 'pubEdit'\n    ...\n}\n\ntasks.register('pubProduct', Publisher) {\n    dependsOn 'transferDenkmal'\n    dataIdent = \"ch.so.avt.verkehrszaehlstellen\"\n    ...\n}\nBei Problemen mit der Datenqualität der Originaldaten schlägt der Task “pubEdit” fehl. Der Job bricht mit Fehler ab, bevor die Daten irgendwo landen."
  },
  {
    "objectID": "gretl/docs/user/publisher.html#parameter",
    "href": "gretl/docs/user/publisher.html#parameter",
    "title": "Publisher",
    "section": "",
    "text": "Parameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataIdent\nString\nIdentifikator der Daten z.B. “ch.so.agi.vermessung.edit”.\n nein\n\n\ndatabase\nConnector\nDatenbank mit Quelldaten z.B. [\"uri\",\"user\",\"password\"]. Alternative zu sourcePath.\n ja\n\n\ndataset\nString\nili2db-Datasetname der Quelldaten “dataset” (Das ili2db-Schema muss also immer mit --createBasketCol erstellt werden)\n ja\n\n\ndbSchema\nString\nSchema in der Datenbank z.B. \"av\"\n ja\n\n\nexportModels\nString\nDas Export-Modell, indem die Daten exportiert werden. Der Parameter wird nur bei der Ausdünnung benötigt. Als Export-Modelle sind Basis-Modelle zulässig.\n ja\n\n\ngrooming\nObject\nKonfiguration für die Ausdünnung z.B. \"grooming.json\". Ohne Angabe wird nicht aufgeräumt.\n ja\n\n\nisUserFormats\nBoolean\nBenutzerformat (Geopackage, Shapefile, Dxf) erstellen. Default: false\n ja\n\n\nkgdiService\nEndpoint\nEndpunkt des SIMI-Services für die Rückmeldung des Publikationsdatums und die Erstellung des Beipackzettels, z.B. [\"http://api.kgdi.ch/metadata\",\"user\",\"pwd\"]. Publisher ergänzt die URL fallabhängig mit /pubsignal respektive /doc.\n ja\n\n\nkgdiTokenService\nEndpoint\nEndpunkt des Authentifizierung-Services, z.B. [\"http://api.kgdi.ch/metadata\",\"user\",\"pwd\"]. Publisher ergänzt die URL mit /v2/oauth/token.\n ja\n\n\nmodeldir\nString\nDateipfade, die Modell-Dateien (ili-Dateien) enthalten. Mehrere Pfade können durch Semikolon ; getrennt werden. Es sind auch URLs von Modell-Repositories möglich. Default: %ITF_DIR;http://models.interlis.ch/. %ITF_DIR ist ein Platzhalter für das Verzeichnis mit der ITF-Datei.\n ja\n\n\nmodelsToPublish\nString\nINTERLIS-Modellnamen der Quelldaten in der DB (Nur für “einfache” Modelle, deren ili2db-Schema ohne --createBasketCol erstellt werden kann).\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\npublishedRegions\nListProperty&lt;String&gt;\nListe der effektiv publizierten Regionen.\n nein\n\n\nregion\nString\nMuster (Regular Expression) der Dateinamen oder Datasetnamen, falls die Publikation Regionen-weise erfolgt z.B. \"[0-9][0-9][0-9][0-9]\". Alternative zum Parameter regions,Bei Quelle “Datei” ist die Angabe einer “stellvertretenden” Transferdatei mittels “sourcePath” zwingend. Bsp.: Bei sourcePath file(\"/transferfiles/dummy.xtf\") werden alle im Ordner “transferfiles” enthaltenen Transferdateien mit dem Muster verglichen und bei “match” selektiert und verarbeitet.\n ja\n\n\nregions\nListProperty&lt;String&gt;\nListe der zu publizierenden Regionen (Dateinamen oder Datasetnamen), falls die Publikation Regionen-weise erfolgen soll. Alternative zum Parameter region.\n ja\n\n\nsourcePath\nObject\nQuelldatei z.B. file(\"/path/file.xtf\")\n ja\n\n\ntarget\nEndpoint\nZielverzeichnis z.B. [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ] oder einfach ein Pfad [file(\"/out\")]\n nein\n\n\nvalidationConfig\nObject\nKonfiguration für die Validierung (eine ilivalidator-config-Datei) z.B. “validationConfig.ini”.\n ja\n\n\nversion\nDate\nnull\n ja"
  },
  {
    "objectID": "gretl/docs/user/publisher.html#xtf---xtf",
    "href": "gretl/docs/user/publisher.html#xtf---xtf",
    "title": "Publisher",
    "section": "",
    "text": "Falls die Daten bereits als XTF-/ITF-Datei vorliegen, muss die Quelldatei angegeben werden.\ntask publishFile(type: Publisher){\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    sourcePath = file(\"/path/file.xtf\")\n}\nDie Daten können alternativ zu SFTP in ein lokales Verzeichnis publiziert werden:\ntask publishFile(type: Publisher){\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [file(\"/out\")]  \n    sourcePath = file(\"/path/file.xtf\")\n}"
  },
  {
    "objectID": "gretl/docs/user/publisher.html#db---xtf",
    "href": "gretl/docs/user/publisher.html#db---xtf",
    "title": "Publisher",
    "section": "",
    "text": "Falls die Daten in einer ili2db konformen PostgreSQL Datenbank vorliegen, muss die Datenbank angegeben werden und welche Daten (Parameter dataset, modelsToPublish, regions, region) aus der Datenbank exportiert werden sollen.\ntasks.register('publishFromDb', Publisher) {\n    dataIdent = \"ch.so.agi.vermessung\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    database = [\"uri\",\"user\",\"password\"]\n    dbSchema \"av\"\n    dataset = \"dataset\"\n}\nNur bei einfachen Modellen (falls das DB Schema ohne createBasketCol erstellt worden ist), kann der Export alternativ mit dem Parameter modelsToPublish mit Angabe des INTERLIS-Modellnamens erfolgen:\ntasks.register('publishFromDb', Publisher) {\n    dataIdent = \"ch.so.agi.vermessung\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    database = [\"uri\",\"user\",\"password\"]\n    dbSchema \"av\"\n    modelsToPublish = \"DM01AVCH24LV95D\"\n}"
  },
  {
    "objectID": "gretl/docs/user/publisher.html#regionen",
    "href": "gretl/docs/user/publisher.html#regionen",
    "title": "Publisher",
    "section": "",
    "text": "Falls die Daten bereits als XTF-/ITF-Dateien vorliegen, muss zusätzlich zu einer möglichen Quelldatei (sourcePath) das Dateinamens-Muster (ohne Nameserweiterung (.xtf oder .itf) der Regionen (region) angegeben werden.\ntasks.register('publishFile', Publisher) {\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    sourcePath = file(\"/transferfiles/file.xtf\")\n    region = \"[0-9][0-9][0-9][0-9]\"  // regex; ersetzt den filename im sourcePath\n}\nDer sourcePath ist wie bei der Verarbeitung eines XTF (ohne Regionen) ein Datei- und nicht ein Ordner-Pfad. Mittels Parameter “region” werden aus allem im Ordner “transferfiles” enthaltenen Transfer-Dateien die zu verarbeitenden selektiert.\nFalls die Daten in einer ili2db konformen PostgreSQL Datenbank vorliegen, muss das Muster der Datensatz-Namen (dataset) angegeben werden (= ein Datensatz pro Region (region)).\ntasks.register('publishFromDb', Publisher) {\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    database = [\"uri\",\"user\",\"password\"]\n    dbSchema \"av\"\n    region = \"[0-9][0-9][0-9][0-9]\"  // regex; ersetzt das dataset\n}\nVerarbeitet werden alle Datensätze, deren Dateiname (Quelle Transferdatei) oder dataset Name (Quelle DB) auf das in Parameter “region” definierte Muster (regular Expression) zutreffen.\n\n\nEs können eindeutige Namen oder auch regular expressions verwendet werden.\nExport mit region aus lokaler DB (Nutzungsplanung mit 3 Datasets: 2580, 2581, 2582)\ntasks.register('publishFromDb2', Publisher) {\n    dataIdent = \"ch.so.arp.nutzungsplanung.publishFromDb2\"\n    database = [dbUriPub, dbUserPub, dbPwdPub]\n    dbSchema = \"arp_nutzungsplanung_pub_v1\"\n    target = [project.buildDir]\n\n    region = \"[2][5][8][0]\"         //exportiert Dataset 2580\n    region = \"2580\"                 //exportiert Dataset 2580\n    region = 2580                   //exportiert Dataset 2580\n    region = \"[0-9][0-9][0-9][0-9]\" //exportiert alle 3 Datasets\n    region = \".*\"                   //exportiert alle 3 Datasets\n    userFormats = true\n    kgdiService = [\"http://localhost:8080/app/rest\",\"admin\",\"admin\"]\n}\nVier xtf-Dateien: a2581.xtf, c2582.xtf, b2583.xtf, d2584.xtf, lokal im Job-Verzeichnis\ntasks.register('publishFile2', Publisher) {\n    dataIdent = \"publishFile2\"\n    sourcePath = file(\"a2581.xtf\") //Angabe zum Ablageort eines der zu publizierenden Files\n    target = [project.buildDir]\n    \n    region = \"[a-d][0-9][0-9][0-9][0-9]\"  //alle 4 Files werden publiziert\n    region = \"[2][5][8][4]\"               //d2584.xtf wird publiziert\n    kgdiService = [\"http://localhost:8080/app/rest\",\"admin\",\"admin\"]\n}"
  },
  {
    "objectID": "gretl/docs/user/publisher.html#verkettung-von-publishern",
    "href": "gretl/docs/user/publisher.html#verkettung-von-publishern",
    "title": "Publisher",
    "section": "",
    "text": "Damit nachfolgende Tasks die Liste der tatsächlich publizierten Regionen auswerten können, kann der Parameter publishedRegions des Tasks Publisher verwendet werden.\ntasks.register('publishFile', Publisher) {\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    sourcePath = file(\"/path/file.xtf\")\n    region = \"[0-9][0-9][0-9][0-9]\"\n}\n\ntasks.register('printPublishedRegions', publishFile) {\n    doLast() {\n      println publishFile.publishedRegions\n    }\n}\nDer Publisher lässt sich somit auch über die zu publizierenden Regionen verketten.\ntasks.register('publishFile0' Publisher) {\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [project.buildDir]\n    sourcePath = file(\"../../../../src/test/resources/data/publisher/files/av_test.itf\")\n    modeldir= file(\"../../../../src/test/resources/data/publisher/ili\")\n    region=\"[0-9][0-9][0-9][0-9]\"\n}\n\ntasks.register('publishFile1', Publisher) {\n    dataIdent = \"ch.so.agi.vermessung.pub\"\n    target = [project.buildDir]\n    sourcePath = file(\"../../../../src/test/resources/data/publisher/files/av_test.itf\")\n    modeldir= file(\"../../../../src/test/resources/data/publisher/ili\")\n    regions=publishFile0.publishedRegions\n}\nEnthält das aktuelle Verzeichnis schon Daten (von Regionen), werden diese (wie sonst auch historisiert), und danach mit den neuen Regionen ergänzt. Der Parameter publishedRegions enthält nur die neu publizierten Regionen (und nicht alle publizierten Regionen). Auch an den KGDI-Service werden nur die neu publizierten Regionen notifiziert (und nicht alle publizierten Regionen). Die Dateien im meta Unterverzeichnis werden neu erstellt."
  },
  {
    "objectID": "gretl/docs/user/publisher.html#validierung",
    "href": "gretl/docs/user/publisher.html#validierung",
    "title": "Publisher",
    "section": "",
    "text": "Die Validierung kann mit einer ilivalidator Konfigurationsdatei konfiguriert werden.\ntasks.register('publishFile', Publisher) {\n    ...\n    validationConfig =  \"validationConfig.ini\"\n}"
  },
  {
    "objectID": "gretl/docs/user/publisher.html#benutzer-formate-gpkg-dxf-shp",
    "href": "gretl/docs/user/publisher.html#benutzer-formate-gpkg-dxf-shp",
    "title": "Publisher",
    "section": "",
    "text": "Optional können Benutzerformate (Geopackage, Shapefile, Dxf) erstellt werden. Die Daten müssen in einer entsprechend flachen Struktur vorliegen.\ntasks.register('publishFromDb', Publisher) {\n    dataIdent = \"ch.so.agi.vermessung\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    database = [\"uri\",\"user\",\"password\"]\n    dbSchema \"av\"\n    dataset = \"dataset\"\n    userFormats = true\n}\n\ntasks.register('publishFromFile', Publisher) {\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [file(\"/out\")]  \n    sourcePath = file(\"/path/file.xtf\")\n    userFormats = true\n}\nFalls das Datenmodell der Quelldaten DM01AVCH24LV95D ist, wird das DXF automatisch in der Geobau-Struktur erstellt und kein Geopackage und kein Shapefile erstellt.\nDie Benutzerformate werden beim Verschieben in den Archiv-Ordner entfernt."
  },
  {
    "objectID": "gretl/docs/user/publisher.html#kgdi-service",
    "href": "gretl/docs/user/publisher.html#kgdi-service",
    "title": "Publisher",
    "section": "",
    "text": "Der Service wird benutzt, um:\n\nden Beipackzettel (im Unterordner meta) zu erstellen/beziehen\ndas Publikationsdatum in den Metadaten nachzuführen\n\ntasks.register('publishFile', Publisher) {\n    ...\n    kgdiTokenService = [\"http://api.kgdi.ch/metadata\",\"superuser\",\"superpwd\"]\n    kgdiService = [\"http://api.kgdi.ch/metadata\",\"user\",\"pwd\"]\n}\n\n\nHTTP GET: endpoint+\"/doc?dataident=\"+dataIdent+\"&published=\"+versionTag\n\n\n\nHTTP PUT: endpoint+\"/pubsignal\",request\nDer Request-Body “ohne Regionen”:\n{\n    \"dataIdent\": \"ch.so.afu.gewaesserschutz\",\n    \"published\": \"2021-12-23T14:54:49.050062\",\n    \"publishedBaskets\": [{\n        \"model\": \"SO_AGI_MOpublic_20201009\",\n        \"topic\": \"Bodenbedeckung\",\n        \"basket\": \"oltenBID\"\n    }, {\n        \"model\": \"DM01\",\n        \"topic\": \"Liegenschaften\",\n        \"basket\": \"wangenBID\"\n    }]\n}\nDer Request-Body “mit Regionen”:\n{\n  \"dataIdent\": \"ch.so.afu.gewaesserschutz\",\n  \"published\": \"2021-12-23T14:54:49.050062\",\n  \"publishedRegions\": [{\n      \"region\": \"olten\",\n      \"publishedBaskets\": [{\n          \"model\": \"SO_AGI_MOpublic_20201009\",\n          \"topic\": \"Bodenbedeckung\",\n          \"basket\": \"oltenBID\"\n      }]\n  }, {\n      \"region\": \"wangen\",\n      \"publishedBaskets\": [{\n          \"model\": \"SO_AGI_MOpublic_20201009\",\n          \"topic\": \"Bodenbedeckung\",\n          \"basket\": \"wangenBID\"\n      }]\n  }]\n}"
  },
  {
    "objectID": "gretl/docs/user/publisher.html#archiv-aufräumen",
    "href": "gretl/docs/user/publisher.html#archiv-aufräumen",
    "title": "Publisher",
    "section": "",
    "text": "tasks.register('publishFile', Publisher) {\n    ...\n    grooming = \"grooming.json\"\n}\nIn der Datei grooming.json wird konfiguriert, wie ausgedünnt wird.\n{\n    \"grooming\": {\n        \"daily\": {\n            \"from\": 0,\n            \"to\": 1\n        },\n        \"weekly\": {\n            \"from\": 1,\n            \"to\": 4\n        },\n        \"monthly\": {\n            \"from\": 4,\n            \"to\": 52\n        },\n        \"yearly\": {\n            \"from\": 52,\n            \"to\": null\n        }\n    }\n}\nDie to Angabe muss mit der from Angabe der nächsthöheren Stufe identisch sein (also z.B daily.to=weekly.from). Alle from und to Angaben sind in Tagen. Einzelne Stufen können weggelassen werden. Bei der niedrigsten Stufe (i.d.R. daily) muss from=0 sein. Bei der höchsten Stufe (i.d.R. yearly) kann to definiert oder null sein. Falls to definiert ist, wird der älteste Stand beim Erreichen des to Alters gelöscht. Falls bei der höchsten Stufe to=null, wird der älteste Stand nicht gelöscht.\nSiehe Repo publisher_test bzgl. Doku der Grooming “Corner-Cases”, Testfälle und Testskript."
  },
  {
    "objectID": "gretl/docs/user/publisher.html#ablauf",
    "href": "gretl/docs/user/publisher.html#ablauf",
    "title": "Publisher",
    "section": "",
    "text": "Der Publisher arbeitet die folgenden Hauptschritte ab:\n\nVerstecktes Verzeichnis für den Datenstand via FTPS erstellen (.yyyy-MM-dd-UUID/). Kein Abbruch, falls das Verzeichnis vorhanden ist.\nXTFs in Verzeichnis ablegen.\n\nFür Datenthemen mit Quelle=Datenbank: XTF-Transferdateien exportieren.\n\nMit ili2pg das xtf erzeugen\nPrüfung des xtf gegen das Modell. Abbruch bei fatalen Fehlern\nPrüf-Bericht (und evtl. Prüf-Konfiguartion) muss auch mit in die ZIP Datei\nZIP-Datei publizieren\n\nFür Datenthemen mit Quelle=XTF: XTF in Verzeichnis kopieren.\n\nPrüfung des xtf gegen das Modell. Abbruch bei fatalen Fehlern\nPrüf-Bericht (und evtl. Prüf-Konfiguartion) muss auch mit in die ZIP Datei\nZIP-Datei publizieren\n\n\n\nAus dem Publikations-xtf die Benutzerformate (Geopackage, Shapefile, Dxf) ableiten und ablegen.\nMetadaten sammeln und im Unterordner meta/ ablegen.\n\nPublikationsdatum\nili-Dateien\nBeipackzettel (HTML via REST-API vom SIMI-Service beziehen)\n\nNeue Ordnernamen setzen.\n\n“aktuell” umbenennen auf Ordnername gemäss Datum in publishdate.json und verschieben in “hist”.\nVerstecktes Verzeichnis umbenennen auf aktuell.\nBenutzerformate in “hist” löschen\n\nPublikationsdatum via REST-API in den KGDI-Metadaten nachführen\nHistorische Stände ausdünnen."
  },
  {
    "objectID": "gretl/docs/user/publisher.html#ordnerstruktur-im-ziel-verzeichnis",
    "href": "gretl/docs/user/publisher.html#ordnerstruktur-im-ziel-verzeichnis",
    "title": "Publisher",
    "section": "",
    "text": "Publikation in den beiden Datenbereitstellungen ch.so.avt.verkehrszaehlstellen und ch.so.avt.verkehrszaehlstellen.edit\nNamenskonvention für die Dateien: \\[Datenbereitstellungs-Identifier\\].\\[Format-Identifier\\].zip\ndata-Verzeichnis:\n\n\nch.so.avt.verkehrszaehlstellen/\n\naktuell/\n\nch.so.avt.verkehrszaehlstellen.dxf.zip\n\nTabelle1.dxf\nTabelle2.dxf\n….\nvalidation.log\nvalidation.ini\n\nch.so.avt.verkehrszaehlstellen.gpkg.zip\n\nch.so.avt.verkehrszaehlstellen.gpkg\nvalidation.log\nvalidation.ini\n\nch.so.avt.verkehrszaehlstellen.shp.zip\n\nTabelle1.prj\nTabelle1.shp\nTabelle1.shx\nTabelle2.dbf\nTabelle2.prj\nTabelle2.shp\nTabelle2.shx\n….\nvalidation.log\nvalidation.ini\n\nch.so.avt.verkehrszaehlstellen.xtf.zip\n\nch.so.avt.verkehrszaehlstellen.xtf\nvalidation.log\nvalidation.ini\n\nmeta/\n\nSO_AVT_Verkehrszaehlstellen_Publikation_20190206.ili\npublishdate.json\n\ndatenbeschreibung.html\n\n\nhist/\n\n2021-04-12/ – intern identisch aufgebaut wie Ordner aktuell/ aber ohne Benutzerformate\n\nch.so.avt.verkehrszaehlstellen.xtf.zip\n\nch.so.avt.verkehrszaehlstellen.xtf\nvalidation.log\nvalidation.ini\n\nmeta/\n\nSO_AVT_Verkehrszaehlstellen_Publikation_20190206.ili\npublishdate.json\n\ndatenbeschreibung.html\n\n\n2021-03-14/\n…\n\n\nch.so.avt.verkehrszaehlstellen.edit/\n\naktuell/\n\nch.so.avt.verkehrszaehlstellen.edit.xtf.zip\n\nch.so.avt.verkehrszaehlstellen.edit.xtf\nvalidation.log\nvalidation.ini\n\nmeta/\n\nSO_AVT_Verkehrszaehlstellen_20190206.ili\n\npublishdate.json\n\ndatenbeschreibung.html\n\n\nhist/\n\n…\n\n\n\n\n\n\n\nDie Regionen werden als Präfix der Dateien abgebildet. Die Ordnerstruktur bleibt gleich. Aufbau Dateiname:\n\\[Regionen-Identifier\\].\\[Datenbereitstellungs-Identifier\\].\\[Format-Identifier\\].zip\nBeispiel AV (Regionen-Identifier ist die BFS-NR):\ndata-Verzeichnis:\n\n\nch.so.agi.av.mopublic/\n\naktuell/\n\n2501.ch.so.agi.av.mopublic.dxf.zip\n2501.ch.so.agi.av.mopublic.gpkg.zip\n2501.ch.so.agi.av.mopublic.shp.zip\n2501.ch.so.agi.av.mopublic.xtf.zip\n2502.ch.so.agi.av.mopublic.dxf.zip\n2502.ch.so.agi.av.mopublic.gpkg.zip\n2502.ch.so.agi.av.mopublic.shp.zip\n2502.ch.so.agi.av.mopublic.xtf.zip\n…\nmeta/\n\n…\n\n\n\nhist/\n\n…"
  },
  {
    "objectID": "gretl/docs/dev/index.html",
    "href": "gretl/docs/dev/index.html",
    "title": "Developing",
    "section": "",
    "text": "Die Version für GRETL wird in der Gradle-Datei versioning.gradle definiert. Es wird nur die Major- und Minor-Version gesetzt. Die Patch-Version soll durch die Pipeline gesetzt werden. Falls lokal gebuildet wird, ist die Patch-Version “LOCALBUILD”.\nEin Minorupdate erfolgte bisher, wenn wir Majorupdates der ilitools durchführten. Oder ausnahmsweisen, wenn Minorupdates der ilitools nicht in jedem Fall rückwärtskompatibel sind.\n\n\n\nEs gibt die beiden Unterprojekte gretl und runtimeImage. Ersteres ist für die eigentliche Entwicklung und das runtimeImage-Unterprojekt dient dem Herstellen des Dockerimages.\n\n\n\nGRETL ist ein Gradle Plugin und entsprechend benötigt es die Gradle API. Wenn man Gradle im Projekt updatet, bedeutet dies, dass auch das Plugin mit dieser API-Version entwickelt wird. Das Dockerimage ist entsprechend abzugleichen. Gradle wird im Projekt wie folgt upgedatet:\n./gradlew wrapper --gradle-version 8.10.2\nDas Dockerimage ist an folgenden Stellen anzupassen:\nENV GRADLE_VERSION 8.10.2\nARG GRADLE_DOWNLOAD_SHA256=31c55713e40233a8303827ceb42ca48a47267a0ad4bab9177123121e71524c26\nEin Update sollte nicht leichtfertig vorgenommen werden, da es verschiedene Auswirkungen haben kann. Andererseits läuft nicht jede Gradle-Version mit jeder neusten Java-Version (was für uns momentan nicht relevant ist).\nDie momentan eingesetzte Version 7 funktioniert bis und mit Java 19. Die Version 8 braucht beim allerersten Durchlauf nochmals länger (siehe Kapitel “Running”) und loggt auch mehr im Lifecycle-Level.\n\n\n\nGRETL verwendet als Schnittstellenwerkzeug sehr viele Abhängigkeiten, die wiederum Abhängigkeiten verwenden. Aus diesem Grund kann es zu Versionskonflikten führen. Diese können mittels ./gradlew gretl:dependencies &gt; dependencies.log herausgefunden werden. Die Konflikte kann man auflösen, indem man z.B. gewissen transitive Abhängigkeiten einer Abhängigkeit exkludiert oder die Version explizit forsiert. Für die ilitools werden die Versionen bewusst forciert. Versionskonflikte können auch das Dockerimage speziell betreffen, da wir dort zusätzlich Gradle-Plugins reinkopieren. In den Ordner stageJars werden sämtliche Jar-Dateien kopiert, was ebenfalls einen guten Überblick gibt. Eine weitere Variante zum Auflösen des Versionskonfliktes von transitiven Abhängigkeiten ist das Verwenden anderer Versionen der Bibliotheken.\n\n\n\nFür jeden selber programmierten Customtask gibt es (in der Regel) auch einen dazugehörigen Step. Sämtliche Businesslogik sollte im Step programmiert werden (und ohne Gradle-Abhänigkeiten auskommen). Der Task ist nur die öffentliche API ohne viel mehr Logik.\n\n\n\nGradle bietet verschiedene Annotationen für die Task Properties an. So können Properties als optional gekennzeichnet werden oder explizit als Input-Datei. Wird etwas als Input oder Output annotiert, berücksichtig Gradle diese Properties bei der Evaluation, ob ein Task up-to-date ist. Hat sich z.B. ein INTERLIS-Datei nicht geändert, muss sie ja nicht nochmals validiert werden. Wir haben ganz geklärt, ob wir diese Verhalten wünschen. Aus diesem Grund gibt es auch Custom-Tasks, bei denen die Properties mit Internal annotiert sind. Diese Properties werden bei der Evaluierung nicht berücksichtigt. Bei unseren Gretljobs haben wird das so gelöst, dass wir den dazu benötigten Cache explizit im Container behalten und nicht lokal speichern. Ein anderes Dockerimage kann das jedoch anders handhaben. Ebenso beim Einsatz des Gradle-Plugins direkt. Hier dient es sich an, dass mit mit --rerun-tasks das Ausführen der Tasks forciert. Diese Option wird auch bei den Integrationstests verwendet.\n\n\n\n… todo …\n\n\n\n\nEs gibt Unit- und Integrationstests. Die Unittests dienen zum Testen der Steps (und einigen anderen Helfer-Klassen). Es gibt zwei zusätzliche Testgruppen, die separat aufgerufen werden müssen: dbTest und s3Test.\n./gradlew gretl:test gretl:dbTest gretl:s3Test\nBeide benötigten Infrastruktur, die wir via Testcontainers (PostgreSQL und Localstack) bereitstellen. Es finden bereits auch in den “normalen” Tests Datenbanktests statt. In diesem Fall wird eine dateibasierte Datenbank verwendet.\nDie Integrationstest sind Tests mit richtigen build.gradle-Dateien und simulieren so die fertige Software. Sie werden zweifach durchgeführt. Zuerst als Gradle-Plugin (“Java pur”) und anschliessend werden alle Jobs nochmals mit dem Dockercontainer geprüft. Aus historischen Gründen (damals kein Localstack verwendet) werden die S3-Tests speziell getaggt.\nNach erfolgreichen Unittests wird das Plugin erstellt und in das lokale Maven-Repository (für das Dockerimage) publiziert:\n./gradlew gretl:build gretl:publishPluginMavenPublicationToMavenLocal -x test\nEs folgen die Gradle-Plugin-Tests:\n./gradlew gretl:jarTest gretl:jarS3Test\nNach dem Builden des Dockerimages\n./gradlew runtimeImage:buildImage\nfolgen die Tests mit dem Dockercontainer:\n./gradlew gretl:imageTest gretl:imageS3Test\nDie Tests können - wie erwähnt - dank Testcontainers komplett lokal durchgeführt werden. Falls in der Github Action Test fehlschlagen, steht der Report zur Verfügung.\nDas Testen des Gradle-Plugins wird mit einem GradleRunner gemacht. Hier ist die Herausforderung, dass der Classpath dem geforkten Java-Prozess (der den Gradle-Job durchführt) bekannt gemacht wird. Nicht ganz nachvollziehbar ist, warum die Lösung mittels .withPluginClasspath() nicht funktioniert. Aus diesem Grund wird der Classpath in eine Datei geschrieben, die wiederum als Grundlage für die Dependencies in der verwendeteten init.gradle-Datei dient. Die Jobs werden mit --rerun-tasks ausgeführt. Dieses Argument ist momentan hardcodiert. Damit wird sichergestellt, dass immer alle Tasks (unabhängig der Annotationen) ausgeführt werden.\n\n\n\n\n\nDas Gradle-Plugin kann problemlos mit Boardmitteln erstellt und publiziert (https://plugins.gradle.org) werden.\n\n\n\nZiel war es von Beginn weg eine Dockerimage zu erstellen, das sämtliche Abhängigkeiten beinhalten (“offline-fähig”). Das wird erreicht, indem alle Abhängigkeiten zuerst mittel Gradle-Job in den Ordner jars4image kopiert werden (inkl. zusätzlichen Gradle-Plugins).\nDie Docker-Befehle sind als Exec-Aufruf ebenfalls im Gradle-Job. Einzig das Einloggen in eine Docker-Registry und das Vorbereiten der buildx-Umgebung wird in der Github Action Konfiguration gemacht (oder muss lokal einmalig gemacht werden). Für die Integrations-Image-Tests wird ein Dockerimage mit dem Tag “test” hergestellt. Erst ganz am Schluss wird der Multi-Arch-Dockerbuild gemacht. Grund dafür ist, dass dieser Build das passende Image nicht in die lokale Registry publiziert und somit auch nicht zur Verfügung steht und vor allem das Publizieren in die externen Registry ist ohne die Build-und-Push-Action sehr mühselig (Manifest-Files etc. pp.).\nFür DuckDB werden auch einige Extensions in das Dockerimage gebrannt. Dies ist notwendig, da sonst bei jedem Run die Extension neu installiert (und heruntergeladen) werden müsste. Dazu werden in der stage-duckdb-extensions.gradle-Datei direkt via JDBC die notwendigen Installationsbefehle ausgeführt. Achtung: Die DuckDB-Version muss hier manuell angepasst werden.\n\n\n\n\nGradle cached verschiedene Informationen. Gewisses Verhalten ist von uns eher gewollt als anderes:\nBeim Einsatz des Gradle-Plugins (ohne Dockerimage) kann es sein, dass gewisse Tasks nicht durchgeführt werden, weil sich der Input (z.B. eine INTERLIS-Datei) nicht geändert hat. Die Ausführung des Tasks kann man mit --rerun-tasks forcieren. Bei unseren Gretljobs ist dies nicht notwendig, weil wir den Projektcache explizit im Container behalten und nicht lokal mounten (--project-cache-dir=/home/gradle/projectcache).\nBeim lokalen Entwickeln ist es jedoch wünschenswert, wenn der Ordner /home/gradle/.gradle/caches gemounted wird, da die Ausführungszeiten (nach der allerersten) massiv schneller werden. Die Integrationstests mit dem Dockerimage werden ebenfalls so durchgeführt, was die Laufzeit auf einen Drittel reduziert (siehe start-gretl.sh und Pipeline “mkdir”). Es scheint auch, dass mit jeder Gradle Major-Version immer mehr beim allerersten Durchlauf evaluiert wird. Das Mounten dieses Caches war mit Gradle Version 5.x noch nicht nötig (resp. war praktisch nicht spürbar). Mit Gradle Version 8.x geht es nochmals länger und es wird auch mehr geloggt.\nEs gibt relativ neu (mit Version 7 immer noch experimental) den Konfigurationscache. Dieser müsste freigeschaltet werden, bringt nach einigen Tests jedoch geschwindigkeitsmässig nicht viel. Zudem müssten Tasks umgeschrieben werden, weil nicht mehr auf das Project-Objekt in gleicher, einfacher Form zugegriffen werden kann.\nGradle-Daemons können mit dem Dockerimage nicht persistiert werden (weil der JVM-Prozess mit dem Container stirbt).\n\n\n\nTODO …\n\n\n\n\n\nSince java.xml is part of the JDK but is also a dependency of the Gradle API (which is automatically added by the java-gradle-plugin) you will get the famous The package javax.xml.transform.stream is accessible from more than one module: ,java.xml errors. Excluding xml-apis with all*.exclude group: 'xml-apis' should be done but will not work for the Gradle API. Workaround:\n\nClone the repository\nRun ./gradlew eclipse\nAdd org.eclipse.jdt.core.compiler.ignoreUnnamedModuleForSplitPackage=enabled to gretl/.settings/org.eclipse.jdt.core.prefs."
  },
  {
    "objectID": "gretl/docs/dev/index.html#interne-struktur",
    "href": "gretl/docs/dev/index.html#interne-struktur",
    "title": "Developing",
    "section": "",
    "text": "Die Version für GRETL wird in der Gradle-Datei versioning.gradle definiert. Es wird nur die Major- und Minor-Version gesetzt. Die Patch-Version soll durch die Pipeline gesetzt werden. Falls lokal gebuildet wird, ist die Patch-Version “LOCALBUILD”.\nEin Minorupdate erfolgte bisher, wenn wir Majorupdates der ilitools durchführten. Oder ausnahmsweisen, wenn Minorupdates der ilitools nicht in jedem Fall rückwärtskompatibel sind.\n\n\n\nEs gibt die beiden Unterprojekte gretl und runtimeImage. Ersteres ist für die eigentliche Entwicklung und das runtimeImage-Unterprojekt dient dem Herstellen des Dockerimages.\n\n\n\nGRETL ist ein Gradle Plugin und entsprechend benötigt es die Gradle API. Wenn man Gradle im Projekt updatet, bedeutet dies, dass auch das Plugin mit dieser API-Version entwickelt wird. Das Dockerimage ist entsprechend abzugleichen. Gradle wird im Projekt wie folgt upgedatet:\n./gradlew wrapper --gradle-version 8.10.2\nDas Dockerimage ist an folgenden Stellen anzupassen:\nENV GRADLE_VERSION 8.10.2\nARG GRADLE_DOWNLOAD_SHA256=31c55713e40233a8303827ceb42ca48a47267a0ad4bab9177123121e71524c26\nEin Update sollte nicht leichtfertig vorgenommen werden, da es verschiedene Auswirkungen haben kann. Andererseits läuft nicht jede Gradle-Version mit jeder neusten Java-Version (was für uns momentan nicht relevant ist).\nDie momentan eingesetzte Version 7 funktioniert bis und mit Java 19. Die Version 8 braucht beim allerersten Durchlauf nochmals länger (siehe Kapitel “Running”) und loggt auch mehr im Lifecycle-Level.\n\n\n\nGRETL verwendet als Schnittstellenwerkzeug sehr viele Abhängigkeiten, die wiederum Abhängigkeiten verwenden. Aus diesem Grund kann es zu Versionskonflikten führen. Diese können mittels ./gradlew gretl:dependencies &gt; dependencies.log herausgefunden werden. Die Konflikte kann man auflösen, indem man z.B. gewissen transitive Abhängigkeiten einer Abhängigkeit exkludiert oder die Version explizit forsiert. Für die ilitools werden die Versionen bewusst forciert. Versionskonflikte können auch das Dockerimage speziell betreffen, da wir dort zusätzlich Gradle-Plugins reinkopieren. In den Ordner stageJars werden sämtliche Jar-Dateien kopiert, was ebenfalls einen guten Überblick gibt. Eine weitere Variante zum Auflösen des Versionskonfliktes von transitiven Abhängigkeiten ist das Verwenden anderer Versionen der Bibliotheken.\n\n\n\nFür jeden selber programmierten Customtask gibt es (in der Regel) auch einen dazugehörigen Step. Sämtliche Businesslogik sollte im Step programmiert werden (und ohne Gradle-Abhänigkeiten auskommen). Der Task ist nur die öffentliche API ohne viel mehr Logik.\n\n\n\nGradle bietet verschiedene Annotationen für die Task Properties an. So können Properties als optional gekennzeichnet werden oder explizit als Input-Datei. Wird etwas als Input oder Output annotiert, berücksichtig Gradle diese Properties bei der Evaluation, ob ein Task up-to-date ist. Hat sich z.B. ein INTERLIS-Datei nicht geändert, muss sie ja nicht nochmals validiert werden. Wir haben ganz geklärt, ob wir diese Verhalten wünschen. Aus diesem Grund gibt es auch Custom-Tasks, bei denen die Properties mit Internal annotiert sind. Diese Properties werden bei der Evaluierung nicht berücksichtigt. Bei unseren Gretljobs haben wird das so gelöst, dass wir den dazu benötigten Cache explizit im Container behalten und nicht lokal speichern. Ein anderes Dockerimage kann das jedoch anders handhaben. Ebenso beim Einsatz des Gradle-Plugins direkt. Hier dient es sich an, dass mit mit --rerun-tasks das Ausführen der Tasks forciert. Diese Option wird auch bei den Integrationstests verwendet.\n\n\n\n… todo …"
  },
  {
    "objectID": "gretl/docs/dev/index.html#testing",
    "href": "gretl/docs/dev/index.html#testing",
    "title": "Developing",
    "section": "",
    "text": "Es gibt Unit- und Integrationstests. Die Unittests dienen zum Testen der Steps (und einigen anderen Helfer-Klassen). Es gibt zwei zusätzliche Testgruppen, die separat aufgerufen werden müssen: dbTest und s3Test.\n./gradlew gretl:test gretl:dbTest gretl:s3Test\nBeide benötigten Infrastruktur, die wir via Testcontainers (PostgreSQL und Localstack) bereitstellen. Es finden bereits auch in den “normalen” Tests Datenbanktests statt. In diesem Fall wird eine dateibasierte Datenbank verwendet.\nDie Integrationstest sind Tests mit richtigen build.gradle-Dateien und simulieren so die fertige Software. Sie werden zweifach durchgeführt. Zuerst als Gradle-Plugin (“Java pur”) und anschliessend werden alle Jobs nochmals mit dem Dockercontainer geprüft. Aus historischen Gründen (damals kein Localstack verwendet) werden die S3-Tests speziell getaggt.\nNach erfolgreichen Unittests wird das Plugin erstellt und in das lokale Maven-Repository (für das Dockerimage) publiziert:\n./gradlew gretl:build gretl:publishPluginMavenPublicationToMavenLocal -x test\nEs folgen die Gradle-Plugin-Tests:\n./gradlew gretl:jarTest gretl:jarS3Test\nNach dem Builden des Dockerimages\n./gradlew runtimeImage:buildImage\nfolgen die Tests mit dem Dockercontainer:\n./gradlew gretl:imageTest gretl:imageS3Test\nDie Tests können - wie erwähnt - dank Testcontainers komplett lokal durchgeführt werden. Falls in der Github Action Test fehlschlagen, steht der Report zur Verfügung.\nDas Testen des Gradle-Plugins wird mit einem GradleRunner gemacht. Hier ist die Herausforderung, dass der Classpath dem geforkten Java-Prozess (der den Gradle-Job durchführt) bekannt gemacht wird. Nicht ganz nachvollziehbar ist, warum die Lösung mittels .withPluginClasspath() nicht funktioniert. Aus diesem Grund wird der Classpath in eine Datei geschrieben, die wiederum als Grundlage für die Dependencies in der verwendeteten init.gradle-Datei dient. Die Jobs werden mit --rerun-tasks ausgeführt. Dieses Argument ist momentan hardcodiert. Damit wird sichergestellt, dass immer alle Tasks (unabhängig der Annotationen) ausgeführt werden."
  },
  {
    "objectID": "gretl/docs/dev/index.html#building",
    "href": "gretl/docs/dev/index.html#building",
    "title": "Developing",
    "section": "",
    "text": "Das Gradle-Plugin kann problemlos mit Boardmitteln erstellt und publiziert (https://plugins.gradle.org) werden.\n\n\n\nZiel war es von Beginn weg eine Dockerimage zu erstellen, das sämtliche Abhängigkeiten beinhalten (“offline-fähig”). Das wird erreicht, indem alle Abhängigkeiten zuerst mittel Gradle-Job in den Ordner jars4image kopiert werden (inkl. zusätzlichen Gradle-Plugins).\nDie Docker-Befehle sind als Exec-Aufruf ebenfalls im Gradle-Job. Einzig das Einloggen in eine Docker-Registry und das Vorbereiten der buildx-Umgebung wird in der Github Action Konfiguration gemacht (oder muss lokal einmalig gemacht werden). Für die Integrations-Image-Tests wird ein Dockerimage mit dem Tag “test” hergestellt. Erst ganz am Schluss wird der Multi-Arch-Dockerbuild gemacht. Grund dafür ist, dass dieser Build das passende Image nicht in die lokale Registry publiziert und somit auch nicht zur Verfügung steht und vor allem das Publizieren in die externen Registry ist ohne die Build-und-Push-Action sehr mühselig (Manifest-Files etc. pp.).\nFür DuckDB werden auch einige Extensions in das Dockerimage gebrannt. Dies ist notwendig, da sonst bei jedem Run die Extension neu installiert (und heruntergeladen) werden müsste. Dazu werden in der stage-duckdb-extensions.gradle-Datei direkt via JDBC die notwendigen Installationsbefehle ausgeführt. Achtung: Die DuckDB-Version muss hier manuell angepasst werden."
  },
  {
    "objectID": "gretl/docs/dev/index.html#running",
    "href": "gretl/docs/dev/index.html#running",
    "title": "Developing",
    "section": "",
    "text": "Gradle cached verschiedene Informationen. Gewisses Verhalten ist von uns eher gewollt als anderes:\nBeim Einsatz des Gradle-Plugins (ohne Dockerimage) kann es sein, dass gewisse Tasks nicht durchgeführt werden, weil sich der Input (z.B. eine INTERLIS-Datei) nicht geändert hat. Die Ausführung des Tasks kann man mit --rerun-tasks forcieren. Bei unseren Gretljobs ist dies nicht notwendig, weil wir den Projektcache explizit im Container behalten und nicht lokal mounten (--project-cache-dir=/home/gradle/projectcache).\nBeim lokalen Entwickeln ist es jedoch wünschenswert, wenn der Ordner /home/gradle/.gradle/caches gemounted wird, da die Ausführungszeiten (nach der allerersten) massiv schneller werden. Die Integrationstests mit dem Dockerimage werden ebenfalls so durchgeführt, was die Laufzeit auf einen Drittel reduziert (siehe start-gretl.sh und Pipeline “mkdir”). Es scheint auch, dass mit jeder Gradle Major-Version immer mehr beim allerersten Durchlauf evaluiert wird. Das Mounten dieses Caches war mit Gradle Version 5.x noch nicht nötig (resp. war praktisch nicht spürbar). Mit Gradle Version 8.x geht es nochmals länger und es wird auch mehr geloggt.\nEs gibt relativ neu (mit Version 7 immer noch experimental) den Konfigurationscache. Dieser müsste freigeschaltet werden, bringt nach einigen Tests jedoch geschwindigkeitsmässig nicht viel. Zudem müssten Tasks umgeschrieben werden, weil nicht mehr auf das Project-Objekt in gleicher, einfacher Form zugegriffen werden kann.\nGradle-Daemons können mit dem Dockerimage nicht persistiert werden (weil der JVM-Prozess mit dem Container stirbt)."
  },
  {
    "objectID": "gretl/docs/dev/index.html#dokumentation",
    "href": "gretl/docs/dev/index.html#dokumentation",
    "title": "Developing",
    "section": "",
    "text": "TODO …"
  },
  {
    "objectID": "gretl/docs/dev/index.html#varia",
    "href": "gretl/docs/dev/index.html#varia",
    "title": "Developing",
    "section": "",
    "text": "Since java.xml is part of the JDK but is also a dependency of the Gradle API (which is automatically added by the java-gradle-plugin) you will get the famous The package javax.xml.transform.stream is accessible from more than one module: ,java.xml errors. Excluding xml-apis with all*.exclude group: 'xml-apis' should be done but will not work for the Gradle API. Workaround:\n\nClone the repository\nRun ./gradlew eclipse\nAdd org.eclipse.jdt.core.compiler.ignoreUnnamedModuleForSplitPackage=enabled to gretl/.settings/org.eclipse.jdt.core.prefs."
  },
  {
    "objectID": "gretl/gretl/src/docs/reference.html",
    "href": "gretl/gretl/src/docs/reference.html",
    "title": "Referenzdokumentation",
    "section": "",
    "text": "Das Datenmanagement-Tool GRETL ist ein Werkzeug, das für Datenimports, Datenumbauten (Modellumbau) und Datenexports eingesetzt wird. GRETL führt Jobs aus, wobei ein Job aus mehreren atomaren Tasks besteht. Damit ein Job als vollständig ausgeführt gilt, muss jeder zum Job gehörende Task vollständig ausgeführt worden sein. Schlägt ein Task fehl, gilt auch der Job als fehlgeschlagen.\nEin Job besteht aus einem oder mehreren Tasks, die gemäss einem gerichteten Graphen (Directed Acyclic Graph; DAG) miteinander verknüpft sind.\nEin Job kann aus z.B. aus einer linearen Kette von Tasks bestehen:\nBeispiel: Datenimport aus INTERLIS-Datei – Datenumbau – Datenexport nach Shapefile.\nEin Job kann sich nach einem Task aber auch auf zwei oder mehr verschiedene weitere Tasks verzweigen:\nBeispiel: Datenimport aus INTERLIS-Datei – Datenumbau in Zielschema 1 und ein zweiter Datenumbau in Zielschema 2.\nEs ist auch möglich, dass zuerst zwei oder mehr Tasks unabhängig voneinander ausgeführt werden müssen, bevor ein einzelner weiterer Task ausgeführt wird.\nDie Tasks eines Jobs werden per Konfigurationsfile konfiguriert."
  },
  {
    "objectID": "gretl/gretl/src/docs/reference.html#systemanforderungen",
    "href": "gretl/gretl/src/docs/reference.html#systemanforderungen",
    "title": "Referenzdokumentation",
    "section": "Systemanforderungen",
    "text": "Systemanforderungen\nFür die aktuelle GRETL-Version wird Java 11 und Gradle 7.6 benötigt."
  },
  {
    "objectID": "gretl/gretl/src/docs/reference.html#installation",
    "href": "gretl/gretl/src/docs/reference.html#installation",
    "title": "Referenzdokumentation",
    "section": "Installation",
    "text": "Installation\nGRETL selbst muss als Gradle-Plugin nicht explizit installiert werden, sondern wird dynamisch durch das Internet bezogen."
  },
  {
    "objectID": "gretl/gretl/src/docs/reference.html#kleines-beispiel",
    "href": "gretl/gretl/src/docs/reference.html#kleines-beispiel",
    "title": "Referenzdokumentation",
    "section": "Kleines Beispiel",
    "text": "Kleines Beispiel\nErstellen Sie in einem neuen Verzeichnis gretldemo eine neue Datei build.gradle:\nimport ch.so.agi.gretl.tasks.*\nimport ch.so.agi.gretl.api.*\n\napply plugin: 'ch.so.agi.gretl'\n\nbuildscript {\n    repositories {\n        maven { url \"https://jars.interlis.ch\" }\n        maven { url \"https://repo.osgeo.org/repository/release/\" }\n        maven { url \"https://plugins.gradle.org/m2/\" }\n        maven { url \"https://s01.oss.sonatype.org/service/local/repositories/releases/content/\" }\n        maven { url \"https://s01.oss.sonatype.org/service/local/repositories/snapshots/content/\" }\n        mavenCentral()\n    }\n    dependencies {\n        classpath group: 'ch.so.agi', name: 'gretl',  version: '3.0.+'\n    }\n}\n\ndefaultTasks 'validate'\n\ntask validate(type: IliValidator){\n    dataFiles = [\"BeispielA.xtf\"]\n}\nDie Datei build.gradle ist die Job-Konfiguration. Dieser kleine Beispiel-Job besteht nur aus einem einzigen Task: validate.\nErstellen Sie nun noch die Datei BeispielA.xtf (damit danach der Job erfolgreich ausgeführt werden kann).\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;TRANSFER xmlns=\"http://www.interlis.ch/INTERLIS2.3\"&gt;\n    &lt;HEADERSECTION SENDER=\"gretldemo\" VERSION=\"2.3\"&gt;\n    &lt;/HEADERSECTION&gt;\n    &lt;DATASECTION&gt;\n        &lt;OeREBKRMtrsfr_V1_1.Transferstruktur BID=\"B01\"&gt;\n        &lt;/OeREBKRMtrsfr_V1_1.Transferstruktur&gt;\n    &lt;/DATASECTION&gt;\n&lt;/TRANSFER&gt;\nUm den Job auszuführen, wechseln Sie ins Verzeichnis mit der Job-Konfiguration, und geben da das Kommando gradle ohne Argument ein:\ncd gretldemo\ngradle\nBUILD SUCCESSFUL zeigt an, dass der Job (die Validierung der Datei BeispielA.xtf) erfolgreich ausgeführt wurde.\nUm dieselbe Job-Konfiguration für verschiedene Datensätze verwenden zu können, muss es parametrisierbar sein. Die Jobs/Tasks können so generisch konfiguriert werden, dass dieselbe Konfiguration z.B. für Daten aus verschiedenen Gemeinden benutzt werden kann. Parameter für die Job Konfiguration können z.B. mittels gradle-Properties (Gradle properties and system properties) dem Job mitgegeben werden, also z.B.\ncd gretldemo\ngradle -Pdataset=Olten"
  },
  {
    "objectID": "gretl/gretl/src/docs/reference.html#ausführen",
    "href": "gretl/gretl/src/docs/reference.html#ausführen",
    "title": "Referenzdokumentation",
    "section": "Ausführen",
    "text": "Ausführen\nUm GRETL auszuführen, geben Sie auf der Kommandozeile folgendes Kommando ein (wobei jobfolder der absolute Pfad zu ihrem Verzeichnis mit der Job Konfiguration ist.)\ngradle --project-dir jobfolder\nAlternativ können Sie auch ins Verzeichnis mit der Job Konfiguration wechseln, und da das Kommando gradle ohne Argument verwenden:\ncd jobfolder\ngradle"
  },
  {
    "objectID": "gretl/gretl/src/docs/reference.html#tasks",
    "href": "gretl/gretl/src/docs/reference.html#tasks",
    "title": "Referenzdokumentation",
    "section": "Tasks",
    "text": "Tasks\n\nAv2ch\nTransformiert eine INTERLIS1-Transferdatei im kantonalen AV-DM01-Modell in das Bundesmodell. Unterstützt werden die Sprachen Deutsch und Italienisch und der Bezugrahmen LV95. Getestet mit Daten aus den Kantonen Solothurn, Glarus und Tessin. Weitere Informationen sind in der Basisbibliothek zu finden: https://github.com/sogis/av2ch.\nDas Bundes-ITF hat denselben Namen wie das Kantons-ITF.\nAufgrund der sehr vielen Logging-Messages einer verwendeten Bibliothek, wird der System.err-Ouput nach dev/null https://github.com/sogis/av2ch/blob/master/src/main/java/ch/so/agi/av/Av2ch.java#L75.\ntasks.register('transform', Av2ch) {\n    inputFile = file(\"254900.itf\")\n    outputDirectory = file(\"output\")\n}\nEs können auch mehrere Dateien gleichzeitig transformiert werden:\ntasks.register('transform', Av2ch) {\n    inputFile = fileTree(\".\").matching {\n            include\"*.itf\"\n        }\n    outputDirectory = file(\"output\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ninputFile\nObject\nZu transformierende ITF-Datei(en). File- oder FileCollection-Objekt.\n nein\n\n\nlanguage\nString\nSprache des Modelles / der Datei (de, it). Default: de\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\noutputDirectory\nObject\nName des Verzeichnisses in das die zu erstellende Datei geschrieben wird.\n nein\n\n\nzip\nBoolean\nDie zu erstellende Datei wird gezippt. Default: false\n ja\n\n\n\n\n\nAv2geobau\nAv2geobau konvertiert eine Interlis-Transferdatei (itf) in eine DXF-Geobau Datei. Av2geobau funktioniert ohne Datenbank.\nDie ITF-Datei muss dem Modell DM01AVCH24LV95D entsprechen. Die Daten werden nicht validiert.\nDie Datenstruktur der DXF-Datei ist im Prinzip sehr einfach: Die verschiedenen Informationen aus dem Datenmodell DM01 werden in verschiedene DXF-Layer abgebildet, z.B. die begehbaren LFP1 werden in den Layer “01111” abgebildet. Oder die Gebäude in den Layer “01211”.\nDer Datenumbau ist nicht konfigurierbar.\ntasks.register('av2geobau', Av2geobau) {\n    itfFiles = \"ch_254900.itf\"\n    dxfDirectory = \"./out/\"\n}\nEs können auch mehrere Dateien angegeben werden.\ntasks.register('av2geobau', Av2geobau) {\n    itfFiles = fileTree(\".\").matching {\n        include\"*.itf\"\n    }\n    dxfDirectory = \"./out/\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndxfDirectory\nObject\nVerzeichnis, in das die DXF-Dateien gespeichert werden.\n nein\n\n\nisZip\nBoolean\nDie zu erstellende Datei wird gezippt und es werden zusätzliche Dateien (Musterplan, Layerbeschreibung, Hinweise) hinzugefügt. Default: false\n ja\n\n\nitfFiles\nObject\nITF-Datei, die nach DXF transformiert werden soll. Es können auch mehrere Dateien angegeben werden. File- oder FileCollection-Objekt.\n nein\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Konvertierung in eine Text-Datei.\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\n\n\n\nCsv2Excel (incubating)\nKonvertiert eine CSV-Datei in eine Excel-Datei (*.xlsx). Datentypen werden anhand eines INTERLIS-Modelles eruiert. Fehlt das Modell, wird alles als Text gespeichert. Die Daten werden vollständig im Speicher vorgehalten. Falls grosse Dateien geschrieben werden müssen, kann das zu Problemen führen. Dann müsste die Apache POI SXSSF Implementierung (Streaming) verwendet werden.\nBeispiel:\ntasks.register('convertData', Csv2Excel) {\n    csvFile = file(\"./20230124_sap_Gebaeude.csv\")\n    firstLineIsHeader = true\n    valueDelimiter = null\n    valueSeparator = \";\"\n    encoding = \"ISO-8859-1\";\n    models = \"SO_HBA_Gebaeude_20230111\";\n    outputDir = file(\".\");\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncsvFile\nFile\nCSV-Datei, die konvertiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der CSV-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nfirstLineIsHeader\nBoolean\nDefiniert, ob eine Headerzeile geschrieben werden soll, oder nicht. Default: true\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell für Definition der Datentypen in der Excel-Datei.\n ja\n\n\noutputDir\nFile\nVerzeichnis, in das die Excel-Datei gespeichert wird. Default: Verzeichnis, in dem die CSV-Datei vorliegt.\n ja\n\n\nvalueDelimiter\nCharacter\nZeichen, das am Anfang und Ende jeden Wertes geschrieben werden soll. Default \"\n ja\n\n\nvalueSeparator\nCharacter\nZeichen, das als Trennzeichen zwischen den Werten verwendet werden soll. Default: ,\n ja\n\n\n\n\n\nCsvExport\nDaten aus einer bestehenden Datenbanktabelle werden in eine CSV-Datei exportiert. Geometriespalten können nicht exportiert werden.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('csvexport', CsvExport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"csvexport\"\n    tableName = \"exportdata\"\n    firstLineIsHeader = true\n    attributes = [ \"t_id\",\"Aint\"]\n    dataFile = \"data.csv\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nattributes\nString[]\nSpalten der DB-Tabelle, die exportiert werden sollen. Definiert die Reihenfolge der Spalten in der CSV-Datei. Default: alle Spalten\n ja\n\n\ndataFile\nObject\nName der CSV-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank aus der exportiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der CSV-Datei, z.B. “UTF-8”. Default: Systemeinstellung\n ja\n\n\nfirstLineIsHeader\nBoolean\nDefiniert, ob eine Headerzeile geschrieben werden soll, oder nicht. Default: true\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\ntableName\nString\nName der DB-Tabelle, die exportiert werden soll.\n nein\n\n\nvalueDelimiter\nCharacter\nZeichen, das am Anfang und Ende jeden Wertes geschrieben werden soll. Default \"\n ja\n\n\nvalueSeparator\nCharacter\nZeichen, das als Trennzeichen zwischen den Werten verwendet werden soll. Default: ,\n ja\n\n\n\n\n\nCsvImport\nDaten aus einer CSV-Datei werden in eine bestehende Datenbanktabelle importiert.\nDie Tabelle kann weitere Spalten enthalten, die in der CSV-Datei nicht vorkommen. Sie müssen aber NULLable sein, oder einen Default-Wert definiert haben.\nGeometriepalten können nicht importiert werden.\nDie Gross-/Kleinschreibung der CSV-Spaltennamen wird für die Zuordnung zu den DB-Spalten ignoriert.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('csvimport',  CsvImport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"csvimport\"\n    tableName = \"importdata\"\n    firstLineIsHeader = true\n    dataFile = \"data1.csv\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nattributes\nString[]\nSpalten der DB-Tabelle, die exportiert werden sollen. Definiert die Reihenfolge der Spalten in der CSV-Datei. Default: alle Spalten\n ja\n\n\ndataFile\nObject\nName der CSV-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank aus der exportiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der CSV-Datei, z.B. “UTF-8”. Default: Systemeinstellung\n ja\n\n\nfirstLineIsHeader\nBoolean\nDefiniert, ob eine Headerzeile geschrieben werden soll, oder nicht. Default: true\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\ntableName\nString\nName der DB-Tabelle, die exportiert werden soll.\n nein\n\n\nvalueDelimiter\nCharacter\nZeichen, das am Anfang und Ende jeden Wertes geschrieben werden soll. Default \"\n ja\n\n\nvalueSeparator\nCharacter\nZeichen, das als Trennzeichen zwischen den Werten verwendet werden soll. Default: ,\n ja\n\n\n\n\n\nCsvValidator\nPrüft eine CSV-Datei gegenüber einem INTERLIS-Modell. Basiert auf dem ilivalidator. Das Datenmodell darf die OID nicht als UUID modellieren (OID AS INTERLIS.UUIDOID).\nBeispiel:\ntasks.register('validate', CsvValidator) {\n    models = \"CsvModel\"\n    firstLineIsHeader = true\n    dataFiles = [\"data1.csv\"]\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nallObjectsAccessible\nBoolean\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n ja\n\n\nconfigFile\nObject\nKonfiguriert die Datenprüfung mit Hilfe einer ini-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\ndataFiles\nObject\nListe der Dateien, die validiert werden sollen. FileCollection oder List. Eine leere Liste ist kein Fehler.\n nein\n\n\ndisableAreaValidation\nBoolean\nSchaltet die AREA-Topologieprüfung aus. Default: false\n ja\n\n\nencoding\nString\nZeichencodierung der CSV-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nfailOnError\nBoolean\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n ja\n\n\nfirstLineIsHeader\nBoolean\nDefiniert, ob die CSV-Datei einer Headerzeile hat, oder nicht. Default: true\n ja\n\n\nforceTypeValidation\nBoolean\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n ja\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n ja\n\n\nmetaConfigFile\nObject\nKonfiguriert den Validator mit Hilfe einer ini-Datei. Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell, gegen das die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Der Name der CSV-Datei.\n ja\n\n\nmultiplicityOff\nBoolean\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n ja\n\n\npluginFolder\nObject\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\nskipPolygonBuilding\nBoolean\nSchaltet die Bildung der Polygone aus (nur ITF). Default: false\n ja\n\n\nvalidationOk\nboolean\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false.\n nein\n\n\nvalueDelimiter\nCharacter\nZeichen, das am Anfang und Ende jeden Wertes vorhanden ist. Default \"\n ja\n\n\nvalueSeparator\nCharacter\nZeichen, das als Trennzeichen zwischen den Werten interpretiert werden soll. Default: ,\n ja\n\n\nxtflogFile\nObject\nSchreibt die log-Meldungen in eine INTERLIS-2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n ja\n\n\n\nFalls die CSV-Datei eine Header-Zeile enthält (mit den Spaltennamen), wird im gegebenen Modell eine Klasse gesucht, welche die Header-Spaltennamen enthält (Gross-/Kleinschreibung sowie optionale “Spalten” der Modell-Klasse werden ignoriert). Wird keine solche Klasse gefunden, gilt das als Validierungsfehler.\nFalls die CSV-Datei keine Header-Zeile enthält (mit den Spaltennamen), wird im gegebenen Modell eine Klasse gesucht, die die selbe Anzahl Attribute hat. Wird keine solche Klasse gefunden, gilt das als Validierungsfehler.\nDie Prüfung von gleichzeitig mehreren CSV-Dateien führt zu Fehlermeldungen wie OID o3158 of object &lt;Modelname&gt;.&lt;Topicname&gt;.&lt;Klassenname&gt; already exists in .... Beim Öffnen und Lesen einer CSV-Datei wird immer der Zähler, der die interne (in der CSV-Datei nicht vorhandene) OID generiert, zurückgesetzt. Somit kann immer nur eine CSV-Datei pro Task geprüft werden.\n\n\nCurl\nSimuliert mit einem HttpClient einige Curl-Befehle.\nBeispiele:\nimport ch.so.agi.gretl.tasks.Curl.MethodType;\n\ntasks.register('uploadData', Curl) {\n    serverUrl = \"https://geodienste.ch/data_agg/interlis/import\"\n    method = MethodType.POST\n    formData = [\"topic\": \"npl_waldgrenzen\", \"lv95_file\": file(\"./test.xtf.zip\"), \"publish\": \"true\", \"replace_all\": \"true\"]\n    user = \"fooUser\"\n    password = \"barPwd\"\n    expectedStatusCode = 200\n    expectedBody = \"\\\"success\\\":true\"\n}\nimport ch.so.agi.gretl.tasks.Curl.MethodType;\n\ntasks.register('uploadData', Curl) {\n    serverUrl = \"https://testweb.so.ch/typo3/api/digiplan\"\n    method = MethodType.POST\n    headers = [\"Content-Type\": \"application/xml\", \"Content-Encoding\": \"gzip\"]\n    dataBinary = file(\"./planregister.xml.gz\")\n    user = \"fooUser\"\n    password = \"barPwd\"\n    expectedStatusCode = 202\n}\nimport ch.so.agi.gretl.tasks.Curl.MethodType;\n\ntasks.register('downloadData', Curl) {\n    serverUrl = \"https://raw.githubusercontent.com/sogis/gretl/master/README.md\"\n    method = MethodType.GET\n    outputFile = file(\"./README.md\")\n    expectedStatusCode = 200\n}\nimport groovy.json.JsonSlurper\n\ndef basicHeader = \"Basic \" + Base64.getEncoder().encodeToString((clientId + \":\" + clientSecret).getBytes())\ndef token = \"\"\n\ntasks.register('getToken', Curl) {\n    serverUrl = \"https://auth.example.ch/oauth2/token\"\n    method = MethodType.POST\n    outputFile = file(\"./token.json\")\n    expectedStatusCode = 200\n    data = \"grant_type=client_credentials&client_id=$clientId\"\n    headers = [\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n        \"Authorization\": basicHeader\n    ]\n\n    doLast {\n        def slurper = new JsonSlurper();\n        def slurped = slurper.parseText(outputFile.text)\n\n        if (slurped.access_token != null)\n            token = slurped.access_token\n\n        if(token.length() == 0)\n            throw new GradleException(\"Failed to retrieve access token\")\n\n        println \"Length of access token is: \" + token.length()\n    }\n}\n\ntasks.register('downloadJson', Curl) {\n    dependsOn 'getToken'\n    serverUrl = \"https://obs.example.ch/rest/v4/docs.json?projects=83505\"\n    method = MethodType.GET\n    outputFile = file(\"./data.json\")\n    expectedStatusCode = 200\n    headers.put(\"Authorization\", providers.provider { \"Bearer \" + token })\n} \n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndata\nString\nString, der via POST hochgeladen werden soll. Entspricht curl [URL] --data.\n ja\n\n\ndataBinary\nFile\nDatei, die hochgeladen werden soll. Entspricht curl [URL] --data-binary.\n ja\n\n\nexpectedBody\nString\nErwarteter Text, der vom Server als Body zurückgelieferd wird.\n ja\n\n\nexpectedStatusCode\nInteger\nErwarteter Status Code, der vom Server zurückgeliefert wird.\n nein\n\n\nformData\nMap&lt;String,Object&gt;\nForm data parameters. Entspricht curl [URL] -F key1=value1 -F file1=@my_file.xtf.\n ja\n\n\nheaders\nMapProperty&lt;String,String&gt;\nRequest-Header. Entspricht curl [URL] -H ... -H .....\n ja\n\n\nmethod\nMethodType\nHTTP-Request-Methode. Unterstützt werden GET und POST.\n ja\n\n\noutputFile\nFile\nDatei, in die der Output gespeichert wird. Entspricht curl [URL] -o.\n ja\n\n\npassword\nString\nPasswort. Wird zusammen mit user in einen Authorization-Header umgewandelt. Entspricht curl [URL] -u user:password.\n ja\n\n\nserverUrl\nString\nDie URL des Servers inklusive Pfad und Queryparameter.\n nein\n\n\nuser\nString\nBenutzername. Wird zusammen mit password in einen Authorization-Header umgewandelt. Entspricht curl [URL] -u user:password.\n ja\n\n\n\n\n\nDatabaseDocumentExport (deprecated)\nSpeichert Dokumente, deren URL in einer Spalte einer Datenbanktabelle gespeichert sind, in einem lokalen Verzeichnis. Zukünftig und bei Bedarf kann der Task so erweitert werden, dass auch BLOBs aus der Datenbank gespeichert werden können.\nRedirect von HTTP nach HTTPS funktionieren nicht. Dies korrekterweise (?) wegen der verwendeten Java-Bibliothek.\nWegen der vom Kanton Solothurn eingesetzten self-signed Zertifikate muss ein unschöner Handstand gemacht werden. Leider kann dieser Usecase schlecht getestet werden, da die Links nur in der privaten Zone verfügbar sind und die zudem noch häufig ändern können. Manuell getestet wurde es jedoch.\nAls Dateiname wird der letzte Teil des URL-Pfades verwendet, z.B. https://artplus.verw.rootso.org/MpWeb-apSolothurnDenkmal/download/2W8v0qRZQBC0ahDnZGut3Q?mode=gis wird mit den Prefix und Extension zu ada_2W8v0qRZQBC0ahDnZGut3Q.pdf.\nEs wird DISTINCT ON (&lt;documentColumn&gt;) und ein Filter WHERE &lt;documentColumn&gt; IS NOT NULL verwendet.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('exportDocuments', DatabaseDocumentExport) {\n    database = [db_uri, db_user, db_pass]\n    qualifiedTableName = \"ada_denkmalschutz.fachapplikation_rechtsvorschrift_link\"\n    documentColumn = \"multimedia_link\"\n    targetDir = file(\".\")\n    fileNamePrefix = \"ada_\"\n    fileNameExtension = \"pdf\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndatabase\nConnector\nDatenbank, aus der die Dokumente exportiert werden sollen.\n nein\n\n\ndocumentColumn\nString\nDB-Tabellenspalte mit dem Dokument resp. der URL zum Dokument.\n nein\n\n\nfileNameExtension\nString\nDateinamen-Extension.\n ja\n\n\nfileNamePrefix\nString\nPrefix für Dateinamen.\n ja\n\n\nqualifiedTableName\nString\nQualifizierter Tabellenname.\n nein\n\n\ntargetDir\nFile\nVerzeichnis in das die Dokumente exportiert werden sollen.\n nein\n\n\n\n\n\nDb2Db\nDies ist prinzipiell ein 1:1-Datenkopie, d.h. es findet kein Datenumbau statt, die Quell- und die Ziel- Tabelle hat jeweils identische Attribute. Es werden auf Seite Quelle in der Regel also simple SELECT-Queries ausgeführt und die Resultate dieser Queries in Tabellen der Ziel-DB eingefügt. Unter bestimmten Bedingungen (insbesondere wenn es sich um einen wenig komplexen Datenumbau handelt), kann dieser Task aber auch zum Datenumbau benutzt werden.\nDie Queries können auf mehrere .sql-Dateien verteilt werden, d.h. der Task muss die Queries mehrerer .sql-Dateien zu einer Transaktion kombinieren können. Jede .sql-Datei gibt genau eine Resultset (RAM-Tabelle) zurück. Das Resultset wird in die konfigurierte Zieltabelle geschrieben. Die Beziehungen sind: Eine bis mehrere Quelltabellen ergeben ein Resultset; das Resultset entspricht bezüglich den Attributen genau der Zieltabelle und wird 1:1 in diese geschrieben. Der Db2Db- Task verarbeitet innerhalb einer Transaktion 1-n Resultsets und wird entsprechend auch mit 1-n SQL-Dateien konfiguriert.\nDie Reihenfolge der .sql-Dateien ist relevant. Dies bedeutet, dass die SQL-Befehle der zuerst angegebenen .sql-Datei zuerst ausgeführt werden müssen, danach die SQL-Befehle der an zweiter Stelle angegebenen .sql-Datei, usw.\nAlle SELECT-Statements werden in einer Transaktion ausgeführt werden, damit ein konsistenter Datenstand gelesen wird. Alle INSERT-Statements werden in einer Transaktion ausgeführt werden, damit bei einem Fehler der bisherige Datenstand bestehen bleibt und also kein unvollständiger Import zurückgelassen wird.\nDamit dieselbe .sql-Datei für verschiedene Datensätze benutzt werden kann, ist es möglich innerhalb der .sql-Datei Parameter zu verwenden und diesen Parametern beim Task einen konkreten Wert zuzuweisen. Innerhalb der .sql-Datei werden Paramter mit folgender Syntax verwendet: ${paramName}.\nEs ist pro .sql-Datei nur ein SELECT-Statement erlaubt. Ausser das erste Statement ist ein “SET search_path TO”-Statement.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('transferSomeData', Db2Db) {\n    sourceDb = [db_uri, db_user, db_pass]\n    targetDb = ['jdbc:sqlite:gretldemo.sqlite',null,null]\n    sqlParameters = [dataset:'Olten']\n    transferSets = [\n        new TransferSet('some.sql', 'albums_dest', true)\n    ];\n}\nDamit mit einer einzigen Task-Definition mehrere Datensätze verarbeitet werden können, kann auch eine Liste von Parametern angegeben werden.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('transferSomeData', Db2Db) {\n    sourceDb = [db_uri, db_user, db_pass]\n    targetDb = ['jdbc:sqlite:gretldemo.sqlite',null,null]\n    sqlParameters = [[dataset:'Olten'],[dataset:'Grenchen']]\n    transferSets = [\n        new TransferSet('some.sql', 'albums_dest', true)\n    ];\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbatchSize\nProperty&lt;Integer&gt;\nAnzahl der Records, die pro Batch in die Ziel-Datenbank geschrieben werden (Default: 5000). Für sehr grosse Tabellen muss ein kleinerer Wert gewählt werden.\n ja\n\n\nfetchSize\nProperty&lt;Integer&gt;\nAnzahl der Records, die auf einmal vom Datenbank-Cursor von der Quell-Datenbank zurückgeliefert werden (Standard: 5000). Für sehr grosse Tabellen muss ein kleinerer Wert gewählt werden.\n ja\n\n\nsourceDb\nListProperty&lt;String&gt;\nDatenbank, aus der gelesen werden soll.\n nein\n\n\nsqlParameters\nProperty&lt;Object&gt;\nEine Map mit Paaren von Parameter-Name und Parameter-Wert (Map&lt;String,String&gt;). Oder eine Liste mit Paaren von Parameter-Name und Parameter-Wert (List&lt;Map&lt;String,String&gt;&gt;).\n ja\n\n\ntargetDb\nListProperty&lt;String&gt;\nDatenbank, in die geschrieben werden soll.\n nein\n\n\ntransferSets\nListProperty&lt;TransferSet&gt;\nEine Liste von TransferSets.\n nein\n\n\n\nEine TransferSet ist\n\neine SQL-Datei (mit SQL-Anweisungen zum Lesen der Daten aus der sourceDb),\ndem Namen der Ziel-Tabelle in der targetDb, und\nder Angabe ob in der Ziel-Tabelle vor dem INSERT zuerst alle Records gelöscht werden sollen.\neinem optionalen vierten Parameter, der verwendet werden kann um den zu erzeugenden SQL-Insert-String zu beeinflussen, u.a. um einen WKT-Geometrie-String in eine PostGIS-Geometrie umzuwandeln\n\nBeispiel, Umwandlung Rechtswert/Hochwertspalten in eine PostGIS-Geometrie (siehe auch GRETL-Job afu_onlinerisk_transfer der eine Punktgeometriespalten aus einer Nicht-Postgis-DB übernimmt):\nnew TransferSet('untersuchungseinheit.sql', 'afu_qrcat_v1.onlinerisk_untersuchungseinheit', true, (String[])[\"geom:wkt:2056\"])\ngeom ist der Geometrie-Spalten-Name der verwendet wird.\nDazugehöriger Auszug aus SQL-Datei zur Erzeugung des WKT-Strings mit Hilfe von concatenation:\n'Point(' || ue.koordinate_x::text || ' ' || ue.koordinate_y::text || ')' AS geom\nUnterstützte Datenbanken: PostgreSQL, SQLite, Oracle, Derby und DuckDB.\n\n\nFtpDelete\nLöscht Daten auf einem FTP-Server.\nBeispiel, löscht alle Daten in einem Verzeichnis:\ntasks.register('ftpdelete', FtpDelete) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir = \"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile = fileTree(pathToTempFolder) { include '*.zip' } \n    //remoteFile = \"*.zip\"\n}\nUm bestimmte Dateien zu löschen:\ntasks.register('ftpdelete', FtpDelete) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir = \"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile = \"*.zip\"\n}\nUm heruntergeladene Daten zu löschen:\ntasks.register('ftpdelete', FtpDelete) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir = \"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile = fileTree(pathToDownloadFolder) { include '*.zip' } \n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncontrolKeepAliveTimeout\nLong\nTimeout bis ein NOOP über den Kontroll-Kanal versendet wird. Default: 300s (=5 Minuten)\n ja\n\n\nfileSeparator\nString\nDefault: /. (Falls systemType Windows ist, ist der Default \\.\n ja\n\n\npassiveMode\nBoolean\nAktiv- oder Passiv-Verbindungsmodus. Default: Passiv (true)\n ja\n\n\npassword\nString\nPasswort für den Zugriff auf dem Server.\n nein\n\n\nremoteDir\nString\nVerzeichnis auf dem Server.\n nein\n\n\nremoteFile\nObject\nDateiname oder Liste der Dateinamen auf dem Server (kann auch ein Muster sein (* oder ?)). Ohne diesen Parameter werden alle Dateien aus dem Remoteverzeichnis gelöscht.\n ja\n\n\nserver\nString\nName des Servers (ohne ftp://).\n nein\n\n\nsystemType\nString\nUNIX oder WINDOWS. Default: UNIX.\n ja\n\n\nuser\nString\nBenutzername auf dem Server.\n nein\n\n\n\n\n\nFtpDownload\nLädt alle Dateien aus dem definierten Verzeichnis des Servers in ein lokales Verzeichnis herunter.\nBeispiel:\ntasks.register('ftpdownload', FtpDownload) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    localDir= \"downloads\"\n    remoteDir= \"\"\n}\nUm eine bestimmte Datei herunterzuladen:\ntasks.register('ftpdownload', FtpDownload) {\n    server=\"ftp.infogrips.ch\"\n    user= \"Hans\"\n    password= \"dummy\"\n    systemType=\"WINDOWS\"\n    localDir= \"downloads\"\n    remoteDir=\"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile=\"240100.zip\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncontrolKeepAliveTimeout\nLong\nTimeout bis ein NOOP über den Kontroll-Kanal versendet wird. Default: 300s (=5 Minuten)\n ja\n\n\nfileSeparator\nString\nDefault: /. (Falls systemType Windows ist, ist der Default \\.\n ja\n\n\nfileType\nString\nASCII oder BINARY. Default: ASCII.\n ja\n\n\nlocalDir\nString\nLokales Verzeichnis, in dem die Dateien gespeichert werden.\n nein\n\n\npassiveMode\nBoolean\nAktiv- oder Passiv-Verbindungsmodus. Default: Passiv (true)\n ja\n\n\npassword\nString\nPasswort für den Zugriff auf dem Server.\n nein\n\n\nremoteDir\nString\nVerzeichnis auf dem Server.\n nein\n\n\nremoteFile\nObject\nDateiname oder Liste der Dateinamen auf dem Server (kann auch ein Muster sein (* oder ?)). Ohne diesen Parameter werden alle Dateien aus dem Remoteverzeichnis heruntergeladen.\n ja\n\n\nserver\nString\nName des Servers (ohne ftp://).\n nein\n\n\nsystemType\nString\nUNIX oder WINDOWS. Default: UNIX.\n ja\n\n\nuser\nString\nBenutzername auf dem Server.\n nein\n\n\n\n\n\nFtpList\nLiefert eine Liste der Dateien aus dem definierten Verzeichnis des Servers.\nBeispiel:\ntasks.register('ftplist', FtpList) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir= \"\"\n    doLast {\n        println files\n    }\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncontrolKeepAliveTimeout\nLong\nTimeout bis ein NOOP über den Kontroll-Kanal versendet wird. Default: 300s (=5 Minuten)\n ja\n\n\nfileSeparator\nString\nDefault: /. (Falls systemType Windows ist, ist der Default \\.\n ja\n\n\npassiveMode\nBoolean\nAktiv- oder Passiv-Verbindungsmodus. Default: Passiv (true)\n ja\n\n\npassword\nString\nPasswort für den Zugriff auf dem Server.\n nein\n\n\nremoteDir\nString\nVerzeichnis auf dem Server.\n nein\n\n\nserver\nString\nName des Servers (ohne ftp://).\n nein\n\n\nsystemType\nString\nUNIX oder WINDOWS. Default: UNIX.\n ja\n\n\nuser\nString\nBenutzername auf dem Server.\n nein\n\n\n\n\n\nGpkg2Dxf\nExportiert alle Tabellen einer GeoPackage-Datei in DXF-Dateien. Als Input wird eine von ili2gpkg erzeugte GeoPackage-Datei benötigt.\nEs werden alle INTERLIS-Klassen exportiert (SELECT tablename FROM T_ILI2DB_TABLE_PROP WHERE setting = 'CLASS'). Der eigentliche SELECT-Befehl ist komplizierter weil für das Layern der einzelnen DXF-Objekte das INTERLIS-Metaattribut !!@dxflayer=\"true\" ausgelesen wird. Gibt es kein solches Metaattribut wird alles in den gleichen DXF-Layer (default) geschrieben.\nEncoding: Die DXF-Dateien sind ISO-8859-1 encodiert.\nAchtung: Task sollte verbessert werden (siehe E-Mail Claude im Rahmen des Publisher-Projektes).\ntasks.register('gpkg2dxf', Gpkg2Dxf) {\n    dataFile = file(\"data.gpkg\")\n    outputDir = file(\"./out/\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nFile\nGeoPackage-Datei, die nach DXF transformiert werden soll.\n nein\n\n\noutputDir\nFile\nVerzeichnis, in das die DXF-Dateien gespeichert werden.\n nein\n\n\n\n\n\nGpkg2Shp\nExportiert alle Tabellen einer GeoPackage-Datei in Shapefiles. Als Input wird eine von ili2gpkg erzeugte GeoPackage-Datei benötigt.\nEs werden alle INTERLIS-Klassen exportiert (SELECT tablename FROM T_ILI2DB_TABLE_PROP WHERE setting = 'CLASS'). Je nach, bei der Erstellung der GeoPackage-Datei, verwendeten Parametern, muss die Query angepasst werden. Es muss jedoch darauf geachtet werden, dass es nur eine Query gibt (für alle Datensätze). Für den vorgesehenen Anwendungsfall (sehr einfache, flache Modelle) dürfte das kein Problem darstellen.\nEncoding: Die Shapefiles sind neu UTF-8 encodiert. Standard ist ISO-8859-1, scheint aber v.a. in QGIS nicht standardmässig zu funktionieren (keine Umlaute).\nAchtung: Task sollte verbessert werden (siehe E-Mail Claude im Rahmen des Publisher-Projektes).\ntasks.register('gpkg2shp', Gpkg2Shp) {\n    dataFile = file(\"data.gpkg\")\n    outputDir = file(\"./out/\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nFile\nGeoPackage-Datei, die nach Shapefile transformiert werden soll.\n nein\n\n\noutputDir\nFile\nVerzeichnis, in das die Shapefile gespeichert werden.\n nein\n\n\n\n\n\nGpkgExport\nDaten aus einer bestehenden Datenbanktabelle werden in eine GeoPackage-Datei exportiert.\nBeispiele:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('gpkgexport', GpkgExport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"gpkgexport\"\n    srcTableName = \"exportdata\"\n    dataFile = \"data.gpkg\"\n    dstTableName = \"exportdata\"\n}\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('gpkgexport', GpkgExport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"gpkgexport\"\n    srcTableName = [\"exportTable1\", \"exportTable2\"]\n    dataFile = \"data.gpkg\"\n    dstTableName = [\"exportTable1\", \"exportTable2\"]\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbatchSize\nInteger\nAnzahl der Records, die pro Batch in die Ziel-Datenbank (GeoPackage) geschrieben werden. Default: 5000\n ja\n\n\ndataFile\nObject\nName der GeoPackage-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndstTableName\nObject\nName der Tabelle(n) in der GeoPackage-Datei. String oder List.\n nein\n\n\nfetchSize\nInteger\nAnzahl der Records, die pro Fetch aus der Quell-Datenbank gelesen werden. Default: 5000\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\nsrcTableName\nObject\nName der DB-Tabelle(n), die exportiert werden soll(en). String oder List.\n nein\n\n\n\n\n\nGpkgImport\nDaten aus einer GeoPackage-Datei in eine bestehende Datenbanktabelle importieren.\nDie Tabelle kann weitere Spalten enthalten, die in der GeoPackage-Datei nicht vorkommen. Sie müssen aber NULLable sein, oder einen Default-Wert definiert haben.\nDie Gross-/Kleinschreibung der GeoPackage-Spaltennamen wird für die Zuordnung zu den DB-Spalten ignoriert.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('gpkgimport', GpkgImport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"gpkgimport\"\n    srcTableName = \"Point\"\n    dstTableName = \"importdata\"\n    dataFile = \"point.gpkg\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbatchSize\nInteger\nAnzahl der Records, die pro Batch in die Ziel-Datenbank geschrieben werden. Default: 5000\n ja\n\n\ndataFile\nObject\nName der GeoPackage-Datei, die gelesen werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, in die importiert werden soll.\n nein\n\n\ndstTableName\nString\nName der DB-Tabelle, in die importiert werden soll.\n nein\n\n\nfetchSize\nInteger\nAnzahl der Records, die pro Fetch aus der Quell-Datenbank gelesen werden. Default: 5000\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\nsrcTableName\nString\nName der GeoPackage-Tabelle, die importiert werden soll.\n nein\n\n\n\n\n\nGpkgValidator\nPrüft eine GeoPackage-Datei gegenüber einem INTERLIS-Modell. Basiert auf dem ilivalidator.\nBeispiel:\ntasks.register('validate', GpkgValidator) {\n    models = \"GpkgModel\"\n    dataFiles = [\"attributes.gpkg\"]\n    tableName = \"Attributes\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nallObjectsAccessible\nBoolean\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n ja\n\n\nconfigFile\nObject\nKonfiguriert die Datenprüfung mit Hilfe einer ini-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\ndataFiles\nObject\nListe der Dateien, die validiert werden sollen. FileCollection oder List. Eine leere Liste ist kein Fehler.\n nein\n\n\ndisableAreaValidation\nBoolean\nSchaltet die AREA-Topologieprüfung aus. Default: false\n ja\n\n\nfailOnError\nBoolean\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n ja\n\n\nforceTypeValidation\nBoolean\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n ja\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n ja\n\n\nmetaConfigFile\nObject\nKonfiguriert den Validator mit Hilfe einer ini-Datei. Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell, gegen das die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Der Name der CSV-Datei.\n ja\n\n\nmultiplicityOff\nBoolean\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n ja\n\n\npluginFolder\nObject\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\nskipPolygonBuilding\nBoolean\nSchaltet die Bildung der Polygone aus (nur ITF). Default: false\n ja\n\n\ntableName\nString\nName der Tabelle in den GeoPackage-Dateien.\n nein\n\n\nvalidationOk\nboolean\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false.\n nein\n\n\nxtflogFile\nObject\nSchreibt die log-Meldungen in eine INTERLIS-2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n ja\n\n\n\n\n\nGzip\nGzipped eine einzelne Datei. Es gibt einen eingebauten Tar-Task, der - nomen est omen - aber immer zuerst eine Tar-Datei erstellt.\nBeispiel:\ntasks.register('compressFile', Gzip) {\n    dataFile = file(\"./planregister.xml\");\n    gzipFile = file(\"./planregister.xml.gz\");\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nProperty&lt;File&gt;\nDatei, die gezipped werden soll.\n nein\n\n\ngzipFile\nProperty&lt;File&gt;\nOutput-Datei\n nein\n\n\n\n\n\nIli2gpkgImport\nImportiert Daten aus einer INTERLIS-Transferdatei in eine GeoPackage-Datei.\nDie Tabellen werden implizit auch angelegt, falls sie noch nicht vorhanden sind. Falls die Tabellen in der Datenbank schon vorhanden sind, können sie zusätzliche Spalten enthalten (z.B. bfsnr, datum etc.), welche beim Import leer bleiben.\nFalls beim Import ein Datensatz-Identifikator (dataset) definiert wird, darf dieser Datensatz-Identifikator in der Datenbank noch nicht vorhanden sein.\nUm die bestehenden (früher importierten) Daten zu ersetzen, kann der Task Ili2gpkgReplace (not yet implemented) verwendet werden.\nDie Option --doSchemaImport wird automatisch gesetzt.\nBeispiel:\ntasks.register('importData', Ili2gpkgImport) {\n    models = \"SO_AGI_AV_GB_Administrative_Einteilungen_20180613\"\n    dataFile = file(\"data.xtf\");\n    dbfile = file(\"data.gpkg\")    \n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nString\nEntspricht der ili2gpkg-Option --baskets\n ja\n\n\ndataFile\nObject\nName der XTF-/ITF-Datei, die gelesen werden soll. Es können auch mehrere Dateien sein. FileCollection oder File\n nein\n\n\ndataset\nObject\nEntspricht der ili2gpkg-Option --dataset\n ja\n\n\ndbfile\nFile\nGeoPackage-Datei in die importiert werden soll.\n nein\n\n\ndefaultSrsCode\nString\nEntspricht der ili2gpkg-Option --defaultSrsCode\n ja\n\n\nisCoalesceJson\nBoolean\nEntspricht der ili2gpkg-Option --coalesceJson\n ja\n\n\nisCreateEnumTabs\nBoolean\nEntspricht der ili2gpkg-Option --createEnumTabs\n ja\n\n\nisCreateGeomIdx\nBoolean\nEntspricht der ili2gpkg-Option --createGeomIdx\n ja\n\n\nisCreateMetaInfo\nBoolean\nEntspricht der ili2gpkg-Option --createMetaInfo\n ja\n\n\nisDeleteData\nBoolean\nEntspricht der ili2gpkg-Option --deleteData\n ja\n\n\nisDisableAreaValidation\nBoolean\nEntspricht der ili2gpkg-Option --disableAreaValidation\n ja\n\n\nisDisableValidation\nBoolean\nEntspricht der ili2gpkg-Option --disableValidation\n ja\n\n\nisForceTypeValidation\nBoolean\nEntspricht der ili2gpkg-Option --forceTypeValidation\n ja\n\n\nisIligml20\nBoolean\nEntspricht der ili2gpkg-Option --iligml20\n ja\n\n\nisImportTid\nBoolean\nEntspricht der ili2gpkg-Option --importTid\n ja\n\n\nisNameByTopic\nBoolean\nEntspricht der ili2gpkg-Option --nameByTopic\n ja\n\n\nisSkipGeometryErrors\nBoolean\nEntspricht der ili2gpkg-Option --skipGeometryErrors\n ja\n\n\nisSkipPolygonBuilding\nBoolean\nEntspricht der ili2gpkg-Option --skipPolygonBuilding\n ja\n\n\nisStrokeArcs\nBoolean\nEntspricht der ili2gpkg-Option --strokeArcs\n ja\n\n\nisTrace\nBoolean\nEntspricht der ili2gpkg-Option --trace\n ja\n\n\nlogFile\nObject\nEntspricht der ili2gpkg-Option --logFile\n ja\n\n\nmodeldir\nString\nEntspricht der ili2gpkg-Option --modeldir\n ja\n\n\nmodels\nString\nEntspricht der ili2gpkg-Option --models\n ja\n\n\npostScript\nFile\nEntspricht der ili2gpkg-Option --postScript\n ja\n\n\npreScript\nFile\nEntspricht der ili2gpkg-Option --preScript\n ja\n\n\nproxy\nString\nEntspricht der ili2gpkg Option --proxy\n ja\n\n\nproxyPort\nInteger\nEntspricht der ili2gpkg-Option --proxyPort\n ja\n\n\ntopics\nString\nEntspricht der ili2gpkg-Option --topics\n ja\n\n\nvalidConfigFile\nFile\nEntspricht der ili2gpkg-Option --validConfigFile\n ja\n\n\n\nFür die Beschreibung der einzenen ili2gpkg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIli2pgDelete\nLöscht einen Datensatz in der PostgreSQL-Datenbank anhand eines Datensatz-Identifikators. Diese Funktion bedingt, dass das Datenbankschema mit der Option createBasketCol erstellt wurde (via Task Ili2pgImportSchema). Der Parameter failOnException muss false sein, ansonsten bricht der Job ab.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('deleteDataset', Ili2pgDelete) {\n    database = [db_uri, db_user, db_pass]\n    models = \"DM01AVSO24LV95\"\n    dbschema = \"dm01\"\n    dataset = \"kammersrohr\"\n}\nEs können auch mehrere Datensätze pro Task gelöscht werden:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('deleteDataset', Ili2pgDelete) {\n    database = [db_uri, db_user, db_pass]\n    dbschema = \"dm01\"\n    dataset = [\"Olten\",\"Grenchen\"]\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIli2pgExport\nExportiert Daten aus der PostgreSQL-Datenbank in eine INTERLIS-Transferdatei.\nMit dem Parameter models, topics, baskets oder dataset wird definiert, welche Daten exportiert werden.\nOb die Daten im INTERLIS-1-, INTERLIS-2- oder GML-Format geschrieben werden, ergibt sich aus der Dateinamenserweiterung der Ausgabedatei. Für eine INTERLIS-1-Transferdatei muss die Erweiterung .itf verwendet werden. Für eine GML-Transferdatei muss die Erweiterung .gml verwendet werden.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('exportData', Ili2pgExport) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900-out.itf\"\n    dataset = \"254900\"\n    logFile = \"ili2pg.log\"\n}\nDamit mit einer einzigen Task-Definition mehrere Datensätze verarbeitet werden können, kann auch eine Liste von Dateinamen und Datensätzen angegeben werden.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('exportData', Ili2pgExport) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = [\"lv03_254900-out.itf\",\"lv03_255000-out.itf\"]\n    dataset = [\"254900\",\"255000\"]\n    logFile = \"ili2pg.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexport3\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --export3.\n ja\n\n\nexportModels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --exportModels.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIli2pgImport\nImportiert Daten aus einer INTERLIS-Transferdatei in die PostgreSQL-Datenbank.\nDie Tabellen werden implizit auch angelegt, falls sie noch nicht vorhanden sind. Falls die Tabellen in der Datenbank schon vorhanden sind, können sie zusätzliche Spalten enthalten (z.B. bfsnr, datum etc.), welche beim Import leer bleiben.\nFalls beim Import ein Datensatz-Identifikator (dataset) definiert wird, darf dieser Datensatz-Identifikator in der Datenbank noch nicht vorhanden sein.\nFalls man mehrere Dateien importieren will, diese jedoch erst zur Laufzeit eruiert werden können, muss der Parameter dataFile eine Gradle FileCollection resp. eine implementierende Klasse (z.B. FileTree) sein. Gleiches gilt für den dataset-Parameter. Als einzelner Wert für das Dataset wird in diesem Fall der Name der Datei ohne Extension und ohne Pfad verwendet. Leider kann nicht bereits in der Task-Definition aus dem Filetree eine Liste gemacht werden, z.B. fileTree(pathToUnzipFolder) { include '*.itf' }.files.name. Diese Liste ist leer.\nUm die bestehenden (früher importierten) Daten zu ersetzen, kann der Task Ili2pgReplace verwendet werden.\nBeispiel 1:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('importData', Ili2pgImport) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900.itf\"\n    logFile = \"ili2pg.log\"\n}\nBeispiel 2:\nImport der AV-Daten. In der t_datasetname-Spalte soll die BFS-Nummer stehen. Die BFS-Nummer entspricht den ersten vier Zeichen des Filenamens.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('importData', Ili2pgImport) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = fileTree(pathToUnzipFolder) { include '*.itf' }\n    dataset = dataFile\n    datasetSubstring = (0..4).toList()\n    logFile = \"ili2pg.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndataFile\nProperty&lt;Object&gt;\nName der XTF-/ITF-Datei, die gelesen werden soll. Es können auch mehrere Dateien sein. FileCollection oder String.\n nein\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIli2pgImportSchema\nErstellt die Tabellenstruktur in der PostgreSQL-Datenbank anhand eines INTERLIS-Modells.\nDer Parameter iliFile oder models muss gesetzt werden.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('importSchema', Ili2pgImportSchema) {\n    database = [db_uri, db_user, db_pass]\n    models = \"DM01AVSO24\"\n    dbschema = \"gretldemo\"\n    logFile = \"ili2pg.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\nbeautifyEnumDispName\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --beautifyEnumDispName.\n ja\n\n\ncoalesceArray\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceArray.\n ja\n\n\ncoalesceCatalogueRef\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceCatalogueRef.\n ja\n\n\ncoalesceJson\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceJson.\n ja\n\n\ncoalesceMultiLine\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceMultiline.\n ja\n\n\ncoalesceMultiSurface\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceMultiSurface.\n ja\n\n\ncreateBasketCol\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createBasketCol.\n ja\n\n\ncreateDatasetCol\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createDatasetCol.\n ja\n\n\ncreateDateTimeChecks\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createDateTimeChecks.\n ja\n\n\ncreateEnumColAsItfCode\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createEnumColAsItfCode.\n ja\n\n\ncreateEnumTabs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createEnumTabs.\n ja\n\n\ncreateEnumTabsWithId\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createEnumTabsWithId.\n ja\n\n\ncreateEnumTxtCol\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createEnumTxtCol.\n ja\n\n\ncreateFk\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createFk.\n ja\n\n\ncreateFkIdx\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createFkIdx.\n ja\n\n\ncreateGeomIdx\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createGeomIdx.\n ja\n\n\ncreateImportTabs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createImportTabs.\n ja\n\n\ncreateMetaInfo\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createMetaInfo.\n ja\n\n\ncreateNumChecks\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createNumChecks.\n ja\n\n\ncreateSingleEnumTab\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createSingleEnumTab.\n ja\n\n\ncreateStdCols\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createStdCols.\n ja\n\n\ncreateTextChecks\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createTextChecks.\n ja\n\n\ncreateTidCol\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createTidCol.\n ja\n\n\ncreateTypeConstraint\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createTypeConstraint.\n ja\n\n\ncreateTypeDiscriminator\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createTypeDescriminator.\n ja\n\n\ncreateUnique\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createUnique.\n ja\n\n\ncreatescript\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --createscript.\n ja\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndefaultSrsAuth\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --defaultSrsAuth.\n ja\n\n\ndefaultSrsCode\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --defaultSrsCode.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableNameOptimization\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableNameOptimization.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\ndropscript\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dropscript.\n ja\n\n\nexpandMultilingual\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --expandMultilingual.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\nidSeqMax\nProperty&lt;Long&gt;\nEntspricht der ili2pg-Option --idSeqMax.\n ja\n\n\nidSeqMin\nProperty&lt;Long&gt;\nEntspricht der ili2pg-Option --idSeqMin.\n ja\n\n\niliFile\nProperty&lt;Object&gt;\nName der ili-Datei, die gelesen werden soll.\n ja\n\n\niliMetaAttrs\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --iliMetaAttrs.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nkeepAreaRef\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --keepAreaRef.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmaxNameLength\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --maxNameLength.\n ja\n\n\nmetaConfig\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --metaConfig.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\nnameByTopic\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --nameByTopic.\n ja\n\n\nnoSmartMapping\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --noSmartMapping.\n ja\n\n\noneGeomPerTable\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --oneGeomPerTable.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nsetupPgExt\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --setupPgExt.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nsmart1Inheritance\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --smart1Inheritance.\n ja\n\n\nsmart2Inheritance\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --smart2Inheritance.\n ja\n\n\nsqlColsAsText\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --sqlColsAsText.\n ja\n\n\nsqlEnableNull\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --sqlEnableNull.\n ja\n\n\nsqlExtRefCols\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --sqlExtRefCols.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\nt_id_Name\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --t_id_Name.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\ntranslation\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --translation.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIli2pgReplace\nErsetzt die Daten in der PostgreSQL-Datenbank anhand eines Datensatz-Identifikators (dataset) mit den Daten aus einer INTERLIS-Transferdatei. Diese Funktion bedingt, dass das Datenbankschema mit der Option createBasketCol erstellt wurde (via Task Ili2pgImportSchema).\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('replaceData', Ili2pgReplace) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900.itf\"\n    dataset = \"254900\"\n    logFile = \"ili2pg.log\"\n}\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('importData', Ili2pgReplace) {\n    database = [db_uri, db_user, db_pass]\n    models = 'DM01AVSO24LV95'\n    dbschema = 'agi_dm01avso24'\n    dataFile = fileTree(dir: dm01SoDir, include: '*.itf')\n    dataset = dataFile\n    datasetSubstring = 0..4\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndataFile\nProperty&lt;Object&gt;\nnull\n nein\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIli2pgUpdate\nAktualisiert die Daten in der PostgreSQL-Datenbank anhand einer INTERLIS-Transferdatei, d.h. neue Objekte werden eingefügt, bestehende Objekte werden aktualisiert und in der Transferdatei nicht mehr vorhandene Objekte werden gelöscht.\nDiese Funktion bedingt, dass das Datenbankschema mit der Option --createBasketCol erstellt wurde (via Task Ili2pgImportSchema), und dass die Klassen und Topics eine stabile OID haben.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('updateData', Ili2pgUpdate) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900.itf\"\n    dataset = \"254900\"\n    logFile = \"ili2pg.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndataFile\nProperty&lt;Object&gt;\nnull\n nein\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIli2pgValidate\nPrüft die Daten ohne diese in eine Datei zu exportieren. Der Task ist erfolgreich, wenn keine Fehler gefunden werden und ist nicht erfolgreich, wenn Fehler gefunden werden. Mit der Option failOnException=false ist der Task erfolgreich, auch wenn Fehler gefunden werden.\nMit dem Parameter --models, --topics, --baskets oder --dataset wird definiert, welche Daten geprüft werden. Parameter --dataset akzeptiert auch eine Liste von Datasets.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('validate', Ili2pgValidate) {\n    database = [db_uri, db_user, db_pass]\n    models = \"SO_AGI_AV_GB_Administrative_Einteilungen_20180613\"\n    modeldir = rootProject.projectDir.toString() + \";http://models.interlis.ch\"\n    dbschema = \"agi_av_gb_admin_einteilungen_fail\"\n    logFile = file(\"fubar.log\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIliValidator\nPrüft eine INTERLIS-Datei (.itf oder .xtf) gegenüber einem INTERLIS-Modell (.ili). Basiert auf dem ilivalidator.\nBeispiel:\ntasks.register('validate', IliValidator) {\n    dataFiles = [\"Beispiel2a.xtf\"]\n    logFile = \"ilivalidator.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nallObjectsAccessible\nBoolean\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n ja\n\n\nconfigFile\nObject\nKonfiguriert die Datenprüfung mit Hilfe einer ini-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\ndataFiles\nObject\nListe der Dateien, die validiert werden sollen. FileCollection oder List. Eine leere Liste ist kein Fehler.\n nein\n\n\ndisableAreaValidation\nBoolean\nSchaltet die AREA-Topologieprüfung aus. Default: false\n ja\n\n\nfailOnError\nBoolean\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n ja\n\n\nforceTypeValidation\nBoolean\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n ja\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n ja\n\n\nmetaConfigFile\nObject\nKonfiguriert den Validator mit Hilfe einer ini-Datei. Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell, gegen das die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Der Name der CSV-Datei.\n ja\n\n\nmultiplicityOff\nBoolean\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n ja\n\n\npluginFolder\nObject\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\nskipPolygonBuilding\nBoolean\nSchaltet die Bildung der Polygone aus (nur ITF). Default: false\n ja\n\n\nvalidationOk\nboolean\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false.\n nein\n\n\nxtflogFile\nObject\nSchreibt die log-Meldungen in eine INTERLIS-2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n ja\n\n\n\nZusatzfunktionen (Custom Functions): Die pluginFolder-Option ist zum jetzigen Zeitpunkt ohne Wirkung. Die Zusatzfunktionen werden als normale Abhängigkeit definiert und in der ilivalidator-Task-Implementierung registriert. Das Laden der Klassen zur Laufzeit in iox-ili hat nicht funktioniert (NoClassDefFoundError…). Der Plugin-Mechanismus von ilivalidator wird momentan ohnehin geändert (“Ahead-Of-Time-tauglich” gemacht).\n\n\nJsonImport\nDaten aus einer Json-Datei in eine Datenbanktabelle importieren. Die gesamte Json-Datei (muss UTF-8 encoded sein) wird als Text in eine Spalte importiert. Ist das Json-Objekt in der Datei ein Top-Level-Array wird für jedes Element des Arrays ein Record in der Datenbanktabelle erzeugt.\nBeispiel:\ntasks.register('importJson', JsonImport) {\n    database = [db_uri, db_user, db_pass]\n    jsonFile = \"data.json\"\n    qualifiedTableName = \"jsonimport.jsonarray\"\n    columnName = \"json_text_col\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncolumnName\nString\nSpaltenname der Tabelle, in die importiert werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, in die importiert werden soll.\n nein\n\n\nisDeleteAllRows\nBoolean\nInhalt der Tabelle vorgängig löschen?\n ja\n\n\njsonFile\nString\nJSON-Datei, die importiert werden soll.\n nein\n\n\nqualifiedTableName\nString\nQualifizierter Namen der Tabelle (“schema.tabelle”), in die importiert werden soll.\n nein\n\n\n\n\n\nPostgisRasterExport\nExportiert eine PostGIS-Raster-Spalte in eine Raster-Datei mittels SQL-Query. Die SQL-Query darf nur einen Record zurückliefern, d.h. es muss unter Umständen ST_Union() verwendet werden. Es wird angenommen, dass die erste bytea-Spalte des Resultsets die Rasterdaten enthält. Weitere bytea-Spalten werden ignoriert. Das Outputformat und die Formatoptionen müssen in der SQL-Datei (in der Select-Query) angegeben werden, z.B.:\nSELECT\n    1::int AS foo, ST_AsGDALRaster((ST_AsRaster(ST_Buffer(ST_Point(2607880,1228287),10),150, 150)), 'AAIGrid', ARRAY[''], 2056) AS raster\n;\nBeispiel:\ntasks.register('exportTiff', PostgisRasterExport) {\n    database = [db_uri, db_user, db_pass]\n    sqlFile = \"raster.sql\"\n    dataFile = \"export.tif\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nObject\nName der Rasterdatei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, aus der exportiert werden soll.\n nein\n\n\nsqlFile\nString\nName der SQL-Datei aus der das SQL-Statement gelesen und ausgeführt wird.\n nein\n\n\nsqlParameters\nMap&lt;String,String&gt;\nEine Map mit Paaren von Parameter-Name und Parameter-Wert.\n ja\n\n\n\n\n\nPublisher\nStellt für Vektordaten die aktuellsten Geodaten-Dateien bereit und pflegt das Archiv der vorherigen Zeitstände.\nDetails\n\n\nS3Bucket2Bucket\nKopiert Objekte von einem Bucket in einen anderen. Die Buckets müssen in der gleichen Region sein. Die Permissions werden nicht mitkopiert und müssen explizit gesetzt werden.\nBeispiel:\ntasks.register('copyFiles', S3Bucket2Bucket) {\n    accessKey = s3AccessKey\n    secretKey = s3SecretKey\n    sourceBucket = s3SourceBucket\n    targetBucket = s3TargetBucket\n    acl = \"public-read\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\naccessKey\nString\nAccessKey\n nein\n\n\nacl\nString\nAccess Control Layer [private, public-read, public-read-write, authenticated-read, aws-exec-read, bucket-owner-read, bucket-owner-full-control]\n nein\n\n\nendPoint\nString\nS3-Endpunkt. Default: https://s3.eu-central-1.amazonaws.com/\n ja\n\n\nmetaData\nMap&lt;String,String&gt;\nMetadaten des Objektes resp. der Objekte, z.B. [\"lastModified\":\"2020-08-28\"].\n ja\n\n\nregion\nString\nS3-Region. Default: eu-central-1\n ja\n\n\nsecretKey\nString\nSecretKey\n nein\n\n\nsourceBucket\nString\nBucket, aus dem die Objekte kopiert werden.\n nein\n\n\ntargetBucket\nString\nBucket, in den die Objekte kopiert werden.\n nein\n\n\n\n\n\nS3Download\nLädt eine Datei aus einem S3-Bucket herunter.\nBeispiel:\ntasks.register('downloadFile', S3Download) {\n    accessKey = 'abcdefg'\n    secretKey = 'hijklmnopqrstuvwxy'\n    downloadDir = file(\"./path/to/dir/\")\n    bucketName = \"ch.so.ada.denkmalschutz\"\n    key = \"foo.pdf\"\n    endPoint = \"https://s3.eu-central-1.amazonaws.com\" \n    region = \"eu-central-1\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\naccessKey\nString\nAccessKey\n nein\n\n\nbucketName\nString\nName des Buckets, in dem die Datei gespeichert ist.\n nein\n\n\ndownloadDir\nFile\nVerzeichnis, in das die Datei heruntergeladen werden soll.\n nein\n\n\nendPoint\nString\nS3-Endpunkt. Default: https://s3.eu-central-1.amazonaws.com/\n ja\n\n\nkey\nString\nName der Datei.\n nein\n\n\nregion\nString\nS3-Region. Default: eu-central-1\n ja\n\n\nsecretKey\nString\nSecretKey\n nein\n\n\n\n\n\nS3Upload\nLädt ein Dokument (sourceFile) oder alle Dokumente in einem Verzeichnis (sourceDir) in einen S3-Bucket (bucketName) hoch.\nMit dem passenden Content-Typ kann man das Verhalten des Browsers steuern. Default ist ‘application/octect-stream’, was dazu führt, dass die Datei immer heruntergeladen wird. Soll z.B. ein PDF oder ein Bild im Browser direkt angezeigt werden, muss der korrekte Content-Typ gewählt werden.\nBeispiel:\ntasks.register('uploadDirectory', S3Upload) {\n    accessKey = 'abcdefg'\n    secretKey = 'hijklmnopqrstuvwxy'\n    sourceDir = file(\"./docs\")\n    bucketName = \"ch.so.ada.denkmalschutz\"\n    endPoint = \"https://s3.eu-central-1.amazonaws.com\" \n    region = \"eu-central-1\"\n    acl = \"public-read\"\n    contentType = \"application/pdf\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\naccessKey\nString\nAccessKey\n nein\n\n\nacl\nString\nAccess Control Layer [private, public-read, public-read-write, authenticated-read, aws-exec-read, bucket-owner-read, bucket-owner-full-control]\n nein\n\n\nbucketName\nString\nName des Buckets, in dem die Dateien gespeichert werden sollen.\n nein\n\n\ncontentType\nString\nContent-Type\n ja\n\n\nendPoint\nString\nS3-Endpunkt. Default: https://s3.eu-central-1.amazonaws.com/\n ja\n\n\nmetaData\nMap&lt;String,String&gt;\nMetadaten des Objektes resp. der Objekte, z.B. [\"lastModified\":\"2020-08-28\"]\n ja\n\n\nregion\nString\nS3-Region. Default: eu-central-1\n nein\n\n\nsecretKey\nString\nSecretKey\n nein\n\n\nsourceDir\nObject\nVerzeichnis mit den Dateien, die hochgeladen werden sollen.\n ja\n\n\nsourceFile\nObject\nDatei, die hochgeladen werden soll.\n ja\n\n\nsourceFiles\nObject\nFileCollection mit den Dateien, die hochgeladen werden sollen, z.B. fileTree(\"/path/to/directoy/\") { include \"*.itf\" }\n ja\n\n\n\n\n\nShpExport\nDaten aus einer bestehenden Datenbanktabelle werden in eine SHP-Datei exportiert.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('shpexport', ShpExport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"shpexport\"\n    tableName = \"exportdata\"\n    dataFile = \"data.shp\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nObject\nName der SHP-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, aus der exportiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der SHP-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\ntableName\nString\nName der DB-Tabelle, die exportiert werden soll.\n nein\n\n\n\n\n\nShpImport\nDaten aus einer SHP-Datei in eine bestehende Datenbanktabelle importieren.\nDie Tabelle kann weitere Spalten enthalten, die in der SHP-Datei nicht vorkommen. Sie müssen aber NULLable sein, oder einen Default-Wert definiert haben.\nDie Tabelle muss eine Geometriespalte enthalten. Der Name der Geometriespalte kann beliebig gewählt werden.\nDie Gross-/Kleinschreibung der SHP-Spaltennamen wird für die Zuordnung zu den DB-Spalten ignoriert.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('shpimport', ShpImport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"shpimport\"\n    tableName = \"importdata\"\n    dataFile = \"data.shp\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nObject\nName der SHP-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, aus der exportiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der SHP-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\ntableName\nString\nName der DB-Tabelle, die exportiert werden soll.\n nein\n\n\n\n\n\nShpValidator\nPrüft eine SHP-Datei gegenüber einem INTERLIS-Modell. Basiert auf dem ilivalidator.\nIm gegebenen Modell wird eine Klasse gesucht, die genau die Attributenamen wie in der Shp-Datei enthält (wobei die Gross-/Kleinschreibung ignoriert wird); die Attributtypen werden ignoriert. Wird keine solche Klasse gefunden, gilt das als Validierungsfehler.\nDie Prüfung von gleichzeitig mehreren Shapefiles führt zu Fehlermeldungen wie OID o3158 of object &lt;Modelname&gt;.&lt;Topicname&gt;.&lt;Klassenname&gt; already exists in .... Beim Öffnen und Lesen eines Shapefiles wird immer der Zähler, der die interne (im Shapefile nicht vorhandene) OID generiert, zurückgesetzt. Somit kann immer nur ein Shapefile pro Task geprüft werden.\nBeispiel:\ntasks.register('validate', ShpValidator) {\n    models = \"ShpModel\"\n    dataFiles = [\"data.shp\"]\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nallObjectsAccessible\nBoolean\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n ja\n\n\nconfigFile\nObject\nKonfiguriert die Datenprüfung mit Hilfe einer ini-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\ndataFiles\nObject\nListe der Dateien, die validiert werden sollen. FileCollection oder List. Eine leere Liste ist kein Fehler.\n nein\n\n\ndisableAreaValidation\nBoolean\nSchaltet die AREA-Topologieprüfung aus. Default: false\n ja\n\n\nencoding\nString\nZeichencodierung der SHP-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nfailOnError\nBoolean\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n ja\n\n\nforceTypeValidation\nBoolean\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n ja\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n ja\n\n\nmetaConfigFile\nObject\nKonfiguriert den Validator mit Hilfe einer ini-Datei. Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell, gegen das die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Der Name der CSV-Datei.\n ja\n\n\nmultiplicityOff\nBoolean\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n ja\n\n\npluginFolder\nObject\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\nskipPolygonBuilding\nBoolean\nSchaltet die Bildung der Polygone aus (nur ITF). Default: false\n ja\n\n\nvalidationOk\nboolean\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false.\n nein\n\n\nxtflogFile\nObject\nSchreibt die log-Meldungen in eine INTERLIS-2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n ja\n\n\n\n\n\nSqlExecutor\nDer SqlExecutor-Task dient dazu, Datenumbauten auszuführen.\nEr wird im Allgemeinen dann benutzt, wenn\n\nder Datenumbau komplex ist und deshalb nicht im Db2Db-Task erledigt werden kann\noder wenn die Quell-DB keine PostgreSQL-DB ist (weil bei komplexen Queries für den Datenumbau möglicherweise fremdsystemspezifische SQL-Syntax verwendet werden müsste)\noder wenn Quell- und Zielschema in derselben Datenbank liegen\n\nIn den Fällen 1 und 2 werden Stagingtabellen bzw. ein Stagingschema benötigt, in welche der Db2Db-Task die Daten zuerst 1:1 hineinschreibt. Der SqlExecutor-Task liest danach die Daten von dort, baut sie um und schreibt sie dann ins Zielschema. Die Queries für den SqlExecutor-Task können alle in einem einzelnen .sql-File sein oder (z.B. aus Gründen der Strukturierung oder Organisation) auf mehrere .sql-Dateien verteilt sein. Die Reihenfolge der .sql-Dateien ist relevant. Dies bedeutet, dass die SQL-Befehle des zuerst angegebenen .sql-Datei zuerst ausgeführt werden müssen, danach dies SQL-Befehle des an zweiter Stelle angegebenen .sql-Datei, usw.\nDer SqlExecutor-Task muss neben Updates ganzer Tabellen (d.h. Löschen des gesamten Inhalts einer Tabelle und gesamter neuer Stand in die Tabelle schreiben) auch Updates von Teilen von Tabellen zulassen. D.h. es muss z.B. möglich sein, innerhalb einer Tabelle nur die Objekte einer bestimmten Gemeinde zu aktualisieren. Darum ist es möglich innerhalb der .sql-Datei Paramater zu verwenden und diesen Parametern beim Task einen konkreten Wert zuzuweisen. Innerhalb der .sql-Datei werden Paramter mit folgender Syntax verwendet: ${paramName}.\nUnterstützte Datenbanken: PostgreSQL, SQLite, Oracle, Derby und DuckDB.\nBeispiele:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('executeSomeSql', SqlExecutor) {\n    database = [db_uri, db_user, db_pass]\n    sqlParameters = [dataset:'Olten']\n    sqlFiles = ['demo.sql']\n}\nDamit mit einer einzigen Task-Definition mehrere Datensätze verarbeitet werden können, kann auch eine Liste von Parametern angegeben werden.\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('executeSomeSql', SqlExecutor) {\n    database = [db_uri, db_user, db_pass]\n    sqlParameters = [[dataset:'Olten'],[dataset:'Grenchen']]\n    sqlFiles = ['demo.sql']\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank, in die importiert werden soll.\n nein\n\n\nsqlFiles\nListProperty&lt;String&gt;\nName der SQL-Datei aus der SQL-Statements gelesen und ausgeführt werden.\n nein\n\n\nsqlParameters\nProperty&lt;Object&gt;\nEine Map mit Paaren von Parameter-Name und Parameter-Wert (Map&lt;String,String&gt;). Oder eine Liste mit Paaren von Parameter-Name und Parameter-Wert (List&lt;Map&lt;String,String&gt;&gt;).\n ja\n\n\n\n\n\nXslTransformer (incubating)\nTransformiert eine Datei mittels einer XSL-Transformation ein eine andere Datei. Ist der xslFile-Parameter ein String, wird erwartet, dass die Datei im GRETL-Verzeichnis im Ressourcenordner src/main/resources/xslt-Verzeichnis gespeichert ist. Falls der xslFile-Parameter ein File-Objekt ist, können lokale Dateien verwendet werden.\nBeispiele:\ntasks.register('transform', XslTransformer) {\n    xslFile = \"eCH0132_to_SO_AGI_SGV_Meldungen_20221109.xsl\"\n    xmlFile = file(\"MeldungAnGeometer_G-0111102_20221103_145001.xml\")\n    outDirectory = file(\".\")\n}\ntasks.register('transform', XslTransformer) {\n    xslFile = file(\"path/to/eCH0132_to_SO_AGI_SGV_Meldungen_20221109.xsl\")\n    xmlFile = file(\"MeldungAnGeometer_G-0111102_20221103_145001.xml\")\n    outDirectory = file(\".\")\n}\ntasks.register('transform', XslTransformer) {\n    xslFile = \"eCH0132_to_SO_AGI_SGV_Meldungen_20221109.xsl\"\n    xmlFile = fileTree(\".\").matching {\n        include \"*.xml\"\n    }\n    outDirectory = file(\".\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nfileExtension\nString\nFileextension der Resultatdatei. Default: xtf\n ja\n\n\noutDirectory\nFile\nVerzeichnis, in das die transformierte Datei gespeichert wird. Der Name der transformierten Datei entspricht standardmässig dem Namen der Input-Datei mit Endung .xtf.\n nein\n\n\nxmlFile\nObject\nDatei oder FileTree, die/der transformiert werden soll.\n nein\n\n\nxslFile\nObject\nName der XSLT-Datei, die im src/main/resources/xslt-Verzeichnis liegen muss oder File-Objekt (beliebiger Pfad).\n nein"
  },
  {
    "objectID": "gretl/docs/user/reference.html",
    "href": "gretl/docs/user/reference.html",
    "title": "Referenzdokumentation",
    "section": "",
    "text": "Das Datenmanagement-Tool GRETL ist ein Werkzeug, das für Datenimports, Datenumbauten (Modellumbau) und Datenexports eingesetzt wird. GRETL führt Jobs aus, wobei ein Job aus mehreren atomaren Tasks besteht. Damit ein Job als vollständig ausgeführt gilt, muss jeder zum Job gehörende Task vollständig ausgeführt worden sein. Schlägt ein Task fehl, gilt auch der Job als fehlgeschlagen.\nEin Job besteht aus einem oder mehreren Tasks, die gemäss einem gerichteten Graphen (Directed Acyclic Graph; DAG) miteinander verknüpft sind.\nEin Job kann aus z.B. aus einer linearen Kette von Tasks bestehen:\nTask 1 – Task 2 – Task 3 – Task n\nBeispiel: Datenimport aus INTERLIS-Datei – Datenumbau – Datenexport nach Shapefile.\nEin Job kann sich nach einem Task aber auch auf zwei oder mehr verschiedene weitere Tasks verzweigen:\n       – Task 2 – Task 3 – Task n\nTask 1 –\n       – Task 4 – Task 5 – Task m\nBeispiel: Datenimport aus INTERLIS-Datei – Datenumbau in Zielschema 1 und ein zweiter Datenumbau in Zielschema 2.\nEs ist auch möglich, dass zuerst zwei oder mehr Tasks unabhängig voneinander ausgeführt werden müssen, bevor ein einzelner weiterer Task ausgeführt wird.\nTask 1 –\n       – Task 3 – Task 4 – Task n\nTask 2 –\nDie Tasks eines Jobs werden per Konfigurationsfile konfiguriert.\n\n\nFür die aktuelle GRETL-Version wird Java 11 und Gradle 7.6 benötigt.\n\n\n\nGRETL selbst muss als Gradle-Plugin nicht explizit installiert werden, sondern wird dynamisch durch das Internet bezogen.\n\n\n\nErstellen Sie in einem neuen Verzeichnis gretldemo eine neue Datei build.gradle:\nimport ch.so.agi.gretl.tasks.*\nimport ch.so.agi.gretl.api.*\n\napply plugin: 'ch.so.agi.gretl'\n\nbuildscript {\n    repositories {\n        maven { url \"https://jars.interlis.ch\" }\n        maven { url \"https://repo.osgeo.org/repository/release/\" }\n        maven { url \"https://plugins.gradle.org/m2/\" }\n        maven { url \"https://s01.oss.sonatype.org/service/local/repositories/releases/content/\" }\n        maven { url \"https://s01.oss.sonatype.org/service/local/repositories/snapshots/content/\" }\n        mavenCentral()\n    }\n    dependencies {\n        classpath group: 'ch.so.agi', name: 'gretl',  version: '3.0.+'\n    }\n}\n\ndefaultTasks 'validate'\n\ntask validate(type: IliValidator){\n    dataFiles = [\"BeispielA.xtf\"]\n}\nDie Datei build.gradle ist die Job-Konfiguration. Dieser kleine Beispiel-Job besteht nur aus einem einzigen Task: validate.\nErstellen Sie nun noch die Datei BeispielA.xtf (damit danach der Job erfolgreich ausgeführt werden kann).\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;TRANSFER xmlns=\"http://www.interlis.ch/INTERLIS2.3\"&gt;\n    &lt;HEADERSECTION SENDER=\"gretldemo\" VERSION=\"2.3\"&gt;\n    &lt;/HEADERSECTION&gt;\n    &lt;DATASECTION&gt;\n        &lt;OeREBKRMtrsfr_V1_1.Transferstruktur BID=\"B01\"&gt;\n        &lt;/OeREBKRMtrsfr_V1_1.Transferstruktur&gt;\n    &lt;/DATASECTION&gt;\n&lt;/TRANSFER&gt;\nUm den Job auszuführen, wechseln Sie ins Verzeichnis mit der Job-Konfiguration, und geben da das Kommando gradle ohne Argument ein:\ncd gretldemo\ngradle\nBUILD SUCCESSFUL zeigt an, dass der Job (die Validierung der Datei BeispielA.xtf) erfolgreich ausgeführt wurde.\nUm dieselbe Job-Konfiguration für verschiedene Datensätze verwenden zu können, muss es parametrisierbar sein. Die Jobs/Tasks können so generisch konfiguriert werden, dass dieselbe Konfiguration z.B. für Daten aus verschiedenen Gemeinden benutzt werden kann. Parameter für die Job Konfiguration können z.B. mittels gradle-Properties (Gradle properties and system properties) dem Job mitgegeben werden, also z.B.\ncd gretldemo\ngradle -Pdataset=Olten\n\n\n\nUm GRETL auszuführen, geben Sie auf der Kommandozeile folgendes Kommando ein (wobei jobfolder der absolute Pfad zu ihrem Verzeichnis mit der Job Konfiguration ist.)\ngradle --project-dir jobfolder\nAlternativ können Sie auch ins Verzeichnis mit der Job Konfiguration wechseln, und da das Kommando gradle ohne Argument verwenden:\ncd jobfolder\ngradle\n\n\n\n\n\nTransformiert eine INTERLIS1-Transferdatei im kantonalen AV-DM01-Modell in das Bundesmodell. Unterstützt werden die Sprachen Deutsch und Italienisch und der Bezugrahmen LV95. Getestet mit Daten aus den Kantonen Solothurn, Glarus und Tessin. Weitere Informationen sind in der Basisbibliothek zu finden: https://github.com/sogis/av2ch.\nDas Bundes-ITF hat denselben Namen wie das Kantons-ITF.\nAufgrund der sehr vielen Logging-Messages einer verwendeten Bibliothek, wird der System.err-Ouput nach dev/null https://github.com/sogis/av2ch/blob/master/src/main/java/ch/so/agi/av/Av2ch.java#L75.\ntasks.register('transform', Av2ch) {\n    inputFile = file(\"254900.itf\")\n    outputDirectory = file(\"output\")\n}\nEs können auch mehrere Dateien gleichzeitig transformiert werden:\ntasks.register('transform', Av2ch) {\n    inputFile = fileTree(\".\").matching {\n            include\"*.itf\"\n        }\n    outputDirectory = file(\"output\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ninputFile\nObject\nZu transformierende ITF-Datei(en). File- oder FileCollection-Objekt.\n nein\n\n\nlanguage\nString\nSprache des Modelles / der Datei (de, it). Default: de\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\noutputDirectory\nObject\nName des Verzeichnisses in das die zu erstellende Datei geschrieben wird.\n nein\n\n\nzip\nBoolean\nDie zu erstellende Datei wird gezippt. Default: false\n ja\n\n\n\n\n\n\nAv2geobau konvertiert eine Interlis-Transferdatei (itf) in eine DXF-Geobau Datei. Av2geobau funktioniert ohne Datenbank.\nDie ITF-Datei muss dem Modell DM01AVCH24LV95D entsprechen. Die Daten werden nicht validiert.\nDie Datenstruktur der DXF-Datei ist im Prinzip sehr einfach: Die verschiedenen Informationen aus dem Datenmodell DM01 werden in verschiedene DXF-Layer abgebildet, z.B. die begehbaren LFP1 werden in den Layer “01111” abgebildet. Oder die Gebäude in den Layer “01211”.\nDer Datenumbau ist nicht konfigurierbar.\ntasks.register('av2geobau', Av2geobau) {\n    itfFiles = \"ch_254900.itf\"\n    dxfDirectory = \"./out/\"\n}\nEs können auch mehrere Dateien angegeben werden.\ntasks.register('av2geobau', Av2geobau) {\n    itfFiles = fileTree(\".\").matching {\n        include\"*.itf\"\n    }\n    dxfDirectory = \"./out/\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndxfDirectory\nObject\nVerzeichnis, in das die DXF-Dateien gespeichert werden.\n nein\n\n\nisZip\nBoolean\nDie zu erstellende Datei wird gezippt und es werden zusätzliche Dateien (Musterplan, Layerbeschreibung, Hinweise) hinzugefügt. Default: false\n ja\n\n\nitfFiles\nObject\nITF-Datei, die nach DXF transformiert werden soll. Es können auch mehrere Dateien angegeben werden. File- oder FileCollection-Objekt.\n nein\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Konvertierung in eine Text-Datei.\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\n\n\n\n\nKonvertiert eine CSV-Datei in eine Excel-Datei (*.xlsx). Datentypen werden anhand eines INTERLIS-Modelles eruiert. Fehlt das Modell, wird alles als Text gespeichert. Die Daten werden vollständig im Speicher vorgehalten. Falls grosse Dateien geschrieben werden müssen, kann das zu Problemen führen. Dann müsste die Apache POI SXSSF Implementierung (Streaming) verwendet werden.\nBeispiel:\ntasks.register('convertData', Csv2Excel) {\n    csvFile = file(\"./20230124_sap_Gebaeude.csv\")\n    firstLineIsHeader = true\n    valueDelimiter = null\n    valueSeparator = \";\"\n    encoding = \"ISO-8859-1\";\n    models = \"SO_HBA_Gebaeude_20230111\";\n    outputDir = file(\".\");\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncsvFile\nFile\nCSV-Datei, die konvertiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der CSV-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nfirstLineIsHeader\nBoolean\nDefiniert, ob eine Headerzeile geschrieben werden soll, oder nicht. Default: true\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell für Definition der Datentypen in der Excel-Datei.\n ja\n\n\noutputDir\nFile\nVerzeichnis, in das die Excel-Datei gespeichert wird. Default: Verzeichnis, in dem die CSV-Datei vorliegt.\n ja\n\n\nvalueDelimiter\nCharacter\nZeichen, das am Anfang und Ende jeden Wertes geschrieben werden soll. Default \"\n ja\n\n\nvalueSeparator\nCharacter\nZeichen, das als Trennzeichen zwischen den Werten verwendet werden soll. Default: ,\n ja\n\n\n\n\n\n\nDaten aus einer bestehenden Datenbanktabelle werden in eine CSV-Datei exportiert. Geometriespalten können nicht exportiert werden.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('csvexport', CsvExport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"csvexport\"\n    tableName = \"exportdata\"\n    firstLineIsHeader = true\n    attributes = [ \"t_id\",\"Aint\"]\n    dataFile = \"data.csv\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nattributes\nString[]\nSpalten der DB-Tabelle, die exportiert werden sollen. Definiert die Reihenfolge der Spalten in der CSV-Datei. Default: alle Spalten\n ja\n\n\ndataFile\nObject\nName der CSV-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank aus der exportiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der CSV-Datei, z.B. “UTF-8”. Default: Systemeinstellung\n ja\n\n\nfirstLineIsHeader\nBoolean\nDefiniert, ob eine Headerzeile geschrieben werden soll, oder nicht. Default: true\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\ntableName\nString\nName der DB-Tabelle, die exportiert werden soll.\n nein\n\n\nvalueDelimiter\nCharacter\nZeichen, das am Anfang und Ende jeden Wertes geschrieben werden soll. Default \"\n ja\n\n\nvalueSeparator\nCharacter\nZeichen, das als Trennzeichen zwischen den Werten verwendet werden soll. Default: ,\n ja\n\n\n\n\n\n\nDaten aus einer CSV-Datei werden in eine bestehende Datenbanktabelle importiert.\nDie Tabelle kann weitere Spalten enthalten, die in der CSV-Datei nicht vorkommen. Sie müssen aber NULLable sein, oder einen Default-Wert definiert haben.\nGeometriepalten können nicht importiert werden.\nDie Gross-/Kleinschreibung der CSV-Spaltennamen wird für die Zuordnung zu den DB-Spalten ignoriert.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('csvimport',  CsvImport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"csvimport\"\n    tableName = \"importdata\"\n    firstLineIsHeader = true\n    dataFile = \"data1.csv\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nattributes\nString[]\nSpalten der DB-Tabelle, die exportiert werden sollen. Definiert die Reihenfolge der Spalten in der CSV-Datei. Default: alle Spalten\n ja\n\n\ndataFile\nObject\nName der CSV-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank aus der exportiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der CSV-Datei, z.B. “UTF-8”. Default: Systemeinstellung\n ja\n\n\nfirstLineIsHeader\nBoolean\nDefiniert, ob eine Headerzeile geschrieben werden soll, oder nicht. Default: true\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\ntableName\nString\nName der DB-Tabelle, die exportiert werden soll.\n nein\n\n\nvalueDelimiter\nCharacter\nZeichen, das am Anfang und Ende jeden Wertes geschrieben werden soll. Default \"\n ja\n\n\nvalueSeparator\nCharacter\nZeichen, das als Trennzeichen zwischen den Werten verwendet werden soll. Default: ,\n ja\n\n\n\n\n\n\nPrüft eine CSV-Datei gegenüber einem INTERLIS-Modell. Basiert auf dem ilivalidator. Das Datenmodell darf die OID nicht als UUID modellieren (OID AS INTERLIS.UUIDOID).\nBeispiel:\ntasks.register('validate', CsvValidator) {\n    models = \"CsvModel\"\n    firstLineIsHeader = true\n    dataFiles = [\"data1.csv\"]\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nallObjectsAccessible\nBoolean\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n ja\n\n\nconfigFile\nObject\nKonfiguriert die Datenprüfung mit Hilfe einer ini-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\ndataFiles\nObject\nListe der Dateien, die validiert werden sollen. FileCollection oder List. Eine leere Liste ist kein Fehler.\n nein\n\n\ndisableAreaValidation\nBoolean\nSchaltet die AREA-Topologieprüfung aus. Default: false\n ja\n\n\nencoding\nString\nZeichencodierung der CSV-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nfailOnError\nBoolean\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n ja\n\n\nfirstLineIsHeader\nBoolean\nDefiniert, ob die CSV-Datei einer Headerzeile hat, oder nicht. Default: true\n ja\n\n\nforceTypeValidation\nBoolean\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n ja\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n ja\n\n\nmetaConfigFile\nObject\nKonfiguriert den Validator mit Hilfe einer ini-Datei. Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell, gegen das die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Der Name der CSV-Datei.\n ja\n\n\nmultiplicityOff\nBoolean\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n ja\n\n\npluginFolder\nObject\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\nskipPolygonBuilding\nBoolean\nSchaltet die Bildung der Polygone aus (nur ITF). Default: false\n ja\n\n\nvalidationOk\nboolean\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false.\n nein\n\n\nvalueDelimiter\nCharacter\nZeichen, das am Anfang und Ende jeden Wertes vorhanden ist. Default \"\n ja\n\n\nvalueSeparator\nCharacter\nZeichen, das als Trennzeichen zwischen den Werten interpretiert werden soll. Default: ,\n ja\n\n\nxtflogFile\nObject\nSchreibt die log-Meldungen in eine INTERLIS-2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n ja\n\n\n\nFalls die CSV-Datei eine Header-Zeile enthält (mit den Spaltennamen), wird im gegebenen Modell eine Klasse gesucht, welche die Header-Spaltennamen enthält (Gross-/Kleinschreibung sowie optionale “Spalten” der Modell-Klasse werden ignoriert). Wird keine solche Klasse gefunden, gilt das als Validierungsfehler.\nFalls die CSV-Datei keine Header-Zeile enthält (mit den Spaltennamen), wird im gegebenen Modell eine Klasse gesucht, die die selbe Anzahl Attribute hat. Wird keine solche Klasse gefunden, gilt das als Validierungsfehler.\nDie Prüfung von gleichzeitig mehreren CSV-Dateien führt zu Fehlermeldungen wie OID o3158 of object &lt;Modelname&gt;.&lt;Topicname&gt;.&lt;Klassenname&gt; already exists in .... Beim Öffnen und Lesen einer CSV-Datei wird immer der Zähler, der die interne (in der CSV-Datei nicht vorhandene) OID generiert, zurückgesetzt. Somit kann immer nur eine CSV-Datei pro Task geprüft werden.\n\n\n\nSimuliert mit einem HttpClient einige Curl-Befehle.\nBeispiele:\nimport ch.so.agi.gretl.tasks.Curl.MethodType;\n\ntasks.register('uploadData', Curl) {\n    serverUrl = \"https://geodienste.ch/data_agg/interlis/import\"\n    method = MethodType.POST\n    formData = [\"topic\": \"npl_waldgrenzen\", \"lv95_file\": file(\"./test.xtf.zip\"), \"publish\": \"true\", \"replace_all\": \"true\"]\n    user = \"fooUser\"\n    password = \"barPwd\"\n    expectedStatusCode = 200\n    expectedBody = \"\\\"success\\\":true\"\n}\nimport ch.so.agi.gretl.tasks.Curl.MethodType;\n\ntasks.register('uploadData', Curl) {\n    serverUrl = \"https://testweb.so.ch/typo3/api/digiplan\"\n    method = MethodType.POST\n    headers = [\"Content-Type\": \"application/xml\", \"Content-Encoding\": \"gzip\"]\n    dataBinary = file(\"./planregister.xml.gz\")\n    user = \"fooUser\"\n    password = \"barPwd\"\n    expectedStatusCode = 202\n}\nimport ch.so.agi.gretl.tasks.Curl.MethodType;\n\ntasks.register('downloadData', Curl) {\n    serverUrl = \"https://raw.githubusercontent.com/sogis/gretl/master/README.md\"\n    method = MethodType.GET\n    outputFile = file(\"./README.md\")\n    expectedStatusCode = 200\n}\nimport groovy.json.JsonSlurper\n\ndef basicHeader = \"Basic \" + Base64.getEncoder().encodeToString((clientId + \":\" + clientSecret).getBytes())\ndef token = \"\"\n\ntasks.register('getToken', Curl) {\n    serverUrl = \"https://auth.example.ch/oauth2/token\"\n    method = MethodType.POST\n    outputFile = file(\"./token.json\")\n    expectedStatusCode = 200\n    data = \"grant_type=client_credentials&client_id=$clientId\"\n    headers = [\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n        \"Authorization\": basicHeader\n    ]\n\n    doLast {\n        def slurper = new JsonSlurper();\n        def slurped = slurper.parseText(outputFile.text)\n\n        if (slurped.access_token != null)\n            token = slurped.access_token\n\n        if(token.length() == 0)\n            throw new GradleException(\"Failed to retrieve access token\")\n\n        println \"Length of access token is: \" + token.length()\n    }\n}\n\ntasks.register('downloadJson', Curl) {\n    dependsOn 'getToken'\n    serverUrl = \"https://obs.example.ch/rest/v4/docs.json?projects=83505\"\n    method = MethodType.GET\n    outputFile = file(\"./data.json\")\n    expectedStatusCode = 200\n    headers.put(\"Authorization\", providers.provider { \"Bearer \" + token })\n} \n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndata\nString\nString, der via POST hochgeladen werden soll. Entspricht curl [URL] --data.\n ja\n\n\ndataBinary\nFile\nDatei, die hochgeladen werden soll. Entspricht curl [URL] --data-binary.\n ja\n\n\nexpectedBody\nString\nErwarteter Text, der vom Server als Body zurückgelieferd wird.\n ja\n\n\nexpectedStatusCode\nInteger\nErwarteter Status Code, der vom Server zurückgeliefert wird.\n nein\n\n\nformData\nMap&lt;String,Object&gt;\nForm data parameters. Entspricht curl [URL] -F key1=value1 -F file1=@my_file.xtf.\n ja\n\n\nheaders\nMapProperty&lt;String,String&gt;\nRequest-Header. Entspricht curl [URL] -H ... -H .....\n ja\n\n\nmethod\nMethodType\nHTTP-Request-Methode. Unterstützt werden GET und POST.\n ja\n\n\noutputFile\nFile\nDatei, in die der Output gespeichert wird. Entspricht curl [URL] -o.\n ja\n\n\npassword\nString\nPasswort. Wird zusammen mit user in einen Authorization-Header umgewandelt. Entspricht curl [URL] -u user:password.\n ja\n\n\nserverUrl\nString\nDie URL des Servers inklusive Pfad und Queryparameter.\n nein\n\n\nuser\nString\nBenutzername. Wird zusammen mit password in einen Authorization-Header umgewandelt. Entspricht curl [URL] -u user:password.\n ja\n\n\n\n\n\n\nSpeichert Dokumente, deren URL in einer Spalte einer Datenbanktabelle gespeichert sind, in einem lokalen Verzeichnis. Zukünftig und bei Bedarf kann der Task so erweitert werden, dass auch BLOBs aus der Datenbank gespeichert werden können.\nRedirect von HTTP nach HTTPS funktionieren nicht. Dies korrekterweise (?) wegen der verwendeten Java-Bibliothek.\nWegen der vom Kanton Solothurn eingesetzten self-signed Zertifikate muss ein unschöner Handstand gemacht werden. Leider kann dieser Usecase schlecht getestet werden, da die Links nur in der privaten Zone verfügbar sind und die zudem noch häufig ändern können. Manuell getestet wurde es jedoch.\nAls Dateiname wird der letzte Teil des URL-Pfades verwendet, z.B. https://artplus.verw.rootso.org/MpWeb-apSolothurnDenkmal/download/2W8v0qRZQBC0ahDnZGut3Q?mode=gis wird mit den Prefix und Extension zu ada_2W8v0qRZQBC0ahDnZGut3Q.pdf.\nEs wird DISTINCT ON (&lt;documentColumn&gt;) und ein Filter WHERE &lt;documentColumn&gt; IS NOT NULL verwendet.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('exportDocuments', DatabaseDocumentExport) {\n    database = [db_uri, db_user, db_pass]\n    qualifiedTableName = \"ada_denkmalschutz.fachapplikation_rechtsvorschrift_link\"\n    documentColumn = \"multimedia_link\"\n    targetDir = file(\".\")\n    fileNamePrefix = \"ada_\"\n    fileNameExtension = \"pdf\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndatabase\nConnector\nDatenbank, aus der die Dokumente exportiert werden sollen.\n nein\n\n\ndocumentColumn\nString\nDB-Tabellenspalte mit dem Dokument resp. der URL zum Dokument.\n nein\n\n\nfileNameExtension\nString\nDateinamen-Extension.\n ja\n\n\nfileNamePrefix\nString\nPrefix für Dateinamen.\n ja\n\n\nqualifiedTableName\nString\nQualifizierter Tabellenname.\n nein\n\n\ntargetDir\nFile\nVerzeichnis in das die Dokumente exportiert werden sollen.\n nein\n\n\n\n\n\n\nDies ist prinzipiell ein 1:1-Datenkopie, d.h. es findet kein Datenumbau statt, die Quell- und die Ziel- Tabelle hat jeweils identische Attribute. Es werden auf Seite Quelle in der Regel also simple SELECT-Queries ausgeführt und die Resultate dieser Queries in Tabellen der Ziel-DB eingefügt. Unter bestimmten Bedingungen (insbesondere wenn es sich um einen wenig komplexen Datenumbau handelt), kann dieser Task aber auch zum Datenumbau benutzt werden.\nDie Queries können auf mehrere .sql-Dateien verteilt werden, d.h. der Task muss die Queries mehrerer .sql-Dateien zu einer Transaktion kombinieren können. Jede .sql-Datei gibt genau eine Resultset (RAM-Tabelle) zurück. Das Resultset wird in die konfigurierte Zieltabelle geschrieben. Die Beziehungen sind: Eine bis mehrere Quelltabellen ergeben ein Resultset; das Resultset entspricht bezüglich den Attributen genau der Zieltabelle und wird 1:1 in diese geschrieben. Der Db2Db- Task verarbeitet innerhalb einer Transaktion 1-n Resultsets und wird entsprechend auch mit 1-n SQL-Dateien konfiguriert.\nDie Reihenfolge der .sql-Dateien ist relevant. Dies bedeutet, dass die SQL-Befehle der zuerst angegebenen .sql-Datei zuerst ausgeführt werden müssen, danach die SQL-Befehle der an zweiter Stelle angegebenen .sql-Datei, usw.\nAlle SELECT-Statements werden in einer Transaktion ausgeführt werden, damit ein konsistenter Datenstand gelesen wird. Alle INSERT-Statements werden in einer Transaktion ausgeführt werden, damit bei einem Fehler der bisherige Datenstand bestehen bleibt und also kein unvollständiger Import zurückgelassen wird.\nDamit dieselbe .sql-Datei für verschiedene Datensätze benutzt werden kann, ist es möglich innerhalb der .sql-Datei Parameter zu verwenden und diesen Parametern beim Task einen konkreten Wert zuzuweisen. Innerhalb der .sql-Datei werden Paramter mit folgender Syntax verwendet: ${paramName}.\nEs ist pro .sql-Datei nur ein SELECT-Statement erlaubt. Ausser das erste Statement ist ein “SET search_path TO”-Statement.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('transferSomeData', Db2Db) {\n    sourceDb = [db_uri, db_user, db_pass]\n    targetDb = ['jdbc:sqlite:gretldemo.sqlite',null,null]\n    sqlParameters = [dataset:'Olten']\n    transferSets = [\n        new TransferSet('some.sql', 'albums_dest', true)\n    ];\n}\nDamit mit einer einzigen Task-Definition mehrere Datensätze verarbeitet werden können, kann auch eine Liste von Parametern angegeben werden.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('transferSomeData', Db2Db) {\n    sourceDb = [db_uri, db_user, db_pass]\n    targetDb = ['jdbc:sqlite:gretldemo.sqlite',null,null]\n    sqlParameters = [[dataset:'Olten'],[dataset:'Grenchen']]\n    transferSets = [\n        new TransferSet('some.sql', 'albums_dest', true)\n    ];\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbatchSize\nProperty&lt;Integer&gt;\nAnzahl der Records, die pro Batch in die Ziel-Datenbank geschrieben werden (Default: 5000). Für sehr grosse Tabellen muss ein kleinerer Wert gewählt werden.\n ja\n\n\nfetchSize\nProperty&lt;Integer&gt;\nAnzahl der Records, die auf einmal vom Datenbank-Cursor von der Quell-Datenbank zurückgeliefert werden (Standard: 5000). Für sehr grosse Tabellen muss ein kleinerer Wert gewählt werden.\n ja\n\n\nsourceDb\nListProperty&lt;String&gt;\nDatenbank, aus der gelesen werden soll.\n nein\n\n\nsqlParameters\nProperty&lt;Object&gt;\nEine Map mit Paaren von Parameter-Name und Parameter-Wert (Map&lt;String,String&gt;). Oder eine Liste mit Paaren von Parameter-Name und Parameter-Wert (List&lt;Map&lt;String,String&gt;&gt;).\n ja\n\n\ntargetDb\nListProperty&lt;String&gt;\nDatenbank, in die geschrieben werden soll.\n nein\n\n\ntransferSets\nListProperty&lt;TransferSet&gt;\nEine Liste von TransferSets.\n nein\n\n\n\nEine TransferSet ist\n\neine SQL-Datei (mit SQL-Anweisungen zum Lesen der Daten aus der sourceDb),\ndem Namen der Ziel-Tabelle in der targetDb, und\nder Angabe ob in der Ziel-Tabelle vor dem INSERT zuerst alle Records gelöscht werden sollen.\neinem optionalen vierten Parameter, der verwendet werden kann um den zu erzeugenden SQL-Insert-String zu beeinflussen, u.a. um einen WKT-Geometrie-String in eine PostGIS-Geometrie umzuwandeln\n\nBeispiel, Umwandlung Rechtswert/Hochwertspalten in eine PostGIS-Geometrie (siehe auch GRETL-Job afu_onlinerisk_transfer der eine Punktgeometriespalten aus einer Nicht-Postgis-DB übernimmt):\nnew TransferSet('untersuchungseinheit.sql', 'afu_qrcat_v1.onlinerisk_untersuchungseinheit', true, (String[])[\"geom:wkt:2056\"])\ngeom ist der Geometrie-Spalten-Name der verwendet wird.\nDazugehöriger Auszug aus SQL-Datei zur Erzeugung des WKT-Strings mit Hilfe von concatenation:\n'Point(' || ue.koordinate_x::text || ' ' || ue.koordinate_y::text || ')' AS geom\nUnterstützte Datenbanken: PostgreSQL, SQLite, Oracle, Derby und DuckDB.\n\n\n\nLöscht Daten auf einem FTP-Server.\nBeispiel, löscht alle Daten in einem Verzeichnis:\ntasks.register('ftpdelete', FtpDelete) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir = \"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile = fileTree(pathToTempFolder) { include '*.zip' } \n    //remoteFile = \"*.zip\"\n}\nUm bestimmte Dateien zu löschen:\ntasks.register('ftpdelete', FtpDelete) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir = \"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile = \"*.zip\"\n}\nUm heruntergeladene Daten zu löschen:\ntasks.register('ftpdelete', FtpDelete) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir = \"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile = fileTree(pathToDownloadFolder) { include '*.zip' } \n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncontrolKeepAliveTimeout\nLong\nTimeout bis ein NOOP über den Kontroll-Kanal versendet wird. Default: 300s (=5 Minuten)\n ja\n\n\nfileSeparator\nString\nDefault: /. (Falls systemType Windows ist, ist der Default \\.\n ja\n\n\npassiveMode\nBoolean\nAktiv- oder Passiv-Verbindungsmodus. Default: Passiv (true)\n ja\n\n\npassword\nString\nPasswort für den Zugriff auf dem Server.\n nein\n\n\nremoteDir\nString\nVerzeichnis auf dem Server.\n nein\n\n\nremoteFile\nObject\nDateiname oder Liste der Dateinamen auf dem Server (kann auch ein Muster sein (* oder ?)). Ohne diesen Parameter werden alle Dateien aus dem Remoteverzeichnis gelöscht.\n ja\n\n\nserver\nString\nName des Servers (ohne ftp://).\n nein\n\n\nsystemType\nString\nUNIX oder WINDOWS. Default: UNIX.\n ja\n\n\nuser\nString\nBenutzername auf dem Server.\n nein\n\n\n\n\n\n\nLädt alle Dateien aus dem definierten Verzeichnis des Servers in ein lokales Verzeichnis herunter.\nBeispiel:\ntasks.register('ftpdownload', FtpDownload) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    localDir= \"downloads\"\n    remoteDir= \"\"\n}\nUm eine bestimmte Datei herunterzuladen:\ntasks.register('ftpdownload', FtpDownload) {\n    server=\"ftp.infogrips.ch\"\n    user= \"Hans\"\n    password= \"dummy\"\n    systemType=\"WINDOWS\"\n    localDir= \"downloads\"\n    remoteDir=\"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile=\"240100.zip\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncontrolKeepAliveTimeout\nLong\nTimeout bis ein NOOP über den Kontroll-Kanal versendet wird. Default: 300s (=5 Minuten)\n ja\n\n\nfileSeparator\nString\nDefault: /. (Falls systemType Windows ist, ist der Default \\.\n ja\n\n\nfileType\nString\nASCII oder BINARY. Default: ASCII.\n ja\n\n\nlocalDir\nString\nLokales Verzeichnis, in dem die Dateien gespeichert werden.\n nein\n\n\npassiveMode\nBoolean\nAktiv- oder Passiv-Verbindungsmodus. Default: Passiv (true)\n ja\n\n\npassword\nString\nPasswort für den Zugriff auf dem Server.\n nein\n\n\nremoteDir\nString\nVerzeichnis auf dem Server.\n nein\n\n\nremoteFile\nObject\nDateiname oder Liste der Dateinamen auf dem Server (kann auch ein Muster sein (* oder ?)). Ohne diesen Parameter werden alle Dateien aus dem Remoteverzeichnis heruntergeladen.\n ja\n\n\nserver\nString\nName des Servers (ohne ftp://).\n nein\n\n\nsystemType\nString\nUNIX oder WINDOWS. Default: UNIX.\n ja\n\n\nuser\nString\nBenutzername auf dem Server.\n nein\n\n\n\n\n\n\nLiefert eine Liste der Dateien aus dem definierten Verzeichnis des Servers.\nBeispiel:\ntasks.register('ftplist', FtpList) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir= \"\"\n    doLast {\n        println files\n    }\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncontrolKeepAliveTimeout\nLong\nTimeout bis ein NOOP über den Kontroll-Kanal versendet wird. Default: 300s (=5 Minuten)\n ja\n\n\nfileSeparator\nString\nDefault: /. (Falls systemType Windows ist, ist der Default \\.\n ja\n\n\npassiveMode\nBoolean\nAktiv- oder Passiv-Verbindungsmodus. Default: Passiv (true)\n ja\n\n\npassword\nString\nPasswort für den Zugriff auf dem Server.\n nein\n\n\nremoteDir\nString\nVerzeichnis auf dem Server.\n nein\n\n\nserver\nString\nName des Servers (ohne ftp://).\n nein\n\n\nsystemType\nString\nUNIX oder WINDOWS. Default: UNIX.\n ja\n\n\nuser\nString\nBenutzername auf dem Server.\n nein\n\n\n\n\n\n\nExportiert alle Tabellen einer GeoPackage-Datei in DXF-Dateien. Als Input wird eine von ili2gpkg erzeugte GeoPackage-Datei benötigt.\nEs werden alle INTERLIS-Klassen exportiert (SELECT tablename FROM T_ILI2DB_TABLE_PROP WHERE setting = 'CLASS'). Der eigentliche SELECT-Befehl ist komplizierter weil für das Layern der einzelnen DXF-Objekte das INTERLIS-Metaattribut !!@dxflayer=\"true\" ausgelesen wird. Gibt es kein solches Metaattribut wird alles in den gleichen DXF-Layer (default) geschrieben.\nEncoding: Die DXF-Dateien sind ISO-8859-1 encodiert.\nAchtung: Task sollte verbessert werden (siehe E-Mail Claude im Rahmen des Publisher-Projektes).\ntasks.register('gpkg2dxf', Gpkg2Dxf) {\n    dataFile = file(\"data.gpkg\")\n    outputDir = file(\"./out/\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nFile\nGeoPackage-Datei, die nach DXF transformiert werden soll.\n nein\n\n\noutputDir\nFile\nVerzeichnis, in das die DXF-Dateien gespeichert werden.\n nein\n\n\n\n\n\n\nExportiert alle Tabellen einer GeoPackage-Datei in Shapefiles. Als Input wird eine von ili2gpkg erzeugte GeoPackage-Datei benötigt.\nEs werden alle INTERLIS-Klassen exportiert (SELECT tablename FROM T_ILI2DB_TABLE_PROP WHERE setting = 'CLASS'). Je nach, bei der Erstellung der GeoPackage-Datei, verwendeten Parametern, muss die Query angepasst werden. Es muss jedoch darauf geachtet werden, dass es nur eine Query gibt (für alle Datensätze). Für den vorgesehenen Anwendungsfall (sehr einfache, flache Modelle) dürfte das kein Problem darstellen.\nEncoding: Die Shapefiles sind neu UTF-8 encodiert. Standard ist ISO-8859-1, scheint aber v.a. in QGIS nicht standardmässig zu funktionieren (keine Umlaute).\nAchtung: Task sollte verbessert werden (siehe E-Mail Claude im Rahmen des Publisher-Projektes).\ntasks.register('gpkg2shp', Gpkg2Shp) {\n    dataFile = file(\"data.gpkg\")\n    outputDir = file(\"./out/\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nFile\nGeoPackage-Datei, die nach Shapefile transformiert werden soll.\n nein\n\n\noutputDir\nFile\nVerzeichnis, in das die Shapefile gespeichert werden.\n nein\n\n\n\n\n\n\nDaten aus einer bestehenden Datenbanktabelle werden in eine GeoPackage-Datei exportiert.\nBeispiele:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('gpkgexport', GpkgExport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"gpkgexport\"\n    srcTableName = \"exportdata\"\n    dataFile = \"data.gpkg\"\n    dstTableName = \"exportdata\"\n}\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('gpkgexport', GpkgExport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"gpkgexport\"\n    srcTableName = [\"exportTable1\", \"exportTable2\"]\n    dataFile = \"data.gpkg\"\n    dstTableName = [\"exportTable1\", \"exportTable2\"]\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbatchSize\nInteger\nAnzahl der Records, die pro Batch in die Ziel-Datenbank (GeoPackage) geschrieben werden. Default: 5000\n ja\n\n\ndataFile\nObject\nName der GeoPackage-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndstTableName\nObject\nName der Tabelle(n) in der GeoPackage-Datei. String oder List.\n nein\n\n\nfetchSize\nInteger\nAnzahl der Records, die pro Fetch aus der Quell-Datenbank gelesen werden. Default: 5000\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\nsrcTableName\nObject\nName der DB-Tabelle(n), die exportiert werden soll(en). String oder List.\n nein\n\n\n\n\n\n\nDaten aus einer GeoPackage-Datei in eine bestehende Datenbanktabelle importieren.\nDie Tabelle kann weitere Spalten enthalten, die in der GeoPackage-Datei nicht vorkommen. Sie müssen aber NULLable sein, oder einen Default-Wert definiert haben.\nDie Gross-/Kleinschreibung der GeoPackage-Spaltennamen wird für die Zuordnung zu den DB-Spalten ignoriert.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('gpkgimport', GpkgImport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"gpkgimport\"\n    srcTableName = \"Point\"\n    dstTableName = \"importdata\"\n    dataFile = \"point.gpkg\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbatchSize\nInteger\nAnzahl der Records, die pro Batch in die Ziel-Datenbank geschrieben werden. Default: 5000\n ja\n\n\ndataFile\nObject\nName der GeoPackage-Datei, die gelesen werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, in die importiert werden soll.\n nein\n\n\ndstTableName\nString\nName der DB-Tabelle, in die importiert werden soll.\n nein\n\n\nfetchSize\nInteger\nAnzahl der Records, die pro Fetch aus der Quell-Datenbank gelesen werden. Default: 5000\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\nsrcTableName\nString\nName der GeoPackage-Tabelle, die importiert werden soll.\n nein\n\n\n\n\n\n\nPrüft eine GeoPackage-Datei gegenüber einem INTERLIS-Modell. Basiert auf dem ilivalidator.\nBeispiel:\ntasks.register('validate', GpkgValidator) {\n    models = \"GpkgModel\"\n    dataFiles = [\"attributes.gpkg\"]\n    tableName = \"Attributes\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nallObjectsAccessible\nBoolean\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n ja\n\n\nconfigFile\nObject\nKonfiguriert die Datenprüfung mit Hilfe einer ini-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\ndataFiles\nObject\nListe der Dateien, die validiert werden sollen. FileCollection oder List. Eine leere Liste ist kein Fehler.\n nein\n\n\ndisableAreaValidation\nBoolean\nSchaltet die AREA-Topologieprüfung aus. Default: false\n ja\n\n\nfailOnError\nBoolean\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n ja\n\n\nforceTypeValidation\nBoolean\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n ja\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n ja\n\n\nmetaConfigFile\nObject\nKonfiguriert den Validator mit Hilfe einer ini-Datei. Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell, gegen das die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Der Name der CSV-Datei.\n ja\n\n\nmultiplicityOff\nBoolean\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n ja\n\n\npluginFolder\nObject\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\nskipPolygonBuilding\nBoolean\nSchaltet die Bildung der Polygone aus (nur ITF). Default: false\n ja\n\n\ntableName\nString\nName der Tabelle in den GeoPackage-Dateien.\n nein\n\n\nvalidationOk\nboolean\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false.\n nein\n\n\nxtflogFile\nObject\nSchreibt die log-Meldungen in eine INTERLIS-2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n ja\n\n\n\n\n\n\nGzipped eine einzelne Datei. Es gibt einen eingebauten Tar-Task, der - nomen est omen - aber immer zuerst eine Tar-Datei erstellt.\nBeispiel:\ntasks.register('compressFile', Gzip) {\n    dataFile = file(\"./planregister.xml\");\n    gzipFile = file(\"./planregister.xml.gz\");\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nProperty&lt;File&gt;\nDatei, die gezipped werden soll.\n nein\n\n\ngzipFile\nProperty&lt;File&gt;\nOutput-Datei\n nein\n\n\n\n\n\n\nImportiert Daten aus einer INTERLIS-Transferdatei in eine GeoPackage-Datei.\nDie Tabellen werden implizit auch angelegt, falls sie noch nicht vorhanden sind. Falls die Tabellen in der Datenbank schon vorhanden sind, können sie zusätzliche Spalten enthalten (z.B. bfsnr, datum etc.), welche beim Import leer bleiben.\nFalls beim Import ein Datensatz-Identifikator (dataset) definiert wird, darf dieser Datensatz-Identifikator in der Datenbank noch nicht vorhanden sein.\nUm die bestehenden (früher importierten) Daten zu ersetzen, kann der Task Ili2gpkgReplace (not yet implemented) verwendet werden.\nDie Option --doSchemaImport wird automatisch gesetzt.\nBeispiel:\ntasks.register('importData', Ili2gpkgImport) {\n    models = \"SO_AGI_AV_GB_Administrative_Einteilungen_20180613\"\n    dataFile = file(\"data.xtf\");\n    dbfile = file(\"data.gpkg\")    \n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nString\nEntspricht der ili2gpkg-Option --baskets\n ja\n\n\ndataFile\nObject\nName der XTF-/ITF-Datei, die gelesen werden soll. Es können auch mehrere Dateien sein. FileCollection oder File\n nein\n\n\ndataset\nObject\nEntspricht der ili2gpkg-Option --dataset\n ja\n\n\ndbfile\nFile\nGeoPackage-Datei in die importiert werden soll.\n nein\n\n\ndefaultSrsCode\nString\nEntspricht der ili2gpkg-Option --defaultSrsCode\n ja\n\n\nisCoalesceJson\nBoolean\nEntspricht der ili2gpkg-Option --coalesceJson\n ja\n\n\nisCreateEnumTabs\nBoolean\nEntspricht der ili2gpkg-Option --createEnumTabs\n ja\n\n\nisCreateGeomIdx\nBoolean\nEntspricht der ili2gpkg-Option --createGeomIdx\n ja\n\n\nisCreateMetaInfo\nBoolean\nEntspricht der ili2gpkg-Option --createMetaInfo\n ja\n\n\nisDeleteData\nBoolean\nEntspricht der ili2gpkg-Option --deleteData\n ja\n\n\nisDisableAreaValidation\nBoolean\nEntspricht der ili2gpkg-Option --disableAreaValidation\n ja\n\n\nisDisableValidation\nBoolean\nEntspricht der ili2gpkg-Option --disableValidation\n ja\n\n\nisForceTypeValidation\nBoolean\nEntspricht der ili2gpkg-Option --forceTypeValidation\n ja\n\n\nisIligml20\nBoolean\nEntspricht der ili2gpkg-Option --iligml20\n ja\n\n\nisImportTid\nBoolean\nEntspricht der ili2gpkg-Option --importTid\n ja\n\n\nisNameByTopic\nBoolean\nEntspricht der ili2gpkg-Option --nameByTopic\n ja\n\n\nisSkipGeometryErrors\nBoolean\nEntspricht der ili2gpkg-Option --skipGeometryErrors\n ja\n\n\nisSkipPolygonBuilding\nBoolean\nEntspricht der ili2gpkg-Option --skipPolygonBuilding\n ja\n\n\nisStrokeArcs\nBoolean\nEntspricht der ili2gpkg-Option --strokeArcs\n ja\n\n\nisTrace\nBoolean\nEntspricht der ili2gpkg-Option --trace\n ja\n\n\nlogFile\nObject\nEntspricht der ili2gpkg-Option --logFile\n ja\n\n\nmodeldir\nString\nEntspricht der ili2gpkg-Option --modeldir\n ja\n\n\nmodels\nString\nEntspricht der ili2gpkg-Option --models\n ja\n\n\npostScript\nFile\nEntspricht der ili2gpkg-Option --postScript\n ja\n\n\npreScript\nFile\nEntspricht der ili2gpkg-Option --preScript\n ja\n\n\nproxy\nString\nEntspricht der ili2gpkg Option --proxy\n ja\n\n\nproxyPort\nInteger\nEntspricht der ili2gpkg-Option --proxyPort\n ja\n\n\ntopics\nString\nEntspricht der ili2gpkg-Option --topics\n ja\n\n\nvalidConfigFile\nFile\nEntspricht der ili2gpkg-Option --validConfigFile\n ja\n\n\n\nFür die Beschreibung der einzenen ili2gpkg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\n\nLöscht einen Datensatz in der PostgreSQL-Datenbank anhand eines Datensatz-Identifikators. Diese Funktion bedingt, dass das Datenbankschema mit der Option createBasketCol erstellt wurde (via Task Ili2pgImportSchema). Der Parameter failOnException muss false sein, ansonsten bricht der Job ab.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('deleteDataset', Ili2pgDelete) {\n    database = [db_uri, db_user, db_pass]\n    models = \"DM01AVSO24LV95\"\n    dbschema = \"dm01\"\n    dataset = \"kammersrohr\"\n}\nEs können auch mehrere Datensätze pro Task gelöscht werden:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('deleteDataset', Ili2pgDelete) {\n    database = [db_uri, db_user, db_pass]\n    dbschema = \"dm01\"\n    dataset = [\"Olten\",\"Grenchen\"]\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\n\nExportiert Daten aus der PostgreSQL-Datenbank in eine INTERLIS-Transferdatei.\nMit dem Parameter models, topics, baskets oder dataset wird definiert, welche Daten exportiert werden.\nOb die Daten im INTERLIS-1-, INTERLIS-2- oder GML-Format geschrieben werden, ergibt sich aus der Dateinamenserweiterung der Ausgabedatei. Für eine INTERLIS-1-Transferdatei muss die Erweiterung .itf verwendet werden. Für eine GML-Transferdatei muss die Erweiterung .gml verwendet werden.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('exportData', Ili2pgExport) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900-out.itf\"\n    dataset = \"254900\"\n    logFile = \"ili2pg.log\"\n}\nDamit mit einer einzigen Task-Definition mehrere Datensätze verarbeitet werden können, kann auch eine Liste von Dateinamen und Datensätzen angegeben werden.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('exportData', Ili2pgExport) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = [\"lv03_254900-out.itf\",\"lv03_255000-out.itf\"]\n    dataset = [\"254900\",\"255000\"]\n    logFile = \"ili2pg.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexport3\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --export3.\n ja\n\n\nexportModels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --exportModels.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\n\nImportiert Daten aus einer INTERLIS-Transferdatei in die PostgreSQL-Datenbank.\nDie Tabellen werden implizit auch angelegt, falls sie noch nicht vorhanden sind. Falls die Tabellen in der Datenbank schon vorhanden sind, können sie zusätzliche Spalten enthalten (z.B. bfsnr, datum etc.), welche beim Import leer bleiben.\nFalls beim Import ein Datensatz-Identifikator (dataset) definiert wird, darf dieser Datensatz-Identifikator in der Datenbank noch nicht vorhanden sein.\nFalls man mehrere Dateien importieren will, diese jedoch erst zur Laufzeit eruiert werden können, muss der Parameter dataFile eine Gradle FileCollection resp. eine implementierende Klasse (z.B. FileTree) sein. Gleiches gilt für den dataset-Parameter. Als einzelner Wert für das Dataset wird in diesem Fall der Name der Datei ohne Extension und ohne Pfad verwendet. Leider kann nicht bereits in der Task-Definition aus dem Filetree eine Liste gemacht werden, z.B. fileTree(pathToUnzipFolder) { include '*.itf' }.files.name. Diese Liste ist leer.\nUm die bestehenden (früher importierten) Daten zu ersetzen, kann der Task Ili2pgReplace verwendet werden.\nBeispiel 1:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('importData', Ili2pgImport) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900.itf\"\n    logFile = \"ili2pg.log\"\n}\nBeispiel 2:\nImport der AV-Daten. In der t_datasetname-Spalte soll die BFS-Nummer stehen. Die BFS-Nummer entspricht den ersten vier Zeichen des Filenamens.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('importData', Ili2pgImport) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = fileTree(pathToUnzipFolder) { include '*.itf' }\n    dataset = dataFile\n    datasetSubstring = (0..4).toList()\n    logFile = \"ili2pg.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndataFile\nProperty&lt;Object&gt;\nName der XTF-/ITF-Datei, die gelesen werden soll. Es können auch mehrere Dateien sein. FileCollection oder String.\n nein\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\n\nErstellt die Tabellenstruktur in der PostgreSQL-Datenbank anhand eines INTERLIS-Modells.\nDer Parameter iliFile oder models muss gesetzt werden.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('importSchema', Ili2pgImportSchema) {\n    database = [db_uri, db_user, db_pass]\n    models = \"DM01AVSO24\"\n    dbschema = \"gretldemo\"\n    logFile = \"ili2pg.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\nbeautifyEnumDispName\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --beautifyEnumDispName.\n ja\n\n\ncoalesceArray\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceArray.\n ja\n\n\ncoalesceCatalogueRef\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceCatalogueRef.\n ja\n\n\ncoalesceJson\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceJson.\n ja\n\n\ncoalesceMultiLine\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceMultiline.\n ja\n\n\ncoalesceMultiSurface\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceMultiSurface.\n ja\n\n\ncreateBasketCol\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createBasketCol.\n ja\n\n\ncreateDatasetCol\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createDatasetCol.\n ja\n\n\ncreateDateTimeChecks\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createDateTimeChecks.\n ja\n\n\ncreateEnumColAsItfCode\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createEnumColAsItfCode.\n ja\n\n\ncreateEnumTabs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createEnumTabs.\n ja\n\n\ncreateEnumTabsWithId\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createEnumTabsWithId.\n ja\n\n\ncreateEnumTxtCol\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createEnumTxtCol.\n ja\n\n\ncreateFk\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createFk.\n ja\n\n\ncreateFkIdx\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createFkIdx.\n ja\n\n\ncreateGeomIdx\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createGeomIdx.\n ja\n\n\ncreateImportTabs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createImportTabs.\n ja\n\n\ncreateMetaInfo\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createMetaInfo.\n ja\n\n\ncreateNumChecks\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createNumChecks.\n ja\n\n\ncreateSingleEnumTab\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createSingleEnumTab.\n ja\n\n\ncreateStdCols\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createStdCols.\n ja\n\n\ncreateTextChecks\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createTextChecks.\n ja\n\n\ncreateTidCol\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createTidCol.\n ja\n\n\ncreateTypeConstraint\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createTypeConstraint.\n ja\n\n\ncreateTypeDiscriminator\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createTypeDescriminator.\n ja\n\n\ncreateUnique\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createUnique.\n ja\n\n\ncreatescript\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --createscript.\n ja\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndefaultSrsAuth\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --defaultSrsAuth.\n ja\n\n\ndefaultSrsCode\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --defaultSrsCode.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableNameOptimization\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableNameOptimization.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\ndropscript\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dropscript.\n ja\n\n\nexpandMultilingual\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --expandMultilingual.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\nidSeqMax\nProperty&lt;Long&gt;\nEntspricht der ili2pg-Option --idSeqMax.\n ja\n\n\nidSeqMin\nProperty&lt;Long&gt;\nEntspricht der ili2pg-Option --idSeqMin.\n ja\n\n\niliFile\nProperty&lt;Object&gt;\nName der ili-Datei, die gelesen werden soll.\n ja\n\n\niliMetaAttrs\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --iliMetaAttrs.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nkeepAreaRef\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --keepAreaRef.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmaxNameLength\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --maxNameLength.\n ja\n\n\nmetaConfig\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --metaConfig.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\nnameByTopic\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --nameByTopic.\n ja\n\n\nnoSmartMapping\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --noSmartMapping.\n ja\n\n\noneGeomPerTable\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --oneGeomPerTable.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nsetupPgExt\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --setupPgExt.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nsmart1Inheritance\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --smart1Inheritance.\n ja\n\n\nsmart2Inheritance\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --smart2Inheritance.\n ja\n\n\nsqlColsAsText\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --sqlColsAsText.\n ja\n\n\nsqlEnableNull\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --sqlEnableNull.\n ja\n\n\nsqlExtRefCols\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --sqlExtRefCols.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\nt_id_Name\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --t_id_Name.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\ntranslation\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --translation.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\n\nErsetzt die Daten in der PostgreSQL-Datenbank anhand eines Datensatz-Identifikators (dataset) mit den Daten aus einer INTERLIS-Transferdatei. Diese Funktion bedingt, dass das Datenbankschema mit der Option createBasketCol erstellt wurde (via Task Ili2pgImportSchema).\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('replaceData', Ili2pgReplace) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900.itf\"\n    dataset = \"254900\"\n    logFile = \"ili2pg.log\"\n}\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('importData', Ili2pgReplace) {\n    database = [db_uri, db_user, db_pass]\n    models = 'DM01AVSO24LV95'\n    dbschema = 'agi_dm01avso24'\n    dataFile = fileTree(dir: dm01SoDir, include: '*.itf')\n    dataset = dataFile\n    datasetSubstring = 0..4\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndataFile\nProperty&lt;Object&gt;\nnull\n nein\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\n\nAktualisiert die Daten in der PostgreSQL-Datenbank anhand einer INTERLIS-Transferdatei, d.h. neue Objekte werden eingefügt, bestehende Objekte werden aktualisiert und in der Transferdatei nicht mehr vorhandene Objekte werden gelöscht.\nDiese Funktion bedingt, dass das Datenbankschema mit der Option --createBasketCol erstellt wurde (via Task Ili2pgImportSchema), und dass die Klassen und Topics eine stabile OID haben.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('updateData', Ili2pgUpdate) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900.itf\"\n    dataset = \"254900\"\n    logFile = \"ili2pg.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndataFile\nProperty&lt;Object&gt;\nnull\n nein\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\n\nPrüft die Daten ohne diese in eine Datei zu exportieren. Der Task ist erfolgreich, wenn keine Fehler gefunden werden und ist nicht erfolgreich, wenn Fehler gefunden werden. Mit der Option failOnException=false ist der Task erfolgreich, auch wenn Fehler gefunden werden.\nMit dem Parameter --models, --topics, --baskets oder --dataset wird definiert, welche Daten geprüft werden. Parameter --dataset akzeptiert auch eine Liste von Datasets.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('validate', Ili2pgValidate) {\n    database = [db_uri, db_user, db_pass]\n    models = \"SO_AGI_AV_GB_Administrative_Einteilungen_20180613\"\n    modeldir = rootProject.projectDir.toString() + \";http://models.interlis.ch\"\n    dbschema = \"agi_av_gb_admin_einteilungen_fail\"\n    logFile = file(\"fubar.log\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\n\nPrüft eine INTERLIS-Datei (.itf oder .xtf) gegenüber einem INTERLIS-Modell (.ili). Basiert auf dem ilivalidator.\nBeispiel:\ntasks.register('validate', IliValidator) {\n    dataFiles = [\"Beispiel2a.xtf\"]\n    logFile = \"ilivalidator.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nallObjectsAccessible\nBoolean\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n ja\n\n\nconfigFile\nObject\nKonfiguriert die Datenprüfung mit Hilfe einer ini-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\ndataFiles\nObject\nListe der Dateien, die validiert werden sollen. FileCollection oder List. Eine leere Liste ist kein Fehler.\n nein\n\n\ndisableAreaValidation\nBoolean\nSchaltet die AREA-Topologieprüfung aus. Default: false\n ja\n\n\nfailOnError\nBoolean\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n ja\n\n\nforceTypeValidation\nBoolean\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n ja\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n ja\n\n\nmetaConfigFile\nObject\nKonfiguriert den Validator mit Hilfe einer ini-Datei. Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell, gegen das die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Der Name der CSV-Datei.\n ja\n\n\nmultiplicityOff\nBoolean\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n ja\n\n\npluginFolder\nObject\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\nskipPolygonBuilding\nBoolean\nSchaltet die Bildung der Polygone aus (nur ITF). Default: false\n ja\n\n\nvalidationOk\nboolean\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false.\n nein\n\n\nxtflogFile\nObject\nSchreibt die log-Meldungen in eine INTERLIS-2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n ja\n\n\n\nZusatzfunktionen (Custom Functions): Die pluginFolder-Option ist zum jetzigen Zeitpunkt ohne Wirkung. Die Zusatzfunktionen werden als normale Abhängigkeit definiert und in der ilivalidator-Task-Implementierung registriert. Das Laden der Klassen zur Laufzeit in iox-ili hat nicht funktioniert (NoClassDefFoundError…). Der Plugin-Mechanismus von ilivalidator wird momentan ohnehin geändert (“Ahead-Of-Time-tauglich” gemacht).\n\n\n\nDaten aus einer Json-Datei in eine Datenbanktabelle importieren. Die gesamte Json-Datei (muss UTF-8 encoded sein) wird als Text in eine Spalte importiert. Ist das Json-Objekt in der Datei ein Top-Level-Array wird für jedes Element des Arrays ein Record in der Datenbanktabelle erzeugt.\nBeispiel:\ntasks.register('importJson', JsonImport) {\n    database = [db_uri, db_user, db_pass]\n    jsonFile = \"data.json\"\n    qualifiedTableName = \"jsonimport.jsonarray\"\n    columnName = \"json_text_col\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncolumnName\nString\nSpaltenname der Tabelle, in die importiert werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, in die importiert werden soll.\n nein\n\n\nisDeleteAllRows\nBoolean\nInhalt der Tabelle vorgängig löschen?\n ja\n\n\njsonFile\nString\nJSON-Datei, die importiert werden soll.\n nein\n\n\nqualifiedTableName\nString\nQualifizierter Namen der Tabelle (“schema.tabelle”), in die importiert werden soll.\n nein\n\n\n\n\n\n\nExportiert eine PostGIS-Raster-Spalte in eine Raster-Datei mittels SQL-Query. Die SQL-Query darf nur einen Record zurückliefern, d.h. es muss unter Umständen ST_Union() verwendet werden. Es wird angenommen, dass die erste bytea-Spalte des Resultsets die Rasterdaten enthält. Weitere bytea-Spalten werden ignoriert. Das Outputformat und die Formatoptionen müssen in der SQL-Datei (in der Select-Query) angegeben werden, z.B.:\nSELECT\n    1::int AS foo, ST_AsGDALRaster((ST_AsRaster(ST_Buffer(ST_Point(2607880,1228287),10),150, 150)), 'AAIGrid', ARRAY[''], 2056) AS raster\n;\nBeispiel:\ntasks.register('exportTiff', PostgisRasterExport) {\n    database = [db_uri, db_user, db_pass]\n    sqlFile = \"raster.sql\"\n    dataFile = \"export.tif\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nObject\nName der Rasterdatei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, aus der exportiert werden soll.\n nein\n\n\nsqlFile\nString\nName der SQL-Datei aus der das SQL-Statement gelesen und ausgeführt wird.\n nein\n\n\nsqlParameters\nMap&lt;String,String&gt;\nEine Map mit Paaren von Parameter-Name und Parameter-Wert.\n ja\n\n\n\n\n\n\nStellt für Vektordaten die aktuellsten Geodaten-Dateien bereit und pflegt das Archiv der vorherigen Zeitstände.\nDetails\n\n\n\nKopiert Objekte von einem Bucket in einen anderen. Die Buckets müssen in der gleichen Region sein. Die Permissions werden nicht mitkopiert und müssen explizit gesetzt werden.\nBeispiel:\ntasks.register('copyFiles', S3Bucket2Bucket) {\n    accessKey = s3AccessKey\n    secretKey = s3SecretKey\n    sourceBucket = s3SourceBucket\n    targetBucket = s3TargetBucket\n    acl = \"public-read\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\naccessKey\nString\nAccessKey\n nein\n\n\nacl\nString\nAccess Control Layer [private, public-read, public-read-write, authenticated-read, aws-exec-read, bucket-owner-read, bucket-owner-full-control]\n nein\n\n\nendPoint\nString\nS3-Endpunkt. Default: https://s3.eu-central-1.amazonaws.com/\n ja\n\n\nmetaData\nMap&lt;String,String&gt;\nMetadaten des Objektes resp. der Objekte, z.B. [\"lastModified\":\"2020-08-28\"].\n ja\n\n\nregion\nString\nS3-Region. Default: eu-central-1\n ja\n\n\nsecretKey\nString\nSecretKey\n nein\n\n\nsourceBucket\nString\nBucket, aus dem die Objekte kopiert werden.\n nein\n\n\ntargetBucket\nString\nBucket, in den die Objekte kopiert werden.\n nein\n\n\n\n\n\n\nLädt eine Datei aus einem S3-Bucket herunter.\nBeispiel:\ntasks.register('downloadFile', S3Download) {\n    accessKey = 'abcdefg'\n    secretKey = 'hijklmnopqrstuvwxy'\n    downloadDir = file(\"./path/to/dir/\")\n    bucketName = \"ch.so.ada.denkmalschutz\"\n    key = \"foo.pdf\"\n    endPoint = \"https://s3.eu-central-1.amazonaws.com\" \n    region = \"eu-central-1\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\naccessKey\nString\nAccessKey\n nein\n\n\nbucketName\nString\nName des Buckets, in dem die Datei gespeichert ist.\n nein\n\n\ndownloadDir\nFile\nVerzeichnis, in das die Datei heruntergeladen werden soll.\n nein\n\n\nendPoint\nString\nS3-Endpunkt. Default: https://s3.eu-central-1.amazonaws.com/\n ja\n\n\nkey\nString\nName der Datei.\n nein\n\n\nregion\nString\nS3-Region. Default: eu-central-1\n ja\n\n\nsecretKey\nString\nSecretKey\n nein\n\n\n\n\n\n\nLädt ein Dokument (sourceFile) oder alle Dokumente in einem Verzeichnis (sourceDir) in einen S3-Bucket (bucketName) hoch.\nMit dem passenden Content-Typ kann man das Verhalten des Browsers steuern. Default ist ‘application/octect-stream’, was dazu führt, dass die Datei immer heruntergeladen wird. Soll z.B. ein PDF oder ein Bild im Browser direkt angezeigt werden, muss der korrekte Content-Typ gewählt werden.\nBeispiel:\ntasks.register('uploadDirectory', S3Upload) {\n    accessKey = 'abcdefg'\n    secretKey = 'hijklmnopqrstuvwxy'\n    sourceDir = file(\"./docs\")\n    bucketName = \"ch.so.ada.denkmalschutz\"\n    endPoint = \"https://s3.eu-central-1.amazonaws.com\" \n    region = \"eu-central-1\"\n    acl = \"public-read\"\n    contentType = \"application/pdf\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\naccessKey\nString\nAccessKey\n nein\n\n\nacl\nString\nAccess Control Layer [private, public-read, public-read-write, authenticated-read, aws-exec-read, bucket-owner-read, bucket-owner-full-control]\n nein\n\n\nbucketName\nString\nName des Buckets, in dem die Dateien gespeichert werden sollen.\n nein\n\n\ncontentType\nString\nContent-Type\n ja\n\n\nendPoint\nString\nS3-Endpunkt. Default: https://s3.eu-central-1.amazonaws.com/\n ja\n\n\nmetaData\nMap&lt;String,String&gt;\nMetadaten des Objektes resp. der Objekte, z.B. [\"lastModified\":\"2020-08-28\"]\n ja\n\n\nregion\nString\nS3-Region. Default: eu-central-1\n nein\n\n\nsecretKey\nString\nSecretKey\n nein\n\n\nsourceDir\nObject\nVerzeichnis mit den Dateien, die hochgeladen werden sollen.\n ja\n\n\nsourceFile\nObject\nDatei, die hochgeladen werden soll.\n ja\n\n\nsourceFiles\nObject\nFileCollection mit den Dateien, die hochgeladen werden sollen, z.B. fileTree(\"/path/to/directoy/\") { include \"*.itf\" }\n ja\n\n\n\n\n\n\nDaten aus einer bestehenden Datenbanktabelle werden in eine SHP-Datei exportiert.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('shpexport', ShpExport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"shpexport\"\n    tableName = \"exportdata\"\n    dataFile = \"data.shp\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nObject\nName der SHP-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, aus der exportiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der SHP-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\ntableName\nString\nName der DB-Tabelle, die exportiert werden soll.\n nein\n\n\n\n\n\n\nDaten aus einer SHP-Datei in eine bestehende Datenbanktabelle importieren.\nDie Tabelle kann weitere Spalten enthalten, die in der SHP-Datei nicht vorkommen. Sie müssen aber NULLable sein, oder einen Default-Wert definiert haben.\nDie Tabelle muss eine Geometriespalte enthalten. Der Name der Geometriespalte kann beliebig gewählt werden.\nDie Gross-/Kleinschreibung der SHP-Spaltennamen wird für die Zuordnung zu den DB-Spalten ignoriert.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('shpimport', ShpImport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"shpimport\"\n    tableName = \"importdata\"\n    dataFile = \"data.shp\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nObject\nName der SHP-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, aus der exportiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der SHP-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\ntableName\nString\nName der DB-Tabelle, die exportiert werden soll.\n nein\n\n\n\n\n\n\nPrüft eine SHP-Datei gegenüber einem INTERLIS-Modell. Basiert auf dem ilivalidator.\nIm gegebenen Modell wird eine Klasse gesucht, die genau die Attributenamen wie in der Shp-Datei enthält (wobei die Gross-/Kleinschreibung ignoriert wird); die Attributtypen werden ignoriert. Wird keine solche Klasse gefunden, gilt das als Validierungsfehler.\nDie Prüfung von gleichzeitig mehreren Shapefiles führt zu Fehlermeldungen wie OID o3158 of object &lt;Modelname&gt;.&lt;Topicname&gt;.&lt;Klassenname&gt; already exists in .... Beim Öffnen und Lesen eines Shapefiles wird immer der Zähler, der die interne (im Shapefile nicht vorhandene) OID generiert, zurückgesetzt. Somit kann immer nur ein Shapefile pro Task geprüft werden.\nBeispiel:\ntasks.register('validate', ShpValidator) {\n    models = \"ShpModel\"\n    dataFiles = [\"data.shp\"]\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nallObjectsAccessible\nBoolean\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n ja\n\n\nconfigFile\nObject\nKonfiguriert die Datenprüfung mit Hilfe einer ini-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\ndataFiles\nObject\nListe der Dateien, die validiert werden sollen. FileCollection oder List. Eine leere Liste ist kein Fehler.\n nein\n\n\ndisableAreaValidation\nBoolean\nSchaltet die AREA-Topologieprüfung aus. Default: false\n ja\n\n\nencoding\nString\nZeichencodierung der SHP-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nfailOnError\nBoolean\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n ja\n\n\nforceTypeValidation\nBoolean\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n ja\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n ja\n\n\nmetaConfigFile\nObject\nKonfiguriert den Validator mit Hilfe einer ini-Datei. Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell, gegen das die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Der Name der CSV-Datei.\n ja\n\n\nmultiplicityOff\nBoolean\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n ja\n\n\npluginFolder\nObject\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\nskipPolygonBuilding\nBoolean\nSchaltet die Bildung der Polygone aus (nur ITF). Default: false\n ja\n\n\nvalidationOk\nboolean\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false.\n nein\n\n\nxtflogFile\nObject\nSchreibt die log-Meldungen in eine INTERLIS-2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n ja\n\n\n\n\n\n\nDer SqlExecutor-Task dient dazu, Datenumbauten auszuführen.\nEr wird im Allgemeinen dann benutzt, wenn\n\nder Datenumbau komplex ist und deshalb nicht im Db2Db-Task erledigt werden kann\noder wenn die Quell-DB keine PostgreSQL-DB ist (weil bei komplexen Queries für den Datenumbau möglicherweise fremdsystemspezifische SQL-Syntax verwendet werden müsste)\noder wenn Quell- und Zielschema in derselben Datenbank liegen\n\nIn den Fällen 1 und 2 werden Stagingtabellen bzw. ein Stagingschema benötigt, in welche der Db2Db-Task die Daten zuerst 1:1 hineinschreibt. Der SqlExecutor-Task liest danach die Daten von dort, baut sie um und schreibt sie dann ins Zielschema. Die Queries für den SqlExecutor-Task können alle in einem einzelnen .sql-File sein oder (z.B. aus Gründen der Strukturierung oder Organisation) auf mehrere .sql-Dateien verteilt sein. Die Reihenfolge der .sql-Dateien ist relevant. Dies bedeutet, dass die SQL-Befehle des zuerst angegebenen .sql-Datei zuerst ausgeführt werden müssen, danach dies SQL-Befehle des an zweiter Stelle angegebenen .sql-Datei, usw.\nDer SqlExecutor-Task muss neben Updates ganzer Tabellen (d.h. Löschen des gesamten Inhalts einer Tabelle und gesamter neuer Stand in die Tabelle schreiben) auch Updates von Teilen von Tabellen zulassen. D.h. es muss z.B. möglich sein, innerhalb einer Tabelle nur die Objekte einer bestimmten Gemeinde zu aktualisieren. Darum ist es möglich innerhalb der .sql-Datei Paramater zu verwenden und diesen Parametern beim Task einen konkreten Wert zuzuweisen. Innerhalb der .sql-Datei werden Paramter mit folgender Syntax verwendet: ${paramName}.\nUnterstützte Datenbanken: PostgreSQL, SQLite, Oracle, Derby und DuckDB.\nBeispiele:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('executeSomeSql', SqlExecutor) {\n    database = [db_uri, db_user, db_pass]\n    sqlParameters = [dataset:'Olten']\n    sqlFiles = ['demo.sql']\n}\nDamit mit einer einzigen Task-Definition mehrere Datensätze verarbeitet werden können, kann auch eine Liste von Parametern angegeben werden.\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('executeSomeSql', SqlExecutor) {\n    database = [db_uri, db_user, db_pass]\n    sqlParameters = [[dataset:'Olten'],[dataset:'Grenchen']]\n    sqlFiles = ['demo.sql']\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank, in die importiert werden soll.\n nein\n\n\nsqlFiles\nListProperty&lt;String&gt;\nName der SQL-Datei aus der SQL-Statements gelesen und ausgeführt werden.\n nein\n\n\nsqlParameters\nProperty&lt;Object&gt;\nEine Map mit Paaren von Parameter-Name und Parameter-Wert (Map&lt;String,String&gt;). Oder eine Liste mit Paaren von Parameter-Name und Parameter-Wert (List&lt;Map&lt;String,String&gt;&gt;).\n ja\n\n\n\n\n\n\nTransformiert eine Datei mittels einer XSL-Transformation ein eine andere Datei. Ist der xslFile-Parameter ein String, wird erwartet, dass die Datei im GRETL-Verzeichnis im Ressourcenordner src/main/resources/xslt-Verzeichnis gespeichert ist. Falls der xslFile-Parameter ein File-Objekt ist, können lokale Dateien verwendet werden.\nBeispiele:\ntasks.register('transform', XslTransformer) {\n    xslFile = \"eCH0132_to_SO_AGI_SGV_Meldungen_20221109.xsl\"\n    xmlFile = file(\"MeldungAnGeometer_G-0111102_20221103_145001.xml\")\n    outDirectory = file(\".\")\n}\ntasks.register('transform', XslTransformer) {\n    xslFile = file(\"path/to/eCH0132_to_SO_AGI_SGV_Meldungen_20221109.xsl\")\n    xmlFile = file(\"MeldungAnGeometer_G-0111102_20221103_145001.xml\")\n    outDirectory = file(\".\")\n}\ntasks.register('transform', XslTransformer) {\n    xslFile = \"eCH0132_to_SO_AGI_SGV_Meldungen_20221109.xsl\"\n    xmlFile = fileTree(\".\").matching {\n        include \"*.xml\"\n    }\n    outDirectory = file(\".\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nfileExtension\nString\nFileextension der Resultatdatei. Default: xtf\n ja\n\n\noutDirectory\nFile\nVerzeichnis, in das die transformierte Datei gespeichert wird. Der Name der transformierten Datei entspricht standardmässig dem Namen der Input-Datei mit Endung .xtf.\n nein\n\n\nxmlFile\nObject\nDatei oder FileTree, die/der transformiert werden soll.\n nein\n\n\nxslFile\nObject\nName der XSLT-Datei, die im src/main/resources/xslt-Verzeichnis liegen muss oder File-Objekt (beliebiger Pfad).\n nein"
  },
  {
    "objectID": "gretl/docs/user/reference.html#systemanforderungen",
    "href": "gretl/docs/user/reference.html#systemanforderungen",
    "title": "Referenzdokumentation",
    "section": "",
    "text": "Für die aktuelle GRETL-Version wird Java 11 und Gradle 7.6 benötigt."
  },
  {
    "objectID": "gretl/docs/user/reference.html#installation",
    "href": "gretl/docs/user/reference.html#installation",
    "title": "Referenzdokumentation",
    "section": "",
    "text": "GRETL selbst muss als Gradle-Plugin nicht explizit installiert werden, sondern wird dynamisch durch das Internet bezogen."
  },
  {
    "objectID": "gretl/docs/user/reference.html#kleines-beispiel",
    "href": "gretl/docs/user/reference.html#kleines-beispiel",
    "title": "Referenzdokumentation",
    "section": "",
    "text": "Erstellen Sie in einem neuen Verzeichnis gretldemo eine neue Datei build.gradle:\nimport ch.so.agi.gretl.tasks.*\nimport ch.so.agi.gretl.api.*\n\napply plugin: 'ch.so.agi.gretl'\n\nbuildscript {\n    repositories {\n        maven { url \"https://jars.interlis.ch\" }\n        maven { url \"https://repo.osgeo.org/repository/release/\" }\n        maven { url \"https://plugins.gradle.org/m2/\" }\n        maven { url \"https://s01.oss.sonatype.org/service/local/repositories/releases/content/\" }\n        maven { url \"https://s01.oss.sonatype.org/service/local/repositories/snapshots/content/\" }\n        mavenCentral()\n    }\n    dependencies {\n        classpath group: 'ch.so.agi', name: 'gretl',  version: '3.0.+'\n    }\n}\n\ndefaultTasks 'validate'\n\ntask validate(type: IliValidator){\n    dataFiles = [\"BeispielA.xtf\"]\n}\nDie Datei build.gradle ist die Job-Konfiguration. Dieser kleine Beispiel-Job besteht nur aus einem einzigen Task: validate.\nErstellen Sie nun noch die Datei BeispielA.xtf (damit danach der Job erfolgreich ausgeführt werden kann).\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;TRANSFER xmlns=\"http://www.interlis.ch/INTERLIS2.3\"&gt;\n    &lt;HEADERSECTION SENDER=\"gretldemo\" VERSION=\"2.3\"&gt;\n    &lt;/HEADERSECTION&gt;\n    &lt;DATASECTION&gt;\n        &lt;OeREBKRMtrsfr_V1_1.Transferstruktur BID=\"B01\"&gt;\n        &lt;/OeREBKRMtrsfr_V1_1.Transferstruktur&gt;\n    &lt;/DATASECTION&gt;\n&lt;/TRANSFER&gt;\nUm den Job auszuführen, wechseln Sie ins Verzeichnis mit der Job-Konfiguration, und geben da das Kommando gradle ohne Argument ein:\ncd gretldemo\ngradle\nBUILD SUCCESSFUL zeigt an, dass der Job (die Validierung der Datei BeispielA.xtf) erfolgreich ausgeführt wurde.\nUm dieselbe Job-Konfiguration für verschiedene Datensätze verwenden zu können, muss es parametrisierbar sein. Die Jobs/Tasks können so generisch konfiguriert werden, dass dieselbe Konfiguration z.B. für Daten aus verschiedenen Gemeinden benutzt werden kann. Parameter für die Job Konfiguration können z.B. mittels gradle-Properties (Gradle properties and system properties) dem Job mitgegeben werden, also z.B.\ncd gretldemo\ngradle -Pdataset=Olten"
  },
  {
    "objectID": "gretl/docs/user/reference.html#ausführen",
    "href": "gretl/docs/user/reference.html#ausführen",
    "title": "Referenzdokumentation",
    "section": "",
    "text": "Um GRETL auszuführen, geben Sie auf der Kommandozeile folgendes Kommando ein (wobei jobfolder der absolute Pfad zu ihrem Verzeichnis mit der Job Konfiguration ist.)\ngradle --project-dir jobfolder\nAlternativ können Sie auch ins Verzeichnis mit der Job Konfiguration wechseln, und da das Kommando gradle ohne Argument verwenden:\ncd jobfolder\ngradle"
  },
  {
    "objectID": "gretl/docs/user/reference.html#tasks",
    "href": "gretl/docs/user/reference.html#tasks",
    "title": "Referenzdokumentation",
    "section": "",
    "text": "Transformiert eine INTERLIS1-Transferdatei im kantonalen AV-DM01-Modell in das Bundesmodell. Unterstützt werden die Sprachen Deutsch und Italienisch und der Bezugrahmen LV95. Getestet mit Daten aus den Kantonen Solothurn, Glarus und Tessin. Weitere Informationen sind in der Basisbibliothek zu finden: https://github.com/sogis/av2ch.\nDas Bundes-ITF hat denselben Namen wie das Kantons-ITF.\nAufgrund der sehr vielen Logging-Messages einer verwendeten Bibliothek, wird der System.err-Ouput nach dev/null https://github.com/sogis/av2ch/blob/master/src/main/java/ch/so/agi/av/Av2ch.java#L75.\ntasks.register('transform', Av2ch) {\n    inputFile = file(\"254900.itf\")\n    outputDirectory = file(\"output\")\n}\nEs können auch mehrere Dateien gleichzeitig transformiert werden:\ntasks.register('transform', Av2ch) {\n    inputFile = fileTree(\".\").matching {\n            include\"*.itf\"\n        }\n    outputDirectory = file(\"output\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ninputFile\nObject\nZu transformierende ITF-Datei(en). File- oder FileCollection-Objekt.\n nein\n\n\nlanguage\nString\nSprache des Modelles / der Datei (de, it). Default: de\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\noutputDirectory\nObject\nName des Verzeichnisses in das die zu erstellende Datei geschrieben wird.\n nein\n\n\nzip\nBoolean\nDie zu erstellende Datei wird gezippt. Default: false\n ja\n\n\n\n\n\n\nAv2geobau konvertiert eine Interlis-Transferdatei (itf) in eine DXF-Geobau Datei. Av2geobau funktioniert ohne Datenbank.\nDie ITF-Datei muss dem Modell DM01AVCH24LV95D entsprechen. Die Daten werden nicht validiert.\nDie Datenstruktur der DXF-Datei ist im Prinzip sehr einfach: Die verschiedenen Informationen aus dem Datenmodell DM01 werden in verschiedene DXF-Layer abgebildet, z.B. die begehbaren LFP1 werden in den Layer “01111” abgebildet. Oder die Gebäude in den Layer “01211”.\nDer Datenumbau ist nicht konfigurierbar.\ntasks.register('av2geobau', Av2geobau) {\n    itfFiles = \"ch_254900.itf\"\n    dxfDirectory = \"./out/\"\n}\nEs können auch mehrere Dateien angegeben werden.\ntasks.register('av2geobau', Av2geobau) {\n    itfFiles = fileTree(\".\").matching {\n        include\"*.itf\"\n    }\n    dxfDirectory = \"./out/\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndxfDirectory\nObject\nVerzeichnis, in das die DXF-Dateien gespeichert werden.\n nein\n\n\nisZip\nBoolean\nDie zu erstellende Datei wird gezippt und es werden zusätzliche Dateien (Musterplan, Layerbeschreibung, Hinweise) hinzugefügt. Default: false\n ja\n\n\nitfFiles\nObject\nITF-Datei, die nach DXF transformiert werden soll. Es können auch mehrere Dateien angegeben werden. File- oder FileCollection-Objekt.\n nein\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Konvertierung in eine Text-Datei.\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\n\n\n\n\nKonvertiert eine CSV-Datei in eine Excel-Datei (*.xlsx). Datentypen werden anhand eines INTERLIS-Modelles eruiert. Fehlt das Modell, wird alles als Text gespeichert. Die Daten werden vollständig im Speicher vorgehalten. Falls grosse Dateien geschrieben werden müssen, kann das zu Problemen führen. Dann müsste die Apache POI SXSSF Implementierung (Streaming) verwendet werden.\nBeispiel:\ntasks.register('convertData', Csv2Excel) {\n    csvFile = file(\"./20230124_sap_Gebaeude.csv\")\n    firstLineIsHeader = true\n    valueDelimiter = null\n    valueSeparator = \";\"\n    encoding = \"ISO-8859-1\";\n    models = \"SO_HBA_Gebaeude_20230111\";\n    outputDir = file(\".\");\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncsvFile\nFile\nCSV-Datei, die konvertiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der CSV-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nfirstLineIsHeader\nBoolean\nDefiniert, ob eine Headerzeile geschrieben werden soll, oder nicht. Default: true\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell für Definition der Datentypen in der Excel-Datei.\n ja\n\n\noutputDir\nFile\nVerzeichnis, in das die Excel-Datei gespeichert wird. Default: Verzeichnis, in dem die CSV-Datei vorliegt.\n ja\n\n\nvalueDelimiter\nCharacter\nZeichen, das am Anfang und Ende jeden Wertes geschrieben werden soll. Default \"\n ja\n\n\nvalueSeparator\nCharacter\nZeichen, das als Trennzeichen zwischen den Werten verwendet werden soll. Default: ,\n ja\n\n\n\n\n\n\nDaten aus einer bestehenden Datenbanktabelle werden in eine CSV-Datei exportiert. Geometriespalten können nicht exportiert werden.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('csvexport', CsvExport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"csvexport\"\n    tableName = \"exportdata\"\n    firstLineIsHeader = true\n    attributes = [ \"t_id\",\"Aint\"]\n    dataFile = \"data.csv\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nattributes\nString[]\nSpalten der DB-Tabelle, die exportiert werden sollen. Definiert die Reihenfolge der Spalten in der CSV-Datei. Default: alle Spalten\n ja\n\n\ndataFile\nObject\nName der CSV-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank aus der exportiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der CSV-Datei, z.B. “UTF-8”. Default: Systemeinstellung\n ja\n\n\nfirstLineIsHeader\nBoolean\nDefiniert, ob eine Headerzeile geschrieben werden soll, oder nicht. Default: true\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\ntableName\nString\nName der DB-Tabelle, die exportiert werden soll.\n nein\n\n\nvalueDelimiter\nCharacter\nZeichen, das am Anfang und Ende jeden Wertes geschrieben werden soll. Default \"\n ja\n\n\nvalueSeparator\nCharacter\nZeichen, das als Trennzeichen zwischen den Werten verwendet werden soll. Default: ,\n ja\n\n\n\n\n\n\nDaten aus einer CSV-Datei werden in eine bestehende Datenbanktabelle importiert.\nDie Tabelle kann weitere Spalten enthalten, die in der CSV-Datei nicht vorkommen. Sie müssen aber NULLable sein, oder einen Default-Wert definiert haben.\nGeometriepalten können nicht importiert werden.\nDie Gross-/Kleinschreibung der CSV-Spaltennamen wird für die Zuordnung zu den DB-Spalten ignoriert.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('csvimport',  CsvImport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"csvimport\"\n    tableName = \"importdata\"\n    firstLineIsHeader = true\n    dataFile = \"data1.csv\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nattributes\nString[]\nSpalten der DB-Tabelle, die exportiert werden sollen. Definiert die Reihenfolge der Spalten in der CSV-Datei. Default: alle Spalten\n ja\n\n\ndataFile\nObject\nName der CSV-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank aus der exportiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der CSV-Datei, z.B. “UTF-8”. Default: Systemeinstellung\n ja\n\n\nfirstLineIsHeader\nBoolean\nDefiniert, ob eine Headerzeile geschrieben werden soll, oder nicht. Default: true\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\ntableName\nString\nName der DB-Tabelle, die exportiert werden soll.\n nein\n\n\nvalueDelimiter\nCharacter\nZeichen, das am Anfang und Ende jeden Wertes geschrieben werden soll. Default \"\n ja\n\n\nvalueSeparator\nCharacter\nZeichen, das als Trennzeichen zwischen den Werten verwendet werden soll. Default: ,\n ja\n\n\n\n\n\n\nPrüft eine CSV-Datei gegenüber einem INTERLIS-Modell. Basiert auf dem ilivalidator. Das Datenmodell darf die OID nicht als UUID modellieren (OID AS INTERLIS.UUIDOID).\nBeispiel:\ntasks.register('validate', CsvValidator) {\n    models = \"CsvModel\"\n    firstLineIsHeader = true\n    dataFiles = [\"data1.csv\"]\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nallObjectsAccessible\nBoolean\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n ja\n\n\nconfigFile\nObject\nKonfiguriert die Datenprüfung mit Hilfe einer ini-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\ndataFiles\nObject\nListe der Dateien, die validiert werden sollen. FileCollection oder List. Eine leere Liste ist kein Fehler.\n nein\n\n\ndisableAreaValidation\nBoolean\nSchaltet die AREA-Topologieprüfung aus. Default: false\n ja\n\n\nencoding\nString\nZeichencodierung der CSV-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nfailOnError\nBoolean\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n ja\n\n\nfirstLineIsHeader\nBoolean\nDefiniert, ob die CSV-Datei einer Headerzeile hat, oder nicht. Default: true\n ja\n\n\nforceTypeValidation\nBoolean\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n ja\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n ja\n\n\nmetaConfigFile\nObject\nKonfiguriert den Validator mit Hilfe einer ini-Datei. Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell, gegen das die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Der Name der CSV-Datei.\n ja\n\n\nmultiplicityOff\nBoolean\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n ja\n\n\npluginFolder\nObject\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\nskipPolygonBuilding\nBoolean\nSchaltet die Bildung der Polygone aus (nur ITF). Default: false\n ja\n\n\nvalidationOk\nboolean\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false.\n nein\n\n\nvalueDelimiter\nCharacter\nZeichen, das am Anfang und Ende jeden Wertes vorhanden ist. Default \"\n ja\n\n\nvalueSeparator\nCharacter\nZeichen, das als Trennzeichen zwischen den Werten interpretiert werden soll. Default: ,\n ja\n\n\nxtflogFile\nObject\nSchreibt die log-Meldungen in eine INTERLIS-2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n ja\n\n\n\nFalls die CSV-Datei eine Header-Zeile enthält (mit den Spaltennamen), wird im gegebenen Modell eine Klasse gesucht, welche die Header-Spaltennamen enthält (Gross-/Kleinschreibung sowie optionale “Spalten” der Modell-Klasse werden ignoriert). Wird keine solche Klasse gefunden, gilt das als Validierungsfehler.\nFalls die CSV-Datei keine Header-Zeile enthält (mit den Spaltennamen), wird im gegebenen Modell eine Klasse gesucht, die die selbe Anzahl Attribute hat. Wird keine solche Klasse gefunden, gilt das als Validierungsfehler.\nDie Prüfung von gleichzeitig mehreren CSV-Dateien führt zu Fehlermeldungen wie OID o3158 of object &lt;Modelname&gt;.&lt;Topicname&gt;.&lt;Klassenname&gt; already exists in .... Beim Öffnen und Lesen einer CSV-Datei wird immer der Zähler, der die interne (in der CSV-Datei nicht vorhandene) OID generiert, zurückgesetzt. Somit kann immer nur eine CSV-Datei pro Task geprüft werden.\n\n\n\nSimuliert mit einem HttpClient einige Curl-Befehle.\nBeispiele:\nimport ch.so.agi.gretl.tasks.Curl.MethodType;\n\ntasks.register('uploadData', Curl) {\n    serverUrl = \"https://geodienste.ch/data_agg/interlis/import\"\n    method = MethodType.POST\n    formData = [\"topic\": \"npl_waldgrenzen\", \"lv95_file\": file(\"./test.xtf.zip\"), \"publish\": \"true\", \"replace_all\": \"true\"]\n    user = \"fooUser\"\n    password = \"barPwd\"\n    expectedStatusCode = 200\n    expectedBody = \"\\\"success\\\":true\"\n}\nimport ch.so.agi.gretl.tasks.Curl.MethodType;\n\ntasks.register('uploadData', Curl) {\n    serverUrl = \"https://testweb.so.ch/typo3/api/digiplan\"\n    method = MethodType.POST\n    headers = [\"Content-Type\": \"application/xml\", \"Content-Encoding\": \"gzip\"]\n    dataBinary = file(\"./planregister.xml.gz\")\n    user = \"fooUser\"\n    password = \"barPwd\"\n    expectedStatusCode = 202\n}\nimport ch.so.agi.gretl.tasks.Curl.MethodType;\n\ntasks.register('downloadData', Curl) {\n    serverUrl = \"https://raw.githubusercontent.com/sogis/gretl/master/README.md\"\n    method = MethodType.GET\n    outputFile = file(\"./README.md\")\n    expectedStatusCode = 200\n}\nimport groovy.json.JsonSlurper\n\ndef basicHeader = \"Basic \" + Base64.getEncoder().encodeToString((clientId + \":\" + clientSecret).getBytes())\ndef token = \"\"\n\ntasks.register('getToken', Curl) {\n    serverUrl = \"https://auth.example.ch/oauth2/token\"\n    method = MethodType.POST\n    outputFile = file(\"./token.json\")\n    expectedStatusCode = 200\n    data = \"grant_type=client_credentials&client_id=$clientId\"\n    headers = [\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n        \"Authorization\": basicHeader\n    ]\n\n    doLast {\n        def slurper = new JsonSlurper();\n        def slurped = slurper.parseText(outputFile.text)\n\n        if (slurped.access_token != null)\n            token = slurped.access_token\n\n        if(token.length() == 0)\n            throw new GradleException(\"Failed to retrieve access token\")\n\n        println \"Length of access token is: \" + token.length()\n    }\n}\n\ntasks.register('downloadJson', Curl) {\n    dependsOn 'getToken'\n    serverUrl = \"https://obs.example.ch/rest/v4/docs.json?projects=83505\"\n    method = MethodType.GET\n    outputFile = file(\"./data.json\")\n    expectedStatusCode = 200\n    headers.put(\"Authorization\", providers.provider { \"Bearer \" + token })\n} \n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndata\nString\nString, der via POST hochgeladen werden soll. Entspricht curl [URL] --data.\n ja\n\n\ndataBinary\nFile\nDatei, die hochgeladen werden soll. Entspricht curl [URL] --data-binary.\n ja\n\n\nexpectedBody\nString\nErwarteter Text, der vom Server als Body zurückgelieferd wird.\n ja\n\n\nexpectedStatusCode\nInteger\nErwarteter Status Code, der vom Server zurückgeliefert wird.\n nein\n\n\nformData\nMap&lt;String,Object&gt;\nForm data parameters. Entspricht curl [URL] -F key1=value1 -F file1=@my_file.xtf.\n ja\n\n\nheaders\nMapProperty&lt;String,String&gt;\nRequest-Header. Entspricht curl [URL] -H ... -H .....\n ja\n\n\nmethod\nMethodType\nHTTP-Request-Methode. Unterstützt werden GET und POST.\n ja\n\n\noutputFile\nFile\nDatei, in die der Output gespeichert wird. Entspricht curl [URL] -o.\n ja\n\n\npassword\nString\nPasswort. Wird zusammen mit user in einen Authorization-Header umgewandelt. Entspricht curl [URL] -u user:password.\n ja\n\n\nserverUrl\nString\nDie URL des Servers inklusive Pfad und Queryparameter.\n nein\n\n\nuser\nString\nBenutzername. Wird zusammen mit password in einen Authorization-Header umgewandelt. Entspricht curl [URL] -u user:password.\n ja\n\n\n\n\n\n\nSpeichert Dokumente, deren URL in einer Spalte einer Datenbanktabelle gespeichert sind, in einem lokalen Verzeichnis. Zukünftig und bei Bedarf kann der Task so erweitert werden, dass auch BLOBs aus der Datenbank gespeichert werden können.\nRedirect von HTTP nach HTTPS funktionieren nicht. Dies korrekterweise (?) wegen der verwendeten Java-Bibliothek.\nWegen der vom Kanton Solothurn eingesetzten self-signed Zertifikate muss ein unschöner Handstand gemacht werden. Leider kann dieser Usecase schlecht getestet werden, da die Links nur in der privaten Zone verfügbar sind und die zudem noch häufig ändern können. Manuell getestet wurde es jedoch.\nAls Dateiname wird der letzte Teil des URL-Pfades verwendet, z.B. https://artplus.verw.rootso.org/MpWeb-apSolothurnDenkmal/download/2W8v0qRZQBC0ahDnZGut3Q?mode=gis wird mit den Prefix und Extension zu ada_2W8v0qRZQBC0ahDnZGut3Q.pdf.\nEs wird DISTINCT ON (&lt;documentColumn&gt;) und ein Filter WHERE &lt;documentColumn&gt; IS NOT NULL verwendet.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('exportDocuments', DatabaseDocumentExport) {\n    database = [db_uri, db_user, db_pass]\n    qualifiedTableName = \"ada_denkmalschutz.fachapplikation_rechtsvorschrift_link\"\n    documentColumn = \"multimedia_link\"\n    targetDir = file(\".\")\n    fileNamePrefix = \"ada_\"\n    fileNameExtension = \"pdf\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndatabase\nConnector\nDatenbank, aus der die Dokumente exportiert werden sollen.\n nein\n\n\ndocumentColumn\nString\nDB-Tabellenspalte mit dem Dokument resp. der URL zum Dokument.\n nein\n\n\nfileNameExtension\nString\nDateinamen-Extension.\n ja\n\n\nfileNamePrefix\nString\nPrefix für Dateinamen.\n ja\n\n\nqualifiedTableName\nString\nQualifizierter Tabellenname.\n nein\n\n\ntargetDir\nFile\nVerzeichnis in das die Dokumente exportiert werden sollen.\n nein\n\n\n\n\n\n\nDies ist prinzipiell ein 1:1-Datenkopie, d.h. es findet kein Datenumbau statt, die Quell- und die Ziel- Tabelle hat jeweils identische Attribute. Es werden auf Seite Quelle in der Regel also simple SELECT-Queries ausgeführt und die Resultate dieser Queries in Tabellen der Ziel-DB eingefügt. Unter bestimmten Bedingungen (insbesondere wenn es sich um einen wenig komplexen Datenumbau handelt), kann dieser Task aber auch zum Datenumbau benutzt werden.\nDie Queries können auf mehrere .sql-Dateien verteilt werden, d.h. der Task muss die Queries mehrerer .sql-Dateien zu einer Transaktion kombinieren können. Jede .sql-Datei gibt genau eine Resultset (RAM-Tabelle) zurück. Das Resultset wird in die konfigurierte Zieltabelle geschrieben. Die Beziehungen sind: Eine bis mehrere Quelltabellen ergeben ein Resultset; das Resultset entspricht bezüglich den Attributen genau der Zieltabelle und wird 1:1 in diese geschrieben. Der Db2Db- Task verarbeitet innerhalb einer Transaktion 1-n Resultsets und wird entsprechend auch mit 1-n SQL-Dateien konfiguriert.\nDie Reihenfolge der .sql-Dateien ist relevant. Dies bedeutet, dass die SQL-Befehle der zuerst angegebenen .sql-Datei zuerst ausgeführt werden müssen, danach die SQL-Befehle der an zweiter Stelle angegebenen .sql-Datei, usw.\nAlle SELECT-Statements werden in einer Transaktion ausgeführt werden, damit ein konsistenter Datenstand gelesen wird. Alle INSERT-Statements werden in einer Transaktion ausgeführt werden, damit bei einem Fehler der bisherige Datenstand bestehen bleibt und also kein unvollständiger Import zurückgelassen wird.\nDamit dieselbe .sql-Datei für verschiedene Datensätze benutzt werden kann, ist es möglich innerhalb der .sql-Datei Parameter zu verwenden und diesen Parametern beim Task einen konkreten Wert zuzuweisen. Innerhalb der .sql-Datei werden Paramter mit folgender Syntax verwendet: ${paramName}.\nEs ist pro .sql-Datei nur ein SELECT-Statement erlaubt. Ausser das erste Statement ist ein “SET search_path TO”-Statement.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('transferSomeData', Db2Db) {\n    sourceDb = [db_uri, db_user, db_pass]\n    targetDb = ['jdbc:sqlite:gretldemo.sqlite',null,null]\n    sqlParameters = [dataset:'Olten']\n    transferSets = [\n        new TransferSet('some.sql', 'albums_dest', true)\n    ];\n}\nDamit mit einer einzigen Task-Definition mehrere Datensätze verarbeitet werden können, kann auch eine Liste von Parametern angegeben werden.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('transferSomeData', Db2Db) {\n    sourceDb = [db_uri, db_user, db_pass]\n    targetDb = ['jdbc:sqlite:gretldemo.sqlite',null,null]\n    sqlParameters = [[dataset:'Olten'],[dataset:'Grenchen']]\n    transferSets = [\n        new TransferSet('some.sql', 'albums_dest', true)\n    ];\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbatchSize\nProperty&lt;Integer&gt;\nAnzahl der Records, die pro Batch in die Ziel-Datenbank geschrieben werden (Default: 5000). Für sehr grosse Tabellen muss ein kleinerer Wert gewählt werden.\n ja\n\n\nfetchSize\nProperty&lt;Integer&gt;\nAnzahl der Records, die auf einmal vom Datenbank-Cursor von der Quell-Datenbank zurückgeliefert werden (Standard: 5000). Für sehr grosse Tabellen muss ein kleinerer Wert gewählt werden.\n ja\n\n\nsourceDb\nListProperty&lt;String&gt;\nDatenbank, aus der gelesen werden soll.\n nein\n\n\nsqlParameters\nProperty&lt;Object&gt;\nEine Map mit Paaren von Parameter-Name und Parameter-Wert (Map&lt;String,String&gt;). Oder eine Liste mit Paaren von Parameter-Name und Parameter-Wert (List&lt;Map&lt;String,String&gt;&gt;).\n ja\n\n\ntargetDb\nListProperty&lt;String&gt;\nDatenbank, in die geschrieben werden soll.\n nein\n\n\ntransferSets\nListProperty&lt;TransferSet&gt;\nEine Liste von TransferSets.\n nein\n\n\n\nEine TransferSet ist\n\neine SQL-Datei (mit SQL-Anweisungen zum Lesen der Daten aus der sourceDb),\ndem Namen der Ziel-Tabelle in der targetDb, und\nder Angabe ob in der Ziel-Tabelle vor dem INSERT zuerst alle Records gelöscht werden sollen.\neinem optionalen vierten Parameter, der verwendet werden kann um den zu erzeugenden SQL-Insert-String zu beeinflussen, u.a. um einen WKT-Geometrie-String in eine PostGIS-Geometrie umzuwandeln\n\nBeispiel, Umwandlung Rechtswert/Hochwertspalten in eine PostGIS-Geometrie (siehe auch GRETL-Job afu_onlinerisk_transfer der eine Punktgeometriespalten aus einer Nicht-Postgis-DB übernimmt):\nnew TransferSet('untersuchungseinheit.sql', 'afu_qrcat_v1.onlinerisk_untersuchungseinheit', true, (String[])[\"geom:wkt:2056\"])\ngeom ist der Geometrie-Spalten-Name der verwendet wird.\nDazugehöriger Auszug aus SQL-Datei zur Erzeugung des WKT-Strings mit Hilfe von concatenation:\n'Point(' || ue.koordinate_x::text || ' ' || ue.koordinate_y::text || ')' AS geom\nUnterstützte Datenbanken: PostgreSQL, SQLite, Oracle, Derby und DuckDB.\n\n\n\nLöscht Daten auf einem FTP-Server.\nBeispiel, löscht alle Daten in einem Verzeichnis:\ntasks.register('ftpdelete', FtpDelete) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir = \"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile = fileTree(pathToTempFolder) { include '*.zip' } \n    //remoteFile = \"*.zip\"\n}\nUm bestimmte Dateien zu löschen:\ntasks.register('ftpdelete', FtpDelete) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir = \"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile = \"*.zip\"\n}\nUm heruntergeladene Daten zu löschen:\ntasks.register('ftpdelete', FtpDelete) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir = \"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile = fileTree(pathToDownloadFolder) { include '*.zip' } \n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncontrolKeepAliveTimeout\nLong\nTimeout bis ein NOOP über den Kontroll-Kanal versendet wird. Default: 300s (=5 Minuten)\n ja\n\n\nfileSeparator\nString\nDefault: /. (Falls systemType Windows ist, ist der Default \\.\n ja\n\n\npassiveMode\nBoolean\nAktiv- oder Passiv-Verbindungsmodus. Default: Passiv (true)\n ja\n\n\npassword\nString\nPasswort für den Zugriff auf dem Server.\n nein\n\n\nremoteDir\nString\nVerzeichnis auf dem Server.\n nein\n\n\nremoteFile\nObject\nDateiname oder Liste der Dateinamen auf dem Server (kann auch ein Muster sein (* oder ?)). Ohne diesen Parameter werden alle Dateien aus dem Remoteverzeichnis gelöscht.\n ja\n\n\nserver\nString\nName des Servers (ohne ftp://).\n nein\n\n\nsystemType\nString\nUNIX oder WINDOWS. Default: UNIX.\n ja\n\n\nuser\nString\nBenutzername auf dem Server.\n nein\n\n\n\n\n\n\nLädt alle Dateien aus dem definierten Verzeichnis des Servers in ein lokales Verzeichnis herunter.\nBeispiel:\ntasks.register('ftpdownload', FtpDownload) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    localDir= \"downloads\"\n    remoteDir= \"\"\n}\nUm eine bestimmte Datei herunterzuladen:\ntasks.register('ftpdownload', FtpDownload) {\n    server=\"ftp.infogrips.ch\"\n    user= \"Hans\"\n    password= \"dummy\"\n    systemType=\"WINDOWS\"\n    localDir= \"downloads\"\n    remoteDir=\"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile=\"240100.zip\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncontrolKeepAliveTimeout\nLong\nTimeout bis ein NOOP über den Kontroll-Kanal versendet wird. Default: 300s (=5 Minuten)\n ja\n\n\nfileSeparator\nString\nDefault: /. (Falls systemType Windows ist, ist der Default \\.\n ja\n\n\nfileType\nString\nASCII oder BINARY. Default: ASCII.\n ja\n\n\nlocalDir\nString\nLokales Verzeichnis, in dem die Dateien gespeichert werden.\n nein\n\n\npassiveMode\nBoolean\nAktiv- oder Passiv-Verbindungsmodus. Default: Passiv (true)\n ja\n\n\npassword\nString\nPasswort für den Zugriff auf dem Server.\n nein\n\n\nremoteDir\nString\nVerzeichnis auf dem Server.\n nein\n\n\nremoteFile\nObject\nDateiname oder Liste der Dateinamen auf dem Server (kann auch ein Muster sein (* oder ?)). Ohne diesen Parameter werden alle Dateien aus dem Remoteverzeichnis heruntergeladen.\n ja\n\n\nserver\nString\nName des Servers (ohne ftp://).\n nein\n\n\nsystemType\nString\nUNIX oder WINDOWS. Default: UNIX.\n ja\n\n\nuser\nString\nBenutzername auf dem Server.\n nein\n\n\n\n\n\n\nLiefert eine Liste der Dateien aus dem definierten Verzeichnis des Servers.\nBeispiel:\ntasks.register('ftplist', FtpList) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir= \"\"\n    doLast {\n        println files\n    }\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncontrolKeepAliveTimeout\nLong\nTimeout bis ein NOOP über den Kontroll-Kanal versendet wird. Default: 300s (=5 Minuten)\n ja\n\n\nfileSeparator\nString\nDefault: /. (Falls systemType Windows ist, ist der Default \\.\n ja\n\n\npassiveMode\nBoolean\nAktiv- oder Passiv-Verbindungsmodus. Default: Passiv (true)\n ja\n\n\npassword\nString\nPasswort für den Zugriff auf dem Server.\n nein\n\n\nremoteDir\nString\nVerzeichnis auf dem Server.\n nein\n\n\nserver\nString\nName des Servers (ohne ftp://).\n nein\n\n\nsystemType\nString\nUNIX oder WINDOWS. Default: UNIX.\n ja\n\n\nuser\nString\nBenutzername auf dem Server.\n nein\n\n\n\n\n\n\nExportiert alle Tabellen einer GeoPackage-Datei in DXF-Dateien. Als Input wird eine von ili2gpkg erzeugte GeoPackage-Datei benötigt.\nEs werden alle INTERLIS-Klassen exportiert (SELECT tablename FROM T_ILI2DB_TABLE_PROP WHERE setting = 'CLASS'). Der eigentliche SELECT-Befehl ist komplizierter weil für das Layern der einzelnen DXF-Objekte das INTERLIS-Metaattribut !!@dxflayer=\"true\" ausgelesen wird. Gibt es kein solches Metaattribut wird alles in den gleichen DXF-Layer (default) geschrieben.\nEncoding: Die DXF-Dateien sind ISO-8859-1 encodiert.\nAchtung: Task sollte verbessert werden (siehe E-Mail Claude im Rahmen des Publisher-Projektes).\ntasks.register('gpkg2dxf', Gpkg2Dxf) {\n    dataFile = file(\"data.gpkg\")\n    outputDir = file(\"./out/\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nFile\nGeoPackage-Datei, die nach DXF transformiert werden soll.\n nein\n\n\noutputDir\nFile\nVerzeichnis, in das die DXF-Dateien gespeichert werden.\n nein\n\n\n\n\n\n\nExportiert alle Tabellen einer GeoPackage-Datei in Shapefiles. Als Input wird eine von ili2gpkg erzeugte GeoPackage-Datei benötigt.\nEs werden alle INTERLIS-Klassen exportiert (SELECT tablename FROM T_ILI2DB_TABLE_PROP WHERE setting = 'CLASS'). Je nach, bei der Erstellung der GeoPackage-Datei, verwendeten Parametern, muss die Query angepasst werden. Es muss jedoch darauf geachtet werden, dass es nur eine Query gibt (für alle Datensätze). Für den vorgesehenen Anwendungsfall (sehr einfache, flache Modelle) dürfte das kein Problem darstellen.\nEncoding: Die Shapefiles sind neu UTF-8 encodiert. Standard ist ISO-8859-1, scheint aber v.a. in QGIS nicht standardmässig zu funktionieren (keine Umlaute).\nAchtung: Task sollte verbessert werden (siehe E-Mail Claude im Rahmen des Publisher-Projektes).\ntasks.register('gpkg2shp', Gpkg2Shp) {\n    dataFile = file(\"data.gpkg\")\n    outputDir = file(\"./out/\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nFile\nGeoPackage-Datei, die nach Shapefile transformiert werden soll.\n nein\n\n\noutputDir\nFile\nVerzeichnis, in das die Shapefile gespeichert werden.\n nein\n\n\n\n\n\n\nDaten aus einer bestehenden Datenbanktabelle werden in eine GeoPackage-Datei exportiert.\nBeispiele:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('gpkgexport', GpkgExport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"gpkgexport\"\n    srcTableName = \"exportdata\"\n    dataFile = \"data.gpkg\"\n    dstTableName = \"exportdata\"\n}\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('gpkgexport', GpkgExport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"gpkgexport\"\n    srcTableName = [\"exportTable1\", \"exportTable2\"]\n    dataFile = \"data.gpkg\"\n    dstTableName = [\"exportTable1\", \"exportTable2\"]\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbatchSize\nInteger\nAnzahl der Records, die pro Batch in die Ziel-Datenbank (GeoPackage) geschrieben werden. Default: 5000\n ja\n\n\ndataFile\nObject\nName der GeoPackage-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndstTableName\nObject\nName der Tabelle(n) in der GeoPackage-Datei. String oder List.\n nein\n\n\nfetchSize\nInteger\nAnzahl der Records, die pro Fetch aus der Quell-Datenbank gelesen werden. Default: 5000\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\nsrcTableName\nObject\nName der DB-Tabelle(n), die exportiert werden soll(en). String oder List.\n nein\n\n\n\n\n\n\nDaten aus einer GeoPackage-Datei in eine bestehende Datenbanktabelle importieren.\nDie Tabelle kann weitere Spalten enthalten, die in der GeoPackage-Datei nicht vorkommen. Sie müssen aber NULLable sein, oder einen Default-Wert definiert haben.\nDie Gross-/Kleinschreibung der GeoPackage-Spaltennamen wird für die Zuordnung zu den DB-Spalten ignoriert.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('gpkgimport', GpkgImport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"gpkgimport\"\n    srcTableName = \"Point\"\n    dstTableName = \"importdata\"\n    dataFile = \"point.gpkg\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbatchSize\nInteger\nAnzahl der Records, die pro Batch in die Ziel-Datenbank geschrieben werden. Default: 5000\n ja\n\n\ndataFile\nObject\nName der GeoPackage-Datei, die gelesen werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, in die importiert werden soll.\n nein\n\n\ndstTableName\nString\nName der DB-Tabelle, in die importiert werden soll.\n nein\n\n\nfetchSize\nInteger\nAnzahl der Records, die pro Fetch aus der Quell-Datenbank gelesen werden. Default: 5000\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\nsrcTableName\nString\nName der GeoPackage-Tabelle, die importiert werden soll.\n nein\n\n\n\n\n\n\nPrüft eine GeoPackage-Datei gegenüber einem INTERLIS-Modell. Basiert auf dem ilivalidator.\nBeispiel:\ntasks.register('validate', GpkgValidator) {\n    models = \"GpkgModel\"\n    dataFiles = [\"attributes.gpkg\"]\n    tableName = \"Attributes\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nallObjectsAccessible\nBoolean\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n ja\n\n\nconfigFile\nObject\nKonfiguriert die Datenprüfung mit Hilfe einer ini-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\ndataFiles\nObject\nListe der Dateien, die validiert werden sollen. FileCollection oder List. Eine leere Liste ist kein Fehler.\n nein\n\n\ndisableAreaValidation\nBoolean\nSchaltet die AREA-Topologieprüfung aus. Default: false\n ja\n\n\nfailOnError\nBoolean\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n ja\n\n\nforceTypeValidation\nBoolean\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n ja\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n ja\n\n\nmetaConfigFile\nObject\nKonfiguriert den Validator mit Hilfe einer ini-Datei. Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell, gegen das die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Der Name der CSV-Datei.\n ja\n\n\nmultiplicityOff\nBoolean\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n ja\n\n\npluginFolder\nObject\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\nskipPolygonBuilding\nBoolean\nSchaltet die Bildung der Polygone aus (nur ITF). Default: false\n ja\n\n\ntableName\nString\nName der Tabelle in den GeoPackage-Dateien.\n nein\n\n\nvalidationOk\nboolean\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false.\n nein\n\n\nxtflogFile\nObject\nSchreibt die log-Meldungen in eine INTERLIS-2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n ja\n\n\n\n\n\n\nGzipped eine einzelne Datei. Es gibt einen eingebauten Tar-Task, der - nomen est omen - aber immer zuerst eine Tar-Datei erstellt.\nBeispiel:\ntasks.register('compressFile', Gzip) {\n    dataFile = file(\"./planregister.xml\");\n    gzipFile = file(\"./planregister.xml.gz\");\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nProperty&lt;File&gt;\nDatei, die gezipped werden soll.\n nein\n\n\ngzipFile\nProperty&lt;File&gt;\nOutput-Datei\n nein\n\n\n\n\n\n\nImportiert Daten aus einer INTERLIS-Transferdatei in eine GeoPackage-Datei.\nDie Tabellen werden implizit auch angelegt, falls sie noch nicht vorhanden sind. Falls die Tabellen in der Datenbank schon vorhanden sind, können sie zusätzliche Spalten enthalten (z.B. bfsnr, datum etc.), welche beim Import leer bleiben.\nFalls beim Import ein Datensatz-Identifikator (dataset) definiert wird, darf dieser Datensatz-Identifikator in der Datenbank noch nicht vorhanden sein.\nUm die bestehenden (früher importierten) Daten zu ersetzen, kann der Task Ili2gpkgReplace (not yet implemented) verwendet werden.\nDie Option --doSchemaImport wird automatisch gesetzt.\nBeispiel:\ntasks.register('importData', Ili2gpkgImport) {\n    models = \"SO_AGI_AV_GB_Administrative_Einteilungen_20180613\"\n    dataFile = file(\"data.xtf\");\n    dbfile = file(\"data.gpkg\")    \n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nString\nEntspricht der ili2gpkg-Option --baskets\n ja\n\n\ndataFile\nObject\nName der XTF-/ITF-Datei, die gelesen werden soll. Es können auch mehrere Dateien sein. FileCollection oder File\n nein\n\n\ndataset\nObject\nEntspricht der ili2gpkg-Option --dataset\n ja\n\n\ndbfile\nFile\nGeoPackage-Datei in die importiert werden soll.\n nein\n\n\ndefaultSrsCode\nString\nEntspricht der ili2gpkg-Option --defaultSrsCode\n ja\n\n\nisCoalesceJson\nBoolean\nEntspricht der ili2gpkg-Option --coalesceJson\n ja\n\n\nisCreateEnumTabs\nBoolean\nEntspricht der ili2gpkg-Option --createEnumTabs\n ja\n\n\nisCreateGeomIdx\nBoolean\nEntspricht der ili2gpkg-Option --createGeomIdx\n ja\n\n\nisCreateMetaInfo\nBoolean\nEntspricht der ili2gpkg-Option --createMetaInfo\n ja\n\n\nisDeleteData\nBoolean\nEntspricht der ili2gpkg-Option --deleteData\n ja\n\n\nisDisableAreaValidation\nBoolean\nEntspricht der ili2gpkg-Option --disableAreaValidation\n ja\n\n\nisDisableValidation\nBoolean\nEntspricht der ili2gpkg-Option --disableValidation\n ja\n\n\nisForceTypeValidation\nBoolean\nEntspricht der ili2gpkg-Option --forceTypeValidation\n ja\n\n\nisIligml20\nBoolean\nEntspricht der ili2gpkg-Option --iligml20\n ja\n\n\nisImportTid\nBoolean\nEntspricht der ili2gpkg-Option --importTid\n ja\n\n\nisNameByTopic\nBoolean\nEntspricht der ili2gpkg-Option --nameByTopic\n ja\n\n\nisSkipGeometryErrors\nBoolean\nEntspricht der ili2gpkg-Option --skipGeometryErrors\n ja\n\n\nisSkipPolygonBuilding\nBoolean\nEntspricht der ili2gpkg-Option --skipPolygonBuilding\n ja\n\n\nisStrokeArcs\nBoolean\nEntspricht der ili2gpkg-Option --strokeArcs\n ja\n\n\nisTrace\nBoolean\nEntspricht der ili2gpkg-Option --trace\n ja\n\n\nlogFile\nObject\nEntspricht der ili2gpkg-Option --logFile\n ja\n\n\nmodeldir\nString\nEntspricht der ili2gpkg-Option --modeldir\n ja\n\n\nmodels\nString\nEntspricht der ili2gpkg-Option --models\n ja\n\n\npostScript\nFile\nEntspricht der ili2gpkg-Option --postScript\n ja\n\n\npreScript\nFile\nEntspricht der ili2gpkg-Option --preScript\n ja\n\n\nproxy\nString\nEntspricht der ili2gpkg Option --proxy\n ja\n\n\nproxyPort\nInteger\nEntspricht der ili2gpkg-Option --proxyPort\n ja\n\n\ntopics\nString\nEntspricht der ili2gpkg-Option --topics\n ja\n\n\nvalidConfigFile\nFile\nEntspricht der ili2gpkg-Option --validConfigFile\n ja\n\n\n\nFür die Beschreibung der einzenen ili2gpkg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\n\nLöscht einen Datensatz in der PostgreSQL-Datenbank anhand eines Datensatz-Identifikators. Diese Funktion bedingt, dass das Datenbankschema mit der Option createBasketCol erstellt wurde (via Task Ili2pgImportSchema). Der Parameter failOnException muss false sein, ansonsten bricht der Job ab.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('deleteDataset', Ili2pgDelete) {\n    database = [db_uri, db_user, db_pass]\n    models = \"DM01AVSO24LV95\"\n    dbschema = \"dm01\"\n    dataset = \"kammersrohr\"\n}\nEs können auch mehrere Datensätze pro Task gelöscht werden:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('deleteDataset', Ili2pgDelete) {\n    database = [db_uri, db_user, db_pass]\n    dbschema = \"dm01\"\n    dataset = [\"Olten\",\"Grenchen\"]\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\n\nExportiert Daten aus der PostgreSQL-Datenbank in eine INTERLIS-Transferdatei.\nMit dem Parameter models, topics, baskets oder dataset wird definiert, welche Daten exportiert werden.\nOb die Daten im INTERLIS-1-, INTERLIS-2- oder GML-Format geschrieben werden, ergibt sich aus der Dateinamenserweiterung der Ausgabedatei. Für eine INTERLIS-1-Transferdatei muss die Erweiterung .itf verwendet werden. Für eine GML-Transferdatei muss die Erweiterung .gml verwendet werden.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('exportData', Ili2pgExport) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900-out.itf\"\n    dataset = \"254900\"\n    logFile = \"ili2pg.log\"\n}\nDamit mit einer einzigen Task-Definition mehrere Datensätze verarbeitet werden können, kann auch eine Liste von Dateinamen und Datensätzen angegeben werden.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('exportData', Ili2pgExport) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = [\"lv03_254900-out.itf\",\"lv03_255000-out.itf\"]\n    dataset = [\"254900\",\"255000\"]\n    logFile = \"ili2pg.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexport3\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --export3.\n ja\n\n\nexportModels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --exportModels.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\n\nImportiert Daten aus einer INTERLIS-Transferdatei in die PostgreSQL-Datenbank.\nDie Tabellen werden implizit auch angelegt, falls sie noch nicht vorhanden sind. Falls die Tabellen in der Datenbank schon vorhanden sind, können sie zusätzliche Spalten enthalten (z.B. bfsnr, datum etc.), welche beim Import leer bleiben.\nFalls beim Import ein Datensatz-Identifikator (dataset) definiert wird, darf dieser Datensatz-Identifikator in der Datenbank noch nicht vorhanden sein.\nFalls man mehrere Dateien importieren will, diese jedoch erst zur Laufzeit eruiert werden können, muss der Parameter dataFile eine Gradle FileCollection resp. eine implementierende Klasse (z.B. FileTree) sein. Gleiches gilt für den dataset-Parameter. Als einzelner Wert für das Dataset wird in diesem Fall der Name der Datei ohne Extension und ohne Pfad verwendet. Leider kann nicht bereits in der Task-Definition aus dem Filetree eine Liste gemacht werden, z.B. fileTree(pathToUnzipFolder) { include '*.itf' }.files.name. Diese Liste ist leer.\nUm die bestehenden (früher importierten) Daten zu ersetzen, kann der Task Ili2pgReplace verwendet werden.\nBeispiel 1:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('importData', Ili2pgImport) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900.itf\"\n    logFile = \"ili2pg.log\"\n}\nBeispiel 2:\nImport der AV-Daten. In der t_datasetname-Spalte soll die BFS-Nummer stehen. Die BFS-Nummer entspricht den ersten vier Zeichen des Filenamens.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('importData', Ili2pgImport) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = fileTree(pathToUnzipFolder) { include '*.itf' }\n    dataset = dataFile\n    datasetSubstring = (0..4).toList()\n    logFile = \"ili2pg.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndataFile\nProperty&lt;Object&gt;\nName der XTF-/ITF-Datei, die gelesen werden soll. Es können auch mehrere Dateien sein. FileCollection oder String.\n nein\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\n\nErstellt die Tabellenstruktur in der PostgreSQL-Datenbank anhand eines INTERLIS-Modells.\nDer Parameter iliFile oder models muss gesetzt werden.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('importSchema', Ili2pgImportSchema) {\n    database = [db_uri, db_user, db_pass]\n    models = \"DM01AVSO24\"\n    dbschema = \"gretldemo\"\n    logFile = \"ili2pg.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\nbeautifyEnumDispName\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --beautifyEnumDispName.\n ja\n\n\ncoalesceArray\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceArray.\n ja\n\n\ncoalesceCatalogueRef\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceCatalogueRef.\n ja\n\n\ncoalesceJson\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceJson.\n ja\n\n\ncoalesceMultiLine\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceMultiline.\n ja\n\n\ncoalesceMultiSurface\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceMultiSurface.\n ja\n\n\ncreateBasketCol\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createBasketCol.\n ja\n\n\ncreateDatasetCol\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createDatasetCol.\n ja\n\n\ncreateDateTimeChecks\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createDateTimeChecks.\n ja\n\n\ncreateEnumColAsItfCode\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createEnumColAsItfCode.\n ja\n\n\ncreateEnumTabs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createEnumTabs.\n ja\n\n\ncreateEnumTabsWithId\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createEnumTabsWithId.\n ja\n\n\ncreateEnumTxtCol\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createEnumTxtCol.\n ja\n\n\ncreateFk\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createFk.\n ja\n\n\ncreateFkIdx\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createFkIdx.\n ja\n\n\ncreateGeomIdx\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createGeomIdx.\n ja\n\n\ncreateImportTabs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createImportTabs.\n ja\n\n\ncreateMetaInfo\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createMetaInfo.\n ja\n\n\ncreateNumChecks\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createNumChecks.\n ja\n\n\ncreateSingleEnumTab\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createSingleEnumTab.\n ja\n\n\ncreateStdCols\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createStdCols.\n ja\n\n\ncreateTextChecks\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createTextChecks.\n ja\n\n\ncreateTidCol\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createTidCol.\n ja\n\n\ncreateTypeConstraint\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createTypeConstraint.\n ja\n\n\ncreateTypeDiscriminator\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createTypeDescriminator.\n ja\n\n\ncreateUnique\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createUnique.\n ja\n\n\ncreatescript\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --createscript.\n ja\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndefaultSrsAuth\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --defaultSrsAuth.\n ja\n\n\ndefaultSrsCode\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --defaultSrsCode.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableNameOptimization\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableNameOptimization.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\ndropscript\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dropscript.\n ja\n\n\nexpandMultilingual\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --expandMultilingual.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\nidSeqMax\nProperty&lt;Long&gt;\nEntspricht der ili2pg-Option --idSeqMax.\n ja\n\n\nidSeqMin\nProperty&lt;Long&gt;\nEntspricht der ili2pg-Option --idSeqMin.\n ja\n\n\niliFile\nProperty&lt;Object&gt;\nName der ili-Datei, die gelesen werden soll.\n ja\n\n\niliMetaAttrs\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --iliMetaAttrs.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nkeepAreaRef\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --keepAreaRef.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmaxNameLength\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --maxNameLength.\n ja\n\n\nmetaConfig\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --metaConfig.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\nnameByTopic\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --nameByTopic.\n ja\n\n\nnoSmartMapping\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --noSmartMapping.\n ja\n\n\noneGeomPerTable\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --oneGeomPerTable.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nsetupPgExt\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --setupPgExt.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nsmart1Inheritance\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --smart1Inheritance.\n ja\n\n\nsmart2Inheritance\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --smart2Inheritance.\n ja\n\n\nsqlColsAsText\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --sqlColsAsText.\n ja\n\n\nsqlEnableNull\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --sqlEnableNull.\n ja\n\n\nsqlExtRefCols\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --sqlExtRefCols.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\nt_id_Name\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --t_id_Name.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\ntranslation\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --translation.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\n\nErsetzt die Daten in der PostgreSQL-Datenbank anhand eines Datensatz-Identifikators (dataset) mit den Daten aus einer INTERLIS-Transferdatei. Diese Funktion bedingt, dass das Datenbankschema mit der Option createBasketCol erstellt wurde (via Task Ili2pgImportSchema).\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('replaceData', Ili2pgReplace) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900.itf\"\n    dataset = \"254900\"\n    logFile = \"ili2pg.log\"\n}\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('importData', Ili2pgReplace) {\n    database = [db_uri, db_user, db_pass]\n    models = 'DM01AVSO24LV95'\n    dbschema = 'agi_dm01avso24'\n    dataFile = fileTree(dir: dm01SoDir, include: '*.itf')\n    dataset = dataFile\n    datasetSubstring = 0..4\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndataFile\nProperty&lt;Object&gt;\nnull\n nein\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\n\nAktualisiert die Daten in der PostgreSQL-Datenbank anhand einer INTERLIS-Transferdatei, d.h. neue Objekte werden eingefügt, bestehende Objekte werden aktualisiert und in der Transferdatei nicht mehr vorhandene Objekte werden gelöscht.\nDiese Funktion bedingt, dass das Datenbankschema mit der Option --createBasketCol erstellt wurde (via Task Ili2pgImportSchema), und dass die Klassen und Topics eine stabile OID haben.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('updateData', Ili2pgUpdate) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900.itf\"\n    dataset = \"254900\"\n    logFile = \"ili2pg.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndataFile\nProperty&lt;Object&gt;\nnull\n nein\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\n\nPrüft die Daten ohne diese in eine Datei zu exportieren. Der Task ist erfolgreich, wenn keine Fehler gefunden werden und ist nicht erfolgreich, wenn Fehler gefunden werden. Mit der Option failOnException=false ist der Task erfolgreich, auch wenn Fehler gefunden werden.\nMit dem Parameter --models, --topics, --baskets oder --dataset wird definiert, welche Daten geprüft werden. Parameter --dataset akzeptiert auch eine Liste von Datasets.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('validate', Ili2pgValidate) {\n    database = [db_uri, db_user, db_pass]\n    models = \"SO_AGI_AV_GB_Administrative_Einteilungen_20180613\"\n    modeldir = rootProject.projectDir.toString() + \";http://models.interlis.ch\"\n    dbschema = \"agi_av_gb_admin_einteilungen_fail\"\n    logFile = file(\"fubar.log\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\n\nPrüft eine INTERLIS-Datei (.itf oder .xtf) gegenüber einem INTERLIS-Modell (.ili). Basiert auf dem ilivalidator.\nBeispiel:\ntasks.register('validate', IliValidator) {\n    dataFiles = [\"Beispiel2a.xtf\"]\n    logFile = \"ilivalidator.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nallObjectsAccessible\nBoolean\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n ja\n\n\nconfigFile\nObject\nKonfiguriert die Datenprüfung mit Hilfe einer ini-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\ndataFiles\nObject\nListe der Dateien, die validiert werden sollen. FileCollection oder List. Eine leere Liste ist kein Fehler.\n nein\n\n\ndisableAreaValidation\nBoolean\nSchaltet die AREA-Topologieprüfung aus. Default: false\n ja\n\n\nfailOnError\nBoolean\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n ja\n\n\nforceTypeValidation\nBoolean\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n ja\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n ja\n\n\nmetaConfigFile\nObject\nKonfiguriert den Validator mit Hilfe einer ini-Datei. Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell, gegen das die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Der Name der CSV-Datei.\n ja\n\n\nmultiplicityOff\nBoolean\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n ja\n\n\npluginFolder\nObject\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\nskipPolygonBuilding\nBoolean\nSchaltet die Bildung der Polygone aus (nur ITF). Default: false\n ja\n\n\nvalidationOk\nboolean\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false.\n nein\n\n\nxtflogFile\nObject\nSchreibt die log-Meldungen in eine INTERLIS-2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n ja\n\n\n\nZusatzfunktionen (Custom Functions): Die pluginFolder-Option ist zum jetzigen Zeitpunkt ohne Wirkung. Die Zusatzfunktionen werden als normale Abhängigkeit definiert und in der ilivalidator-Task-Implementierung registriert. Das Laden der Klassen zur Laufzeit in iox-ili hat nicht funktioniert (NoClassDefFoundError…). Der Plugin-Mechanismus von ilivalidator wird momentan ohnehin geändert (“Ahead-Of-Time-tauglich” gemacht).\n\n\n\nDaten aus einer Json-Datei in eine Datenbanktabelle importieren. Die gesamte Json-Datei (muss UTF-8 encoded sein) wird als Text in eine Spalte importiert. Ist das Json-Objekt in der Datei ein Top-Level-Array wird für jedes Element des Arrays ein Record in der Datenbanktabelle erzeugt.\nBeispiel:\ntasks.register('importJson', JsonImport) {\n    database = [db_uri, db_user, db_pass]\n    jsonFile = \"data.json\"\n    qualifiedTableName = \"jsonimport.jsonarray\"\n    columnName = \"json_text_col\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncolumnName\nString\nSpaltenname der Tabelle, in die importiert werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, in die importiert werden soll.\n nein\n\n\nisDeleteAllRows\nBoolean\nInhalt der Tabelle vorgängig löschen?\n ja\n\n\njsonFile\nString\nJSON-Datei, die importiert werden soll.\n nein\n\n\nqualifiedTableName\nString\nQualifizierter Namen der Tabelle (“schema.tabelle”), in die importiert werden soll.\n nein\n\n\n\n\n\n\nExportiert eine PostGIS-Raster-Spalte in eine Raster-Datei mittels SQL-Query. Die SQL-Query darf nur einen Record zurückliefern, d.h. es muss unter Umständen ST_Union() verwendet werden. Es wird angenommen, dass die erste bytea-Spalte des Resultsets die Rasterdaten enthält. Weitere bytea-Spalten werden ignoriert. Das Outputformat und die Formatoptionen müssen in der SQL-Datei (in der Select-Query) angegeben werden, z.B.:\nSELECT\n    1::int AS foo, ST_AsGDALRaster((ST_AsRaster(ST_Buffer(ST_Point(2607880,1228287),10),150, 150)), 'AAIGrid', ARRAY[''], 2056) AS raster\n;\nBeispiel:\ntasks.register('exportTiff', PostgisRasterExport) {\n    database = [db_uri, db_user, db_pass]\n    sqlFile = \"raster.sql\"\n    dataFile = \"export.tif\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nObject\nName der Rasterdatei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, aus der exportiert werden soll.\n nein\n\n\nsqlFile\nString\nName der SQL-Datei aus der das SQL-Statement gelesen und ausgeführt wird.\n nein\n\n\nsqlParameters\nMap&lt;String,String&gt;\nEine Map mit Paaren von Parameter-Name und Parameter-Wert.\n ja\n\n\n\n\n\n\nStellt für Vektordaten die aktuellsten Geodaten-Dateien bereit und pflegt das Archiv der vorherigen Zeitstände.\nDetails\n\n\n\nKopiert Objekte von einem Bucket in einen anderen. Die Buckets müssen in der gleichen Region sein. Die Permissions werden nicht mitkopiert und müssen explizit gesetzt werden.\nBeispiel:\ntasks.register('copyFiles', S3Bucket2Bucket) {\n    accessKey = s3AccessKey\n    secretKey = s3SecretKey\n    sourceBucket = s3SourceBucket\n    targetBucket = s3TargetBucket\n    acl = \"public-read\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\naccessKey\nString\nAccessKey\n nein\n\n\nacl\nString\nAccess Control Layer [private, public-read, public-read-write, authenticated-read, aws-exec-read, bucket-owner-read, bucket-owner-full-control]\n nein\n\n\nendPoint\nString\nS3-Endpunkt. Default: https://s3.eu-central-1.amazonaws.com/\n ja\n\n\nmetaData\nMap&lt;String,String&gt;\nMetadaten des Objektes resp. der Objekte, z.B. [\"lastModified\":\"2020-08-28\"].\n ja\n\n\nregion\nString\nS3-Region. Default: eu-central-1\n ja\n\n\nsecretKey\nString\nSecretKey\n nein\n\n\nsourceBucket\nString\nBucket, aus dem die Objekte kopiert werden.\n nein\n\n\ntargetBucket\nString\nBucket, in den die Objekte kopiert werden.\n nein\n\n\n\n\n\n\nLädt eine Datei aus einem S3-Bucket herunter.\nBeispiel:\ntasks.register('downloadFile', S3Download) {\n    accessKey = 'abcdefg'\n    secretKey = 'hijklmnopqrstuvwxy'\n    downloadDir = file(\"./path/to/dir/\")\n    bucketName = \"ch.so.ada.denkmalschutz\"\n    key = \"foo.pdf\"\n    endPoint = \"https://s3.eu-central-1.amazonaws.com\" \n    region = \"eu-central-1\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\naccessKey\nString\nAccessKey\n nein\n\n\nbucketName\nString\nName des Buckets, in dem die Datei gespeichert ist.\n nein\n\n\ndownloadDir\nFile\nVerzeichnis, in das die Datei heruntergeladen werden soll.\n nein\n\n\nendPoint\nString\nS3-Endpunkt. Default: https://s3.eu-central-1.amazonaws.com/\n ja\n\n\nkey\nString\nName der Datei.\n nein\n\n\nregion\nString\nS3-Region. Default: eu-central-1\n ja\n\n\nsecretKey\nString\nSecretKey\n nein\n\n\n\n\n\n\nLädt ein Dokument (sourceFile) oder alle Dokumente in einem Verzeichnis (sourceDir) in einen S3-Bucket (bucketName) hoch.\nMit dem passenden Content-Typ kann man das Verhalten des Browsers steuern. Default ist ‘application/octect-stream’, was dazu führt, dass die Datei immer heruntergeladen wird. Soll z.B. ein PDF oder ein Bild im Browser direkt angezeigt werden, muss der korrekte Content-Typ gewählt werden.\nBeispiel:\ntasks.register('uploadDirectory', S3Upload) {\n    accessKey = 'abcdefg'\n    secretKey = 'hijklmnopqrstuvwxy'\n    sourceDir = file(\"./docs\")\n    bucketName = \"ch.so.ada.denkmalschutz\"\n    endPoint = \"https://s3.eu-central-1.amazonaws.com\" \n    region = \"eu-central-1\"\n    acl = \"public-read\"\n    contentType = \"application/pdf\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\naccessKey\nString\nAccessKey\n nein\n\n\nacl\nString\nAccess Control Layer [private, public-read, public-read-write, authenticated-read, aws-exec-read, bucket-owner-read, bucket-owner-full-control]\n nein\n\n\nbucketName\nString\nName des Buckets, in dem die Dateien gespeichert werden sollen.\n nein\n\n\ncontentType\nString\nContent-Type\n ja\n\n\nendPoint\nString\nS3-Endpunkt. Default: https://s3.eu-central-1.amazonaws.com/\n ja\n\n\nmetaData\nMap&lt;String,String&gt;\nMetadaten des Objektes resp. der Objekte, z.B. [\"lastModified\":\"2020-08-28\"]\n ja\n\n\nregion\nString\nS3-Region. Default: eu-central-1\n nein\n\n\nsecretKey\nString\nSecretKey\n nein\n\n\nsourceDir\nObject\nVerzeichnis mit den Dateien, die hochgeladen werden sollen.\n ja\n\n\nsourceFile\nObject\nDatei, die hochgeladen werden soll.\n ja\n\n\nsourceFiles\nObject\nFileCollection mit den Dateien, die hochgeladen werden sollen, z.B. fileTree(\"/path/to/directoy/\") { include \"*.itf\" }\n ja\n\n\n\n\n\n\nDaten aus einer bestehenden Datenbanktabelle werden in eine SHP-Datei exportiert.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('shpexport', ShpExport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"shpexport\"\n    tableName = \"exportdata\"\n    dataFile = \"data.shp\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nObject\nName der SHP-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, aus der exportiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der SHP-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\ntableName\nString\nName der DB-Tabelle, die exportiert werden soll.\n nein\n\n\n\n\n\n\nDaten aus einer SHP-Datei in eine bestehende Datenbanktabelle importieren.\nDie Tabelle kann weitere Spalten enthalten, die in der SHP-Datei nicht vorkommen. Sie müssen aber NULLable sein, oder einen Default-Wert definiert haben.\nDie Tabelle muss eine Geometriespalte enthalten. Der Name der Geometriespalte kann beliebig gewählt werden.\nDie Gross-/Kleinschreibung der SHP-Spaltennamen wird für die Zuordnung zu den DB-Spalten ignoriert.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('shpimport', ShpImport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"shpimport\"\n    tableName = \"importdata\"\n    dataFile = \"data.shp\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nObject\nName der SHP-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, aus der exportiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der SHP-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\ntableName\nString\nName der DB-Tabelle, die exportiert werden soll.\n nein\n\n\n\n\n\n\nPrüft eine SHP-Datei gegenüber einem INTERLIS-Modell. Basiert auf dem ilivalidator.\nIm gegebenen Modell wird eine Klasse gesucht, die genau die Attributenamen wie in der Shp-Datei enthält (wobei die Gross-/Kleinschreibung ignoriert wird); die Attributtypen werden ignoriert. Wird keine solche Klasse gefunden, gilt das als Validierungsfehler.\nDie Prüfung von gleichzeitig mehreren Shapefiles führt zu Fehlermeldungen wie OID o3158 of object &lt;Modelname&gt;.&lt;Topicname&gt;.&lt;Klassenname&gt; already exists in .... Beim Öffnen und Lesen eines Shapefiles wird immer der Zähler, der die interne (im Shapefile nicht vorhandene) OID generiert, zurückgesetzt. Somit kann immer nur ein Shapefile pro Task geprüft werden.\nBeispiel:\ntasks.register('validate', ShpValidator) {\n    models = \"ShpModel\"\n    dataFiles = [\"data.shp\"]\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nallObjectsAccessible\nBoolean\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n ja\n\n\nconfigFile\nObject\nKonfiguriert die Datenprüfung mit Hilfe einer ini-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\ndataFiles\nObject\nListe der Dateien, die validiert werden sollen. FileCollection oder List. Eine leere Liste ist kein Fehler.\n nein\n\n\ndisableAreaValidation\nBoolean\nSchaltet die AREA-Topologieprüfung aus. Default: false\n ja\n\n\nencoding\nString\nZeichencodierung der SHP-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nfailOnError\nBoolean\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n ja\n\n\nforceTypeValidation\nBoolean\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n ja\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n ja\n\n\nmetaConfigFile\nObject\nKonfiguriert den Validator mit Hilfe einer ini-Datei. Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell, gegen das die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Der Name der CSV-Datei.\n ja\n\n\nmultiplicityOff\nBoolean\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n ja\n\n\npluginFolder\nObject\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\nskipPolygonBuilding\nBoolean\nSchaltet die Bildung der Polygone aus (nur ITF). Default: false\n ja\n\n\nvalidationOk\nboolean\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false.\n nein\n\n\nxtflogFile\nObject\nSchreibt die log-Meldungen in eine INTERLIS-2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n ja\n\n\n\n\n\n\nDer SqlExecutor-Task dient dazu, Datenumbauten auszuführen.\nEr wird im Allgemeinen dann benutzt, wenn\n\nder Datenumbau komplex ist und deshalb nicht im Db2Db-Task erledigt werden kann\noder wenn die Quell-DB keine PostgreSQL-DB ist (weil bei komplexen Queries für den Datenumbau möglicherweise fremdsystemspezifische SQL-Syntax verwendet werden müsste)\noder wenn Quell- und Zielschema in derselben Datenbank liegen\n\nIn den Fällen 1 und 2 werden Stagingtabellen bzw. ein Stagingschema benötigt, in welche der Db2Db-Task die Daten zuerst 1:1 hineinschreibt. Der SqlExecutor-Task liest danach die Daten von dort, baut sie um und schreibt sie dann ins Zielschema. Die Queries für den SqlExecutor-Task können alle in einem einzelnen .sql-File sein oder (z.B. aus Gründen der Strukturierung oder Organisation) auf mehrere .sql-Dateien verteilt sein. Die Reihenfolge der .sql-Dateien ist relevant. Dies bedeutet, dass die SQL-Befehle des zuerst angegebenen .sql-Datei zuerst ausgeführt werden müssen, danach dies SQL-Befehle des an zweiter Stelle angegebenen .sql-Datei, usw.\nDer SqlExecutor-Task muss neben Updates ganzer Tabellen (d.h. Löschen des gesamten Inhalts einer Tabelle und gesamter neuer Stand in die Tabelle schreiben) auch Updates von Teilen von Tabellen zulassen. D.h. es muss z.B. möglich sein, innerhalb einer Tabelle nur die Objekte einer bestimmten Gemeinde zu aktualisieren. Darum ist es möglich innerhalb der .sql-Datei Paramater zu verwenden und diesen Parametern beim Task einen konkreten Wert zuzuweisen. Innerhalb der .sql-Datei werden Paramter mit folgender Syntax verwendet: ${paramName}.\nUnterstützte Datenbanken: PostgreSQL, SQLite, Oracle, Derby und DuckDB.\nBeispiele:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('executeSomeSql', SqlExecutor) {\n    database = [db_uri, db_user, db_pass]\n    sqlParameters = [dataset:'Olten']\n    sqlFiles = ['demo.sql']\n}\nDamit mit einer einzigen Task-Definition mehrere Datensätze verarbeitet werden können, kann auch eine Liste von Parametern angegeben werden.\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('executeSomeSql', SqlExecutor) {\n    database = [db_uri, db_user, db_pass]\n    sqlParameters = [[dataset:'Olten'],[dataset:'Grenchen']]\n    sqlFiles = ['demo.sql']\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank, in die importiert werden soll.\n nein\n\n\nsqlFiles\nListProperty&lt;String&gt;\nName der SQL-Datei aus der SQL-Statements gelesen und ausgeführt werden.\n nein\n\n\nsqlParameters\nProperty&lt;Object&gt;\nEine Map mit Paaren von Parameter-Name und Parameter-Wert (Map&lt;String,String&gt;). Oder eine Liste mit Paaren von Parameter-Name und Parameter-Wert (List&lt;Map&lt;String,String&gt;&gt;).\n ja\n\n\n\n\n\n\nTransformiert eine Datei mittels einer XSL-Transformation ein eine andere Datei. Ist der xslFile-Parameter ein String, wird erwartet, dass die Datei im GRETL-Verzeichnis im Ressourcenordner src/main/resources/xslt-Verzeichnis gespeichert ist. Falls der xslFile-Parameter ein File-Objekt ist, können lokale Dateien verwendet werden.\nBeispiele:\ntasks.register('transform', XslTransformer) {\n    xslFile = \"eCH0132_to_SO_AGI_SGV_Meldungen_20221109.xsl\"\n    xmlFile = file(\"MeldungAnGeometer_G-0111102_20221103_145001.xml\")\n    outDirectory = file(\".\")\n}\ntasks.register('transform', XslTransformer) {\n    xslFile = file(\"path/to/eCH0132_to_SO_AGI_SGV_Meldungen_20221109.xsl\")\n    xmlFile = file(\"MeldungAnGeometer_G-0111102_20221103_145001.xml\")\n    outDirectory = file(\".\")\n}\ntasks.register('transform', XslTransformer) {\n    xslFile = \"eCH0132_to_SO_AGI_SGV_Meldungen_20221109.xsl\"\n    xmlFile = fileTree(\".\").matching {\n        include \"*.xml\"\n    }\n    outDirectory = file(\".\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nfileExtension\nString\nFileextension der Resultatdatei. Default: xtf\n ja\n\n\noutDirectory\nFile\nVerzeichnis, in das die transformierte Datei gespeichert wird. Der Name der transformierten Datei entspricht standardmässig dem Namen der Input-Datei mit Endung .xtf.\n nein\n\n\nxmlFile\nObject\nDatei oder FileTree, die/der transformiert werden soll.\n nein\n\n\nxslFile\nObject\nName der XSLT-Datei, die im src/main/resources/xslt-Verzeichnis liegen muss oder File-Objekt (beliebiger Pfad).\n nein"
  },
  {
    "objectID": "gretl/docs/architecture/architecture.html",
    "href": "gretl/docs/architecture/architecture.html",
    "title": "Einführung und Ziele",
    "section": "",
    "text": "Dieses Dokument ist nach arc42 Template Revision 7.0 strukturiert.\nDiagramme werden online auf draw.io erstellt. Die XML Definitionen der Diagramme liegen neben den exportierten Bildern.\n\n\n\n\nGRETL ist eine Gradle Erweiterung zur Ausführung von GEO Daten Transformationen.\nDer Name GRETL ist eine Kombination von Gradle und ETL\n\n\n\n\n\nStandardisierter und automatisierter Build-Prozess."
  },
  {
    "objectID": "gretl/docs/architecture/architecture.html#aufgabenstellung",
    "href": "gretl/docs/architecture/architecture.html#aufgabenstellung",
    "title": "Einführung und Ziele",
    "section": "",
    "text": "GRETL ist eine Gradle Erweiterung zur Ausführung von GEO Daten Transformationen.\nDer Name GRETL ist eine Kombination von Gradle und ETL"
  },
  {
    "objectID": "gretl/docs/architecture/architecture.html#qualitätsziele",
    "href": "gretl/docs/architecture/architecture.html#qualitätsziele",
    "title": "Einführung und Ziele",
    "section": "",
    "text": "Standardisierter und automatisierter Build-Prozess."
  },
  {
    "objectID": "gretl/docs/architecture/architecture.html#organisatorische-randbedingungen",
    "href": "gretl/docs/architecture/architecture.html#organisatorische-randbedingungen",
    "title": "Einführung und Ziele",
    "section": "Organisatorische Randbedingungen",
    "text": "Organisatorische Randbedingungen\n\nVeröffentlichung als Open Source\nDie Lösung wird als Open Source verfügbar gemacht.\nLizenz: MIT"
  },
  {
    "objectID": "gretl/docs/architecture/architecture.html#fachlicher-kontext",
    "href": "gretl/docs/architecture/architecture.html#fachlicher-kontext",
    "title": "Einführung und Ziele",
    "section": "Fachlicher Kontext",
    "text": "Fachlicher Kontext\n\n\n\nBusiness context\n\n\n\nAGI: erstellt Jobs im gretljobs Repository auf GitHub und verwaltet deren Ausführung über den GRETL Jenkins.\nGitHub: webbasierter Online-Dienst für das Versionsverwaltungssystem Git: https://github.com/\ngretljobs: Transformations-Job Konfigurationen für die GRETL Runtime.\nGRETL: System für die Ausführung der GRETL Jobs.\nGRETL Jenkins: Verwaltungsoberfläche / UI für GRETL Jobs.\nGRETL Runtime: Runtime für GRETL Jobs.\n\n\nGRETL-Job Generierung (Seeder-Job)\nBeim GRETL-Jenkins ist ein Seeder-Job vor konfiguriert, welcher die GRETL-Jobs generiert. Der Job verwendet die Job DSL.\n\n\n\nJenkins_Seeder_Job\n\n\nDer GRETL-Jenkins bekommt die Konfiguration vom Seeder-Job über Umgebungsvariablen: * GRETL_JOB_REPO_URL: git://github.com/sogis/gretljobs.git * GRETL_JOB_FILE_PATH:  * GRETL_JOB_FILE_NAME**: gretl-job.groovy\nDies ist die produktive Konfiguration auf das gretljobs Repo. Über alle Ordner hinweg werden Dateien mit dem Namen gretl-job.groovy gesucht. Für jede gefundene Datei/Skript wird ein Job mit der Definition aus dem Skript (Jenkins Pipeline) erstellt. Der Name vom Job ist der Name vom Ordner, wo das Skript gefunden wurde."
  },
  {
    "objectID": "gretl/docs/architecture/architecture.html#technischer-kontext",
    "href": "gretl/docs/architecture/architecture.html#technischer-kontext",
    "title": "Einführung und Ziele",
    "section": "Technischer Kontext",
    "text": "Technischer Kontext\n\n&lt;Gradle&gt;\nTODO\n\n\n&lt;Jenkins&gt;\nTODO"
  },
  {
    "objectID": "gretl/docs/architecture/architecture.html#gradle-als-job-runtime",
    "href": "gretl/docs/architecture/architecture.html#gradle-als-job-runtime",
    "title": "Einführung und Ziele",
    "section": "Gradle als Job Runtime",
    "text": "Gradle als Job Runtime\n\nFragestellung\nWieso wird Gradle als Runtime eingesetzt?\n\n\nEntscheidung\nTODO"
  },
  {
    "objectID": "gretl/docs/architecture/architecture.html#gretl-als-gradle-plugin",
    "href": "gretl/docs/architecture/architecture.html#gretl-als-gradle-plugin",
    "title": "Einführung und Ziele",
    "section": "GRETL als Gradle Plugin",
    "text": "GRETL als Gradle Plugin\n\nFragestellung\nWieso wird die ETL Logik als Gradle Plugin geschrieben?\n\n\nEntscheidung\nTODO"
  },
  {
    "objectID": "gretl/docs/architecture/architecture.html#jenkins-als-benutzeroberfläche",
    "href": "gretl/docs/architecture/architecture.html#jenkins-als-benutzeroberfläche",
    "title": "Einführung und Ziele",
    "section": "Jenkins als Benutzeroberfläche",
    "text": "Jenkins als Benutzeroberfläche\n\nFragestellung\nWarum ist Jenkins das UI?\n\n\nEntscheidung\nTODO"
  },
  {
    "objectID": "gretl/docs/architecture/architecture.html#gretl-job-ausführung-auf-build-container",
    "href": "gretl/docs/architecture/architecture.html#gretl-job-ausführung-auf-build-container",
    "title": "Einführung und Ziele",
    "section": "GRETL Job Ausführung auf Build-Container",
    "text": "GRETL Job Ausführung auf Build-Container\n\nFragestellung\nWieso wird für jede Job Ausführung ein eigener Container gestartet?\n\n\nAlternativen\n\nEinzelner Server läuft und steht für Jobs zur Verfügung.\nJeder Job hat einen laufenden Container.\n\n\n\nEntscheidung\nEs wird das Prinzip vom Build-Container eingesetzt. Da das Scheduling von Jenkins übernommen wird, braucht es keine lang-laufenden Container. Zum Ausnützen der Stärken von Container Plattformen werden kurz-lebige Container eingesetzt. Dadurch sind sie unabhängig von einander und die Resourcen können besser genutzt werden."
  },
  {
    "objectID": "gretl/docs/architecture/architecture.html#gretl-runtime-als-jenkins-slave",
    "href": "gretl/docs/architecture/architecture.html#gretl-runtime-als-jenkins-slave",
    "title": "Einführung und Ziele",
    "section": "GRETL Runtime als Jenkins Slave",
    "text": "GRETL Runtime als Jenkins Slave\n\nFragestellung\nWieso ist die GRETL Runtime als Jenkins Slave realisiert?\n\n\nAlternativen\n\nEinzelner Server läuft immer und arbeitet Jobs bei Aufruf ab.\n\n\n\nEntscheidung\nTODO"
  },
  {
    "objectID": "gretl/docs/architecture/architecture.html#qualitätsbaum",
    "href": "gretl/docs/architecture/architecture.html#qualitätsbaum",
    "title": "Einführung und Ziele",
    "section": "Qualitätsbaum",
    "text": "Qualitätsbaum"
  },
  {
    "objectID": "gretl/docs/architecture/architecture.html#qualitätsszenarien",
    "href": "gretl/docs/architecture/architecture.html#qualitätsszenarien",
    "title": "Einführung und Ziele",
    "section": "Qualitätsszenarien",
    "text": "Qualitätsszenarien"
  },
  {
    "objectID": "reference.html",
    "href": "reference.html",
    "title": "Referenzdokumentation",
    "section": "",
    "text": "Das Datenmanagement-Tool GRETL ist ein Werkzeug, das für Datenimports, Datenumbauten (Modellumbau) und Datenexports eingesetzt wird. GRETL führt Jobs aus, wobei ein Job aus mehreren atomaren Tasks besteht. Damit ein Job als vollständig ausgeführt gilt, muss jeder zum Job gehörende Task vollständig ausgeführt worden sein. Schlägt ein Task fehl, gilt auch der Job als fehlgeschlagen.\nEin Job besteht aus einem oder mehreren Tasks, die gemäss einem gerichteten Graphen (Directed Acyclic Graph; DAG) miteinander verknüpft sind.\nEin Job kann aus z.B. aus einer linearen Kette von Tasks bestehen:\nBeispiel: Datenimport aus INTERLIS-Datei – Datenumbau – Datenexport nach Shapefile.\nEin Job kann sich nach einem Task aber auch auf zwei oder mehr verschiedene weitere Tasks verzweigen:\nBeispiel: Datenimport aus INTERLIS-Datei – Datenumbau in Zielschema 1 und ein zweiter Datenumbau in Zielschema 2.\nEs ist auch möglich, dass zuerst zwei oder mehr Tasks unabhängig voneinander ausgeführt werden müssen, bevor ein einzelner weiterer Task ausgeführt wird.\nDie Tasks eines Jobs werden per Konfigurationsfile konfiguriert."
  },
  {
    "objectID": "reference.html#systemanforderungen",
    "href": "reference.html#systemanforderungen",
    "title": "Referenzdokumentation",
    "section": "Systemanforderungen",
    "text": "Systemanforderungen\nFür die aktuelle GRETL-Version wird Java 11 und Gradle 7.6 benötigt."
  },
  {
    "objectID": "reference.html#installation",
    "href": "reference.html#installation",
    "title": "Referenzdokumentation",
    "section": "Installation",
    "text": "Installation\nGRETL selbst muss als Gradle-Plugin nicht explizit installiert werden, sondern wird dynamisch durch das Internet bezogen."
  },
  {
    "objectID": "reference.html#kleines-beispiel",
    "href": "reference.html#kleines-beispiel",
    "title": "Referenzdokumentation",
    "section": "Kleines Beispiel",
    "text": "Kleines Beispiel\nErstellen Sie in einem neuen Verzeichnis gretldemo eine neue Datei build.gradle:\nimport ch.so.agi.gretl.tasks.*\nimport ch.so.agi.gretl.api.*\n\napply plugin: 'ch.so.agi.gretl'\n\nbuildscript {\n    repositories {\n        maven { url \"https://jars.interlis.ch\" }\n        maven { url \"https://repo.osgeo.org/repository/release/\" }\n        maven { url \"https://plugins.gradle.org/m2/\" }\n        maven { url \"https://s01.oss.sonatype.org/service/local/repositories/releases/content/\" }\n        maven { url \"https://s01.oss.sonatype.org/service/local/repositories/snapshots/content/\" }\n        mavenCentral()\n    }\n    dependencies {\n        classpath group: 'ch.so.agi', name: 'gretl',  version: '3.0.+'\n    }\n}\n\ndefaultTasks 'validate'\n\ntask validate(type: IliValidator){\n    dataFiles = [\"BeispielA.xtf\"]\n}\nDie Datei build.gradle ist die Job-Konfiguration. Dieser kleine Beispiel-Job besteht nur aus einem einzigen Task: validate.\nErstellen Sie nun noch die Datei BeispielA.xtf (damit danach der Job erfolgreich ausgeführt werden kann).\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;TRANSFER xmlns=\"http://www.interlis.ch/INTERLIS2.3\"&gt;\n    &lt;HEADERSECTION SENDER=\"gretldemo\" VERSION=\"2.3\"&gt;\n    &lt;/HEADERSECTION&gt;\n    &lt;DATASECTION&gt;\n        &lt;OeREBKRMtrsfr_V1_1.Transferstruktur BID=\"B01\"&gt;\n        &lt;/OeREBKRMtrsfr_V1_1.Transferstruktur&gt;\n    &lt;/DATASECTION&gt;\n&lt;/TRANSFER&gt;\nUm den Job auszuführen, wechseln Sie ins Verzeichnis mit der Job-Konfiguration, und geben da das Kommando gradle ohne Argument ein:\ncd gretldemo\ngradle\nBUILD SUCCESSFUL zeigt an, dass der Job (die Validierung der Datei BeispielA.xtf) erfolgreich ausgeführt wurde.\nUm dieselbe Job-Konfiguration für verschiedene Datensätze verwenden zu können, muss es parametrisierbar sein. Die Jobs/Tasks können so generisch konfiguriert werden, dass dieselbe Konfiguration z.B. für Daten aus verschiedenen Gemeinden benutzt werden kann. Parameter für die Job Konfiguration können z.B. mittels gradle-Properties (Gradle properties and system properties) dem Job mitgegeben werden, also z.B.\ncd gretldemo\ngradle -Pdataset=Olten"
  },
  {
    "objectID": "reference.html#ausführen",
    "href": "reference.html#ausführen",
    "title": "Referenzdokumentation",
    "section": "Ausführen",
    "text": "Ausführen\nUm GRETL auszuführen, geben Sie auf der Kommandozeile folgendes Kommando ein (wobei jobfolder der absolute Pfad zu ihrem Verzeichnis mit der Job Konfiguration ist.)\ngradle --project-dir jobfolder\nAlternativ können Sie auch ins Verzeichnis mit der Job Konfiguration wechseln, und da das Kommando gradle ohne Argument verwenden:\ncd jobfolder\ngradle"
  },
  {
    "objectID": "reference.html#tasks",
    "href": "reference.html#tasks",
    "title": "Referenzdokumentation",
    "section": "Tasks",
    "text": "Tasks\n\nAv2ch\nTransformiert eine INTERLIS1-Transferdatei im kantonalen AV-DM01-Modell in das Bundesmodell. Unterstützt werden die Sprachen Deutsch und Italienisch und der Bezugrahmen LV95. Getestet mit Daten aus den Kantonen Solothurn, Glarus und Tessin. Weitere Informationen sind in der Basisbibliothek zu finden: https://github.com/sogis/av2ch.\nDas Bundes-ITF hat denselben Namen wie das Kantons-ITF.\nAufgrund der sehr vielen Logging-Messages einer verwendeten Bibliothek, wird der System.err-Ouput nach dev/null https://github.com/sogis/av2ch/blob/master/src/main/java/ch/so/agi/av/Av2ch.java#L75.\ntasks.register('transform', Av2ch) {\n    inputFile = file(\"254900.itf\")\n    outputDirectory = file(\"output\")\n}\nEs können auch mehrere Dateien gleichzeitig transformiert werden:\ntasks.register('transform', Av2ch) {\n    inputFile = fileTree(\".\").matching {\n            include\"*.itf\"\n        }\n    outputDirectory = file(\"output\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ninputFile\nObject\nZu transformierende ITF-Datei(en). File- oder FileCollection-Objekt.\n nein\n\n\nlanguage\nString\nSprache des Modelles / der Datei (de, it). Default: de\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\noutputDirectory\nObject\nName des Verzeichnisses in das die zu erstellende Datei geschrieben wird.\n nein\n\n\nzip\nBoolean\nDie zu erstellende Datei wird gezippt. Default: false\n ja\n\n\n\n\n\nAv2geobau\nAv2geobau konvertiert eine Interlis-Transferdatei (itf) in eine DXF-Geobau Datei. Av2geobau funktioniert ohne Datenbank.\nDie ITF-Datei muss dem Modell DM01AVCH24LV95D entsprechen. Die Daten werden nicht validiert.\nDie Datenstruktur der DXF-Datei ist im Prinzip sehr einfach: Die verschiedenen Informationen aus dem Datenmodell DM01 werden in verschiedene DXF-Layer abgebildet, z.B. die begehbaren LFP1 werden in den Layer “01111” abgebildet. Oder die Gebäude in den Layer “01211”.\nDer Datenumbau ist nicht konfigurierbar.\ntasks.register('av2geobau', Av2geobau) {\n    itfFiles = \"ch_254900.itf\"\n    dxfDirectory = \"./out/\"\n}\nEs können auch mehrere Dateien angegeben werden.\ntasks.register('av2geobau', Av2geobau) {\n    itfFiles = fileTree(\".\").matching {\n        include\"*.itf\"\n    }\n    dxfDirectory = \"./out/\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndxfDirectory\nObject\nVerzeichnis, in das die DXF-Dateien gespeichert werden.\n nein\n\n\nisZip\nBoolean\nDie zu erstellende Datei wird gezippt und es werden zusätzliche Dateien (Musterplan, Layerbeschreibung, Hinweise) hinzugefügt. Default: false\n ja\n\n\nitfFiles\nObject\nITF-Datei, die nach DXF transformiert werden soll. Es können auch mehrere Dateien angegeben werden. File- oder FileCollection-Objekt.\n nein\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Konvertierung in eine Text-Datei.\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\n\n\n\nCsv2Excel (incubating)\nKonvertiert eine CSV-Datei in eine Excel-Datei (*.xlsx). Datentypen werden anhand eines INTERLIS-Modelles eruiert. Fehlt das Modell, wird alles als Text gespeichert. Die Daten werden vollständig im Speicher vorgehalten. Falls grosse Dateien geschrieben werden müssen, kann das zu Problemen führen. Dann müsste die Apache POI SXSSF Implementierung (Streaming) verwendet werden.\nBeispiel:\ntasks.register('convertData', Csv2Excel) {\n    csvFile = file(\"./20230124_sap_Gebaeude.csv\")\n    firstLineIsHeader = true\n    valueDelimiter = null\n    valueSeparator = \";\"\n    encoding = \"ISO-8859-1\";\n    models = \"SO_HBA_Gebaeude_20230111\";\n    outputDir = file(\".\");\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncsvFile\nFile\nCSV-Datei, die konvertiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der CSV-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nfirstLineIsHeader\nBoolean\nDefiniert, ob eine Headerzeile geschrieben werden soll, oder nicht. Default: true\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell für Definition der Datentypen in der Excel-Datei.\n ja\n\n\noutputDir\nFile\nVerzeichnis, in das die Excel-Datei gespeichert wird. Default: Verzeichnis, in dem die CSV-Datei vorliegt.\n ja\n\n\nvalueDelimiter\nCharacter\nZeichen, das am Anfang und Ende jeden Wertes geschrieben werden soll. Default \"\n ja\n\n\nvalueSeparator\nCharacter\nZeichen, das als Trennzeichen zwischen den Werten verwendet werden soll. Default: ,\n ja\n\n\n\n\n\nCsvExport\nDaten aus einer bestehenden Datenbanktabelle werden in eine CSV-Datei exportiert. Geometriespalten können nicht exportiert werden.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('csvexport', CsvExport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"csvexport\"\n    tableName = \"exportdata\"\n    firstLineIsHeader = true\n    attributes = [ \"t_id\",\"Aint\"]\n    dataFile = \"data.csv\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nattributes\nString[]\nSpalten der DB-Tabelle, die exportiert werden sollen. Definiert die Reihenfolge der Spalten in der CSV-Datei. Default: alle Spalten\n ja\n\n\ndataFile\nObject\nName der CSV-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank aus der exportiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der CSV-Datei, z.B. “UTF-8”. Default: Systemeinstellung\n ja\n\n\nfirstLineIsHeader\nBoolean\nDefiniert, ob eine Headerzeile geschrieben werden soll, oder nicht. Default: true\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\ntableName\nString\nName der DB-Tabelle, die exportiert werden soll.\n nein\n\n\nvalueDelimiter\nCharacter\nZeichen, das am Anfang und Ende jeden Wertes geschrieben werden soll. Default \"\n ja\n\n\nvalueSeparator\nCharacter\nZeichen, das als Trennzeichen zwischen den Werten verwendet werden soll. Default: ,\n ja\n\n\n\n\n\nCsvImport\nDaten aus einer CSV-Datei werden in eine bestehende Datenbanktabelle importiert.\nDie Tabelle kann weitere Spalten enthalten, die in der CSV-Datei nicht vorkommen. Sie müssen aber NULLable sein, oder einen Default-Wert definiert haben.\nGeometriepalten können nicht importiert werden.\nDie Gross-/Kleinschreibung der CSV-Spaltennamen wird für die Zuordnung zu den DB-Spalten ignoriert.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('csvimport',  CsvImport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"csvimport\"\n    tableName = \"importdata\"\n    firstLineIsHeader = true\n    dataFile = \"data1.csv\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nattributes\nString[]\nSpalten der DB-Tabelle, die exportiert werden sollen. Definiert die Reihenfolge der Spalten in der CSV-Datei. Default: alle Spalten\n ja\n\n\ndataFile\nObject\nName der CSV-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank aus der exportiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der CSV-Datei, z.B. “UTF-8”. Default: Systemeinstellung\n ja\n\n\nfirstLineIsHeader\nBoolean\nDefiniert, ob eine Headerzeile geschrieben werden soll, oder nicht. Default: true\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\ntableName\nString\nName der DB-Tabelle, die exportiert werden soll.\n nein\n\n\nvalueDelimiter\nCharacter\nZeichen, das am Anfang und Ende jeden Wertes geschrieben werden soll. Default \"\n ja\n\n\nvalueSeparator\nCharacter\nZeichen, das als Trennzeichen zwischen den Werten verwendet werden soll. Default: ,\n ja\n\n\n\n\n\nCsvValidator\nPrüft eine CSV-Datei gegenüber einem INTERLIS-Modell. Basiert auf dem ilivalidator. Das Datenmodell darf die OID nicht als UUID modellieren (OID AS INTERLIS.UUIDOID).\nBeispiel:\ntasks.register('validate', CsvValidator) {\n    models = \"CsvModel\"\n    firstLineIsHeader = true\n    dataFiles = [\"data1.csv\"]\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nallObjectsAccessible\nBoolean\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n ja\n\n\nconfigFile\nObject\nKonfiguriert die Datenprüfung mit Hilfe einer ini-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\ndataFiles\nObject\nListe der Dateien, die validiert werden sollen. FileCollection oder List. Eine leere Liste ist kein Fehler.\n nein\n\n\ndisableAreaValidation\nBoolean\nSchaltet die AREA-Topologieprüfung aus. Default: false\n ja\n\n\nencoding\nString\nZeichencodierung der CSV-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nfailOnError\nBoolean\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n ja\n\n\nfirstLineIsHeader\nBoolean\nDefiniert, ob die CSV-Datei einer Headerzeile hat, oder nicht. Default: true\n ja\n\n\nforceTypeValidation\nBoolean\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n ja\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n ja\n\n\nmetaConfigFile\nObject\nKonfiguriert den Validator mit Hilfe einer ini-Datei. Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell, gegen das die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Der Name der CSV-Datei.\n ja\n\n\nmultiplicityOff\nBoolean\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n ja\n\n\npluginFolder\nObject\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\nskipPolygonBuilding\nBoolean\nSchaltet die Bildung der Polygone aus (nur ITF). Default: false\n ja\n\n\nvalidationOk\nboolean\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false.\n nein\n\n\nvalueDelimiter\nCharacter\nZeichen, das am Anfang und Ende jeden Wertes vorhanden ist. Default \"\n ja\n\n\nvalueSeparator\nCharacter\nZeichen, das als Trennzeichen zwischen den Werten interpretiert werden soll. Default: ,\n ja\n\n\nxtflogFile\nObject\nSchreibt die log-Meldungen in eine INTERLIS-2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n ja\n\n\n\nFalls die CSV-Datei eine Header-Zeile enthält (mit den Spaltennamen), wird im gegebenen Modell eine Klasse gesucht, welche die Header-Spaltennamen enthält (Gross-/Kleinschreibung sowie optionale “Spalten” der Modell-Klasse werden ignoriert). Wird keine solche Klasse gefunden, gilt das als Validierungsfehler.\nFalls die CSV-Datei keine Header-Zeile enthält (mit den Spaltennamen), wird im gegebenen Modell eine Klasse gesucht, die die selbe Anzahl Attribute hat. Wird keine solche Klasse gefunden, gilt das als Validierungsfehler.\nDie Prüfung von gleichzeitig mehreren CSV-Dateien führt zu Fehlermeldungen wie OID o3158 of object &lt;Modelname&gt;.&lt;Topicname&gt;.&lt;Klassenname&gt; already exists in .... Beim Öffnen und Lesen einer CSV-Datei wird immer der Zähler, der die interne (in der CSV-Datei nicht vorhandene) OID generiert, zurückgesetzt. Somit kann immer nur eine CSV-Datei pro Task geprüft werden.\n\n\nCurl\nSimuliert mit einem HttpClient einige Curl-Befehle.\nBeispiele:\nimport ch.so.agi.gretl.tasks.Curl.MethodType;\n\ntasks.register('uploadData', Curl) {\n    serverUrl = \"https://geodienste.ch/data_agg/interlis/import\"\n    method = MethodType.POST\n    formData = [\"topic\": \"npl_waldgrenzen\", \"lv95_file\": file(\"./test.xtf.zip\"), \"publish\": \"true\", \"replace_all\": \"true\"]\n    user = \"fooUser\"\n    password = \"barPwd\"\n    expectedStatusCode = 200\n    expectedBody = \"\\\"success\\\":true\"\n}\nimport ch.so.agi.gretl.tasks.Curl.MethodType;\n\ntasks.register('uploadData', Curl) {\n    serverUrl = \"https://testweb.so.ch/typo3/api/digiplan\"\n    method = MethodType.POST\n    headers = [\"Content-Type\": \"application/xml\", \"Content-Encoding\": \"gzip\"]\n    dataBinary = file(\"./planregister.xml.gz\")\n    user = \"fooUser\"\n    password = \"barPwd\"\n    expectedStatusCode = 202\n}\nimport ch.so.agi.gretl.tasks.Curl.MethodType;\n\ntasks.register('downloadData', Curl) {\n    serverUrl = \"https://raw.githubusercontent.com/sogis/gretl/master/README.md\"\n    method = MethodType.GET\n    outputFile = file(\"./README.md\")\n    expectedStatusCode = 200\n}\nimport groovy.json.JsonSlurper\n\ndef basicHeader = \"Basic \" + Base64.getEncoder().encodeToString((clientId + \":\" + clientSecret).getBytes())\ndef token = \"\"\n\ntasks.register('getToken', Curl) {\n    serverUrl = \"https://auth.example.ch/oauth2/token\"\n    method = MethodType.POST\n    outputFile = file(\"./token.json\")\n    expectedStatusCode = 200\n    data = \"grant_type=client_credentials&client_id=$clientId\"\n    headers = [\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n        \"Authorization\": basicHeader\n    ]\n\n    doLast {\n        def slurper = new JsonSlurper();\n        def slurped = slurper.parseText(outputFile.text)\n\n        if (slurped.access_token != null)\n            token = slurped.access_token\n\n        if(token.length() == 0)\n            throw new GradleException(\"Failed to retrieve access token\")\n\n        println \"Length of access token is: \" + token.length()\n    }\n}\n\ntasks.register('downloadJson', Curl) {\n    dependsOn 'getToken'\n    serverUrl = \"https://obs.example.ch/rest/v4/docs.json?projects=83505\"\n    method = MethodType.GET\n    outputFile = file(\"./data.json\")\n    expectedStatusCode = 200\n    headers.put(\"Authorization\", providers.provider { \"Bearer \" + token })\n} \n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndata\nString\nString, der via POST hochgeladen werden soll. Entspricht curl [URL] --data.\n ja\n\n\ndataBinary\nFile\nDatei, die hochgeladen werden soll. Entspricht curl [URL] --data-binary.\n ja\n\n\nexpectedBody\nString\nErwarteter Text, der vom Server als Body zurückgelieferd wird.\n ja\n\n\nexpectedStatusCode\nInteger\nErwarteter Status Code, der vom Server zurückgeliefert wird.\n nein\n\n\nformData\nMap&lt;String,Object&gt;\nForm data parameters. Entspricht curl [URL] -F key1=value1 -F file1=@my_file.xtf.\n ja\n\n\nheaders\nMapProperty&lt;String,String&gt;\nRequest-Header. Entspricht curl [URL] -H ... -H .....\n ja\n\n\nmethod\nMethodType\nHTTP-Request-Methode. Unterstützt werden GET und POST.\n ja\n\n\noutputFile\nFile\nDatei, in die der Output gespeichert wird. Entspricht curl [URL] -o.\n ja\n\n\npassword\nString\nPasswort. Wird zusammen mit user in einen Authorization-Header umgewandelt. Entspricht curl [URL] -u user:password.\n ja\n\n\nserverUrl\nString\nDie URL des Servers inklusive Pfad und Queryparameter.\n nein\n\n\nuser\nString\nBenutzername. Wird zusammen mit password in einen Authorization-Header umgewandelt. Entspricht curl [URL] -u user:password.\n ja\n\n\n\n\n\nDatabaseDocumentExport (deprecated)\nSpeichert Dokumente, deren URL in einer Spalte einer Datenbanktabelle gespeichert sind, in einem lokalen Verzeichnis. Zukünftig und bei Bedarf kann der Task so erweitert werden, dass auch BLOBs aus der Datenbank gespeichert werden können.\nRedirect von HTTP nach HTTPS funktionieren nicht. Dies korrekterweise (?) wegen der verwendeten Java-Bibliothek.\nWegen der vom Kanton Solothurn eingesetzten self-signed Zertifikate muss ein unschöner Handstand gemacht werden. Leider kann dieser Usecase schlecht getestet werden, da die Links nur in der privaten Zone verfügbar sind und die zudem noch häufig ändern können. Manuell getestet wurde es jedoch.\nAls Dateiname wird der letzte Teil des URL-Pfades verwendet, z.B. https://artplus.verw.rootso.org/MpWeb-apSolothurnDenkmal/download/2W8v0qRZQBC0ahDnZGut3Q?mode=gis wird mit den Prefix und Extension zu ada_2W8v0qRZQBC0ahDnZGut3Q.pdf.\nEs wird DISTINCT ON (&lt;documentColumn&gt;) und ein Filter WHERE &lt;documentColumn&gt; IS NOT NULL verwendet.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('exportDocuments', DatabaseDocumentExport) {\n    database = [db_uri, db_user, db_pass]\n    qualifiedTableName = \"ada_denkmalschutz.fachapplikation_rechtsvorschrift_link\"\n    documentColumn = \"multimedia_link\"\n    targetDir = file(\".\")\n    fileNamePrefix = \"ada_\"\n    fileNameExtension = \"pdf\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndatabase\nConnector\nDatenbank, aus der die Dokumente exportiert werden sollen.\n nein\n\n\ndocumentColumn\nString\nDB-Tabellenspalte mit dem Dokument resp. der URL zum Dokument.\n nein\n\n\nfileNameExtension\nString\nDateinamen-Extension.\n ja\n\n\nfileNamePrefix\nString\nPrefix für Dateinamen.\n ja\n\n\nqualifiedTableName\nString\nQualifizierter Tabellenname.\n nein\n\n\ntargetDir\nFile\nVerzeichnis in das die Dokumente exportiert werden sollen.\n nein\n\n\n\n\n\nDb2Db\nDies ist prinzipiell ein 1:1-Datenkopie, d.h. es findet kein Datenumbau statt, die Quell- und die Ziel- Tabelle hat jeweils identische Attribute. Es werden auf Seite Quelle in der Regel also simple SELECT-Queries ausgeführt und die Resultate dieser Queries in Tabellen der Ziel-DB eingefügt. Unter bestimmten Bedingungen (insbesondere wenn es sich um einen wenig komplexen Datenumbau handelt), kann dieser Task aber auch zum Datenumbau benutzt werden.\nDie Queries können auf mehrere .sql-Dateien verteilt werden, d.h. der Task muss die Queries mehrerer .sql-Dateien zu einer Transaktion kombinieren können. Jede .sql-Datei gibt genau eine Resultset (RAM-Tabelle) zurück. Das Resultset wird in die konfigurierte Zieltabelle geschrieben. Die Beziehungen sind: Eine bis mehrere Quelltabellen ergeben ein Resultset; das Resultset entspricht bezüglich den Attributen genau der Zieltabelle und wird 1:1 in diese geschrieben. Der Db2Db- Task verarbeitet innerhalb einer Transaktion 1-n Resultsets und wird entsprechend auch mit 1-n SQL-Dateien konfiguriert.\nDie Reihenfolge der .sql-Dateien ist relevant. Dies bedeutet, dass die SQL-Befehle der zuerst angegebenen .sql-Datei zuerst ausgeführt werden müssen, danach die SQL-Befehle der an zweiter Stelle angegebenen .sql-Datei, usw.\nAlle SELECT-Statements werden in einer Transaktion ausgeführt werden, damit ein konsistenter Datenstand gelesen wird. Alle INSERT-Statements werden in einer Transaktion ausgeführt werden, damit bei einem Fehler der bisherige Datenstand bestehen bleibt und also kein unvollständiger Import zurückgelassen wird.\nDamit dieselbe .sql-Datei für verschiedene Datensätze benutzt werden kann, ist es möglich innerhalb der .sql-Datei Parameter zu verwenden und diesen Parametern beim Task einen konkreten Wert zuzuweisen. Innerhalb der .sql-Datei werden Paramter mit folgender Syntax verwendet: ${paramName}.\nEs ist pro .sql-Datei nur ein SELECT-Statement erlaubt. Ausser das erste Statement ist ein “SET search_path TO”-Statement.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('transferSomeData', Db2Db) {\n    sourceDb = [db_uri, db_user, db_pass]\n    targetDb = ['jdbc:sqlite:gretldemo.sqlite',null,null]\n    sqlParameters = [dataset:'Olten']\n    transferSets = [\n        new TransferSet('some.sql', 'albums_dest', true)\n    ];\n}\nDamit mit einer einzigen Task-Definition mehrere Datensätze verarbeitet werden können, kann auch eine Liste von Parametern angegeben werden.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('transferSomeData', Db2Db) {\n    sourceDb = [db_uri, db_user, db_pass]\n    targetDb = ['jdbc:sqlite:gretldemo.sqlite',null,null]\n    sqlParameters = [[dataset:'Olten'],[dataset:'Grenchen']]\n    transferSets = [\n        new TransferSet('some.sql', 'albums_dest', true)\n    ];\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbatchSize\nProperty&lt;Integer&gt;\nAnzahl der Records, die pro Batch in die Ziel-Datenbank geschrieben werden (Default: 5000). Für sehr grosse Tabellen muss ein kleinerer Wert gewählt werden.\n ja\n\n\nfetchSize\nProperty&lt;Integer&gt;\nAnzahl der Records, die auf einmal vom Datenbank-Cursor von der Quell-Datenbank zurückgeliefert werden (Standard: 5000). Für sehr grosse Tabellen muss ein kleinerer Wert gewählt werden.\n ja\n\n\nsourceDb\nListProperty&lt;String&gt;\nDatenbank, aus der gelesen werden soll.\n nein\n\n\nsqlParameters\nProperty&lt;Object&gt;\nEine Map mit Paaren von Parameter-Name und Parameter-Wert (Map&lt;String,String&gt;). Oder eine Liste mit Paaren von Parameter-Name und Parameter-Wert (List&lt;Map&lt;String,String&gt;&gt;).\n ja\n\n\ntargetDb\nListProperty&lt;String&gt;\nDatenbank, in die geschrieben werden soll.\n nein\n\n\ntransferSets\nListProperty&lt;TransferSet&gt;\nEine Liste von TransferSets.\n nein\n\n\n\nEine TransferSet ist\n\neine SQL-Datei (mit SQL-Anweisungen zum Lesen der Daten aus der sourceDb),\ndem Namen der Ziel-Tabelle in der targetDb, und\nder Angabe ob in der Ziel-Tabelle vor dem INSERT zuerst alle Records gelöscht werden sollen.\neinem optionalen vierten Parameter, der verwendet werden kann um den zu erzeugenden SQL-Insert-String zu beeinflussen, u.a. um einen WKT-Geometrie-String in eine PostGIS-Geometrie umzuwandeln\n\nBeispiel, Umwandlung Rechtswert/Hochwertspalten in eine PostGIS-Geometrie (siehe auch GRETL-Job afu_onlinerisk_transfer der eine Punktgeometriespalten aus einer Nicht-Postgis-DB übernimmt):\nnew TransferSet('untersuchungseinheit.sql', 'afu_qrcat_v1.onlinerisk_untersuchungseinheit', true, (String[])[\"geom:wkt:2056\"])\ngeom ist der Geometrie-Spalten-Name der verwendet wird.\nDazugehöriger Auszug aus SQL-Datei zur Erzeugung des WKT-Strings mit Hilfe von concatenation:\n'Point(' || ue.koordinate_x::text || ' ' || ue.koordinate_y::text || ')' AS geom\nUnterstützte Datenbanken: PostgreSQL, SQLite, Oracle, Derby und DuckDB.\n\n\nFtpDelete\nLöscht Daten auf einem FTP-Server.\nBeispiel, löscht alle Daten in einem Verzeichnis:\ntasks.register('ftpdelete', FtpDelete) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir = \"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile = fileTree(pathToTempFolder) { include '*.zip' } \n    //remoteFile = \"*.zip\"\n}\nUm bestimmte Dateien zu löschen:\ntasks.register('ftpdelete', FtpDelete) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir = \"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile = \"*.zip\"\n}\nUm heruntergeladene Daten zu löschen:\ntasks.register('ftpdelete', FtpDelete) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir = \"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile = fileTree(pathToDownloadFolder) { include '*.zip' } \n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncontrolKeepAliveTimeout\nLong\nTimeout bis ein NOOP über den Kontroll-Kanal versendet wird. Default: 300s (=5 Minuten)\n ja\n\n\nfileSeparator\nString\nDefault: /. (Falls systemType Windows ist, ist der Default \\.\n ja\n\n\npassiveMode\nBoolean\nAktiv- oder Passiv-Verbindungsmodus. Default: Passiv (true)\n ja\n\n\npassword\nString\nPasswort für den Zugriff auf dem Server.\n nein\n\n\nremoteDir\nString\nVerzeichnis auf dem Server.\n nein\n\n\nremoteFile\nObject\nDateiname oder Liste der Dateinamen auf dem Server (kann auch ein Muster sein (* oder ?)). Ohne diesen Parameter werden alle Dateien aus dem Remoteverzeichnis gelöscht.\n ja\n\n\nserver\nString\nName des Servers (ohne ftp://).\n nein\n\n\nsystemType\nString\nUNIX oder WINDOWS. Default: UNIX.\n ja\n\n\nuser\nString\nBenutzername auf dem Server.\n nein\n\n\n\n\n\nFtpDownload\nLädt alle Dateien aus dem definierten Verzeichnis des Servers in ein lokales Verzeichnis herunter.\nBeispiel:\ntasks.register('ftpdownload', FtpDownload) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    localDir= \"downloads\"\n    remoteDir= \"\"\n}\nUm eine bestimmte Datei herunterzuladen:\ntasks.register('ftpdownload', FtpDownload) {\n    server=\"ftp.infogrips.ch\"\n    user= \"Hans\"\n    password= \"dummy\"\n    systemType=\"WINDOWS\"\n    localDir= \"downloads\"\n    remoteDir=\"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile=\"240100.zip\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncontrolKeepAliveTimeout\nLong\nTimeout bis ein NOOP über den Kontroll-Kanal versendet wird. Default: 300s (=5 Minuten)\n ja\n\n\nfileSeparator\nString\nDefault: /. (Falls systemType Windows ist, ist der Default \\.\n ja\n\n\nfileType\nString\nASCII oder BINARY. Default: ASCII.\n ja\n\n\nlocalDir\nString\nLokales Verzeichnis, in dem die Dateien gespeichert werden.\n nein\n\n\npassiveMode\nBoolean\nAktiv- oder Passiv-Verbindungsmodus. Default: Passiv (true)\n ja\n\n\npassword\nString\nPasswort für den Zugriff auf dem Server.\n nein\n\n\nremoteDir\nString\nVerzeichnis auf dem Server.\n nein\n\n\nremoteFile\nObject\nDateiname oder Liste der Dateinamen auf dem Server (kann auch ein Muster sein (* oder ?)). Ohne diesen Parameter werden alle Dateien aus dem Remoteverzeichnis heruntergeladen.\n ja\n\n\nserver\nString\nName des Servers (ohne ftp://).\n nein\n\n\nsystemType\nString\nUNIX oder WINDOWS. Default: UNIX.\n ja\n\n\nuser\nString\nBenutzername auf dem Server.\n nein\n\n\n\n\n\nFtpList\nLiefert eine Liste der Dateien aus dem definierten Verzeichnis des Servers.\nBeispiel:\ntasks.register('ftplist', FtpList) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir= \"\"\n    doLast {\n        println files\n    }\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncontrolKeepAliveTimeout\nLong\nTimeout bis ein NOOP über den Kontroll-Kanal versendet wird. Default: 300s (=5 Minuten)\n ja\n\n\nfileSeparator\nString\nDefault: /. (Falls systemType Windows ist, ist der Default \\.\n ja\n\n\npassiveMode\nBoolean\nAktiv- oder Passiv-Verbindungsmodus. Default: Passiv (true)\n ja\n\n\npassword\nString\nPasswort für den Zugriff auf dem Server.\n nein\n\n\nremoteDir\nString\nVerzeichnis auf dem Server.\n nein\n\n\nserver\nString\nName des Servers (ohne ftp://).\n nein\n\n\nsystemType\nString\nUNIX oder WINDOWS. Default: UNIX.\n ja\n\n\nuser\nString\nBenutzername auf dem Server.\n nein\n\n\n\n\n\nGpkg2Dxf\nExportiert alle Tabellen einer GeoPackage-Datei in DXF-Dateien. Als Input wird eine von ili2gpkg erzeugte GeoPackage-Datei benötigt.\nEs werden alle INTERLIS-Klassen exportiert (SELECT tablename FROM T_ILI2DB_TABLE_PROP WHERE setting = 'CLASS'). Der eigentliche SELECT-Befehl ist komplizierter weil für das Layern der einzelnen DXF-Objekte das INTERLIS-Metaattribut !!@dxflayer=\"true\" ausgelesen wird. Gibt es kein solches Metaattribut wird alles in den gleichen DXF-Layer (default) geschrieben.\nEncoding: Die DXF-Dateien sind ISO-8859-1 encodiert.\nAchtung: Task sollte verbessert werden (siehe E-Mail Claude im Rahmen des Publisher-Projektes).\ntasks.register('gpkg2dxf', Gpkg2Dxf) {\n    dataFile = file(\"data.gpkg\")\n    outputDir = file(\"./out/\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nFile\nGeoPackage-Datei, die nach DXF transformiert werden soll.\n nein\n\n\noutputDir\nFile\nVerzeichnis, in das die DXF-Dateien gespeichert werden.\n nein\n\n\n\n\n\nGpkg2Shp\nExportiert alle Tabellen einer GeoPackage-Datei in Shapefiles. Als Input wird eine von ili2gpkg erzeugte GeoPackage-Datei benötigt.\nEs werden alle INTERLIS-Klassen exportiert (SELECT tablename FROM T_ILI2DB_TABLE_PROP WHERE setting = 'CLASS'). Je nach, bei der Erstellung der GeoPackage-Datei, verwendeten Parametern, muss die Query angepasst werden. Es muss jedoch darauf geachtet werden, dass es nur eine Query gibt (für alle Datensätze). Für den vorgesehenen Anwendungsfall (sehr einfache, flache Modelle) dürfte das kein Problem darstellen.\nEncoding: Die Shapefiles sind neu UTF-8 encodiert. Standard ist ISO-8859-1, scheint aber v.a. in QGIS nicht standardmässig zu funktionieren (keine Umlaute).\nAchtung: Task sollte verbessert werden (siehe E-Mail Claude im Rahmen des Publisher-Projektes).\ntasks.register('gpkg2shp', Gpkg2Shp) {\n    dataFile = file(\"data.gpkg\")\n    outputDir = file(\"./out/\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nFile\nGeoPackage-Datei, die nach Shapefile transformiert werden soll.\n nein\n\n\noutputDir\nFile\nVerzeichnis, in das die Shapefile gespeichert werden.\n nein\n\n\n\n\n\nGpkgExport\nDaten aus einer bestehenden Datenbanktabelle werden in eine GeoPackage-Datei exportiert.\nBeispiele:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('gpkgexport', GpkgExport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"gpkgexport\"\n    srcTableName = \"exportdata\"\n    dataFile = \"data.gpkg\"\n    dstTableName = \"exportdata\"\n}\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('gpkgexport', GpkgExport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"gpkgexport\"\n    srcTableName = [\"exportTable1\", \"exportTable2\"]\n    dataFile = \"data.gpkg\"\n    dstTableName = [\"exportTable1\", \"exportTable2\"]\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbatchSize\nInteger\nAnzahl der Records, die pro Batch in die Ziel-Datenbank (GeoPackage) geschrieben werden. Default: 5000\n ja\n\n\ndataFile\nObject\nName der GeoPackage-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndstTableName\nObject\nName der Tabelle(n) in der GeoPackage-Datei. String oder List.\n nein\n\n\nfetchSize\nInteger\nAnzahl der Records, die pro Fetch aus der Quell-Datenbank gelesen werden. Default: 5000\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\nsrcTableName\nObject\nName der DB-Tabelle(n), die exportiert werden soll(en). String oder List.\n nein\n\n\n\n\n\nGpkgImport\nDaten aus einer GeoPackage-Datei in eine bestehende Datenbanktabelle importieren.\nDie Tabelle kann weitere Spalten enthalten, die in der GeoPackage-Datei nicht vorkommen. Sie müssen aber NULLable sein, oder einen Default-Wert definiert haben.\nDie Gross-/Kleinschreibung der GeoPackage-Spaltennamen wird für die Zuordnung zu den DB-Spalten ignoriert.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('gpkgimport', GpkgImport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"gpkgimport\"\n    srcTableName = \"Point\"\n    dstTableName = \"importdata\"\n    dataFile = \"point.gpkg\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbatchSize\nInteger\nAnzahl der Records, die pro Batch in die Ziel-Datenbank geschrieben werden. Default: 5000\n ja\n\n\ndataFile\nObject\nName der GeoPackage-Datei, die gelesen werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, in die importiert werden soll.\n nein\n\n\ndstTableName\nString\nName der DB-Tabelle, in die importiert werden soll.\n nein\n\n\nfetchSize\nInteger\nAnzahl der Records, die pro Fetch aus der Quell-Datenbank gelesen werden. Default: 5000\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\nsrcTableName\nString\nName der GeoPackage-Tabelle, die importiert werden soll.\n nein\n\n\n\n\n\nGpkgValidator\nPrüft eine GeoPackage-Datei gegenüber einem INTERLIS-Modell. Basiert auf dem ilivalidator.\nBeispiel:\ntasks.register('validate', GpkgValidator) {\n    models = \"GpkgModel\"\n    dataFiles = [\"attributes.gpkg\"]\n    tableName = \"Attributes\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nallObjectsAccessible\nBoolean\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n ja\n\n\nconfigFile\nObject\nKonfiguriert die Datenprüfung mit Hilfe einer ini-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\ndataFiles\nObject\nListe der Dateien, die validiert werden sollen. FileCollection oder List. Eine leere Liste ist kein Fehler.\n nein\n\n\ndisableAreaValidation\nBoolean\nSchaltet die AREA-Topologieprüfung aus. Default: false\n ja\n\n\nfailOnError\nBoolean\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n ja\n\n\nforceTypeValidation\nBoolean\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n ja\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n ja\n\n\nmetaConfigFile\nObject\nKonfiguriert den Validator mit Hilfe einer ini-Datei. Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell, gegen das die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Der Name der CSV-Datei.\n ja\n\n\nmultiplicityOff\nBoolean\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n ja\n\n\npluginFolder\nObject\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\nskipPolygonBuilding\nBoolean\nSchaltet die Bildung der Polygone aus (nur ITF). Default: false\n ja\n\n\ntableName\nString\nName der Tabelle in den GeoPackage-Dateien.\n nein\n\n\nvalidationOk\nboolean\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false.\n nein\n\n\nxtflogFile\nObject\nSchreibt die log-Meldungen in eine INTERLIS-2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n ja\n\n\n\n\n\nGzip\nGzipped eine einzelne Datei. Es gibt einen eingebauten Tar-Task, der - nomen est omen - aber immer zuerst eine Tar-Datei erstellt.\nBeispiel:\ntasks.register('compressFile', Gzip) {\n    dataFile = file(\"./planregister.xml\");\n    gzipFile = file(\"./planregister.xml.gz\");\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nProperty&lt;File&gt;\nDatei, die gezipped werden soll.\n nein\n\n\ngzipFile\nProperty&lt;File&gt;\nOutput-Datei\n nein\n\n\n\n\n\nIli2gpkgImport\nImportiert Daten aus einer INTERLIS-Transferdatei in eine GeoPackage-Datei.\nDie Tabellen werden implizit auch angelegt, falls sie noch nicht vorhanden sind. Falls die Tabellen in der Datenbank schon vorhanden sind, können sie zusätzliche Spalten enthalten (z.B. bfsnr, datum etc.), welche beim Import leer bleiben.\nFalls beim Import ein Datensatz-Identifikator (dataset) definiert wird, darf dieser Datensatz-Identifikator in der Datenbank noch nicht vorhanden sein.\nUm die bestehenden (früher importierten) Daten zu ersetzen, kann der Task Ili2gpkgReplace (not yet implemented) verwendet werden.\nDie Option --doSchemaImport wird automatisch gesetzt.\nBeispiel:\ntasks.register('importData', Ili2gpkgImport) {\n    models = \"SO_AGI_AV_GB_Administrative_Einteilungen_20180613\"\n    dataFile = file(\"data.xtf\");\n    dbfile = file(\"data.gpkg\")    \n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nString\nEntspricht der ili2gpkg-Option --baskets\n ja\n\n\ndataFile\nObject\nName der XTF-/ITF-Datei, die gelesen werden soll. Es können auch mehrere Dateien sein. FileCollection oder File\n nein\n\n\ndataset\nObject\nEntspricht der ili2gpkg-Option --dataset\n ja\n\n\ndbfile\nFile\nGeoPackage-Datei in die importiert werden soll.\n nein\n\n\ndefaultSrsCode\nString\nEntspricht der ili2gpkg-Option --defaultSrsCode\n ja\n\n\nisCoalesceJson\nBoolean\nEntspricht der ili2gpkg-Option --coalesceJson\n ja\n\n\nisCreateEnumTabs\nBoolean\nEntspricht der ili2gpkg-Option --createEnumTabs\n ja\n\n\nisCreateGeomIdx\nBoolean\nEntspricht der ili2gpkg-Option --createGeomIdx\n ja\n\n\nisCreateMetaInfo\nBoolean\nEntspricht der ili2gpkg-Option --createMetaInfo\n ja\n\n\nisDeleteData\nBoolean\nEntspricht der ili2gpkg-Option --deleteData\n ja\n\n\nisDisableAreaValidation\nBoolean\nEntspricht der ili2gpkg-Option --disableAreaValidation\n ja\n\n\nisDisableValidation\nBoolean\nEntspricht der ili2gpkg-Option --disableValidation\n ja\n\n\nisForceTypeValidation\nBoolean\nEntspricht der ili2gpkg-Option --forceTypeValidation\n ja\n\n\nisIligml20\nBoolean\nEntspricht der ili2gpkg-Option --iligml20\n ja\n\n\nisImportTid\nBoolean\nEntspricht der ili2gpkg-Option --importTid\n ja\n\n\nisNameByTopic\nBoolean\nEntspricht der ili2gpkg-Option --nameByTopic\n ja\n\n\nisSkipGeometryErrors\nBoolean\nEntspricht der ili2gpkg-Option --skipGeometryErrors\n ja\n\n\nisSkipPolygonBuilding\nBoolean\nEntspricht der ili2gpkg-Option --skipPolygonBuilding\n ja\n\n\nisStrokeArcs\nBoolean\nEntspricht der ili2gpkg-Option --strokeArcs\n ja\n\n\nisTrace\nBoolean\nEntspricht der ili2gpkg-Option --trace\n ja\n\n\nlogFile\nObject\nEntspricht der ili2gpkg-Option --logFile\n ja\n\n\nmodeldir\nString\nEntspricht der ili2gpkg-Option --modeldir\n ja\n\n\nmodels\nString\nEntspricht der ili2gpkg-Option --models\n ja\n\n\npostScript\nFile\nEntspricht der ili2gpkg-Option --postScript\n ja\n\n\npreScript\nFile\nEntspricht der ili2gpkg-Option --preScript\n ja\n\n\nproxy\nString\nEntspricht der ili2gpkg Option --proxy\n ja\n\n\nproxyPort\nInteger\nEntspricht der ili2gpkg-Option --proxyPort\n ja\n\n\ntopics\nString\nEntspricht der ili2gpkg-Option --topics\n ja\n\n\nvalidConfigFile\nFile\nEntspricht der ili2gpkg-Option --validConfigFile\n ja\n\n\n\nFür die Beschreibung der einzenen ili2gpkg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIli2pgDelete\nLöscht einen Datensatz in der PostgreSQL-Datenbank anhand eines Datensatz-Identifikators. Diese Funktion bedingt, dass das Datenbankschema mit der Option createBasketCol erstellt wurde (via Task Ili2pgImportSchema). Der Parameter failOnException muss false sein, ansonsten bricht der Job ab.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('deleteDataset', Ili2pgDelete) {\n    database = [db_uri, db_user, db_pass]\n    models = \"DM01AVSO24LV95\"\n    dbschema = \"dm01\"\n    dataset = \"kammersrohr\"\n}\nEs können auch mehrere Datensätze pro Task gelöscht werden:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('deleteDataset', Ili2pgDelete) {\n    database = [db_uri, db_user, db_pass]\n    dbschema = \"dm01\"\n    dataset = [\"Olten\",\"Grenchen\"]\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIli2pgExport\nExportiert Daten aus der PostgreSQL-Datenbank in eine INTERLIS-Transferdatei.\nMit dem Parameter models, topics, baskets oder dataset wird definiert, welche Daten exportiert werden.\nOb die Daten im INTERLIS-1-, INTERLIS-2- oder GML-Format geschrieben werden, ergibt sich aus der Dateinamenserweiterung der Ausgabedatei. Für eine INTERLIS-1-Transferdatei muss die Erweiterung .itf verwendet werden. Für eine GML-Transferdatei muss die Erweiterung .gml verwendet werden.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('exportData', Ili2pgExport) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900-out.itf\"\n    dataset = \"254900\"\n    logFile = \"ili2pg.log\"\n}\nDamit mit einer einzigen Task-Definition mehrere Datensätze verarbeitet werden können, kann auch eine Liste von Dateinamen und Datensätzen angegeben werden.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('exportData', Ili2pgExport) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = [\"lv03_254900-out.itf\",\"lv03_255000-out.itf\"]\n    dataset = [\"254900\",\"255000\"]\n    logFile = \"ili2pg.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexport3\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --export3.\n ja\n\n\nexportModels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --exportModels.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIli2pgImport\nImportiert Daten aus einer INTERLIS-Transferdatei in die PostgreSQL-Datenbank.\nDie Tabellen werden implizit auch angelegt, falls sie noch nicht vorhanden sind. Falls die Tabellen in der Datenbank schon vorhanden sind, können sie zusätzliche Spalten enthalten (z.B. bfsnr, datum etc.), welche beim Import leer bleiben.\nFalls beim Import ein Datensatz-Identifikator (dataset) definiert wird, darf dieser Datensatz-Identifikator in der Datenbank noch nicht vorhanden sein.\nFalls man mehrere Dateien importieren will, diese jedoch erst zur Laufzeit eruiert werden können, muss der Parameter dataFile eine Gradle FileCollection resp. eine implementierende Klasse (z.B. FileTree) sein. Gleiches gilt für den dataset-Parameter. Als einzelner Wert für das Dataset wird in diesem Fall der Name der Datei ohne Extension und ohne Pfad verwendet. Leider kann nicht bereits in der Task-Definition aus dem Filetree eine Liste gemacht werden, z.B. fileTree(pathToUnzipFolder) { include '*.itf' }.files.name. Diese Liste ist leer.\nUm die bestehenden (früher importierten) Daten zu ersetzen, kann der Task Ili2pgReplace verwendet werden.\nBeispiel 1:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('importData', Ili2pgImport) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900.itf\"\n    logFile = \"ili2pg.log\"\n}\nBeispiel 2:\nImport der AV-Daten. In der t_datasetname-Spalte soll die BFS-Nummer stehen. Die BFS-Nummer entspricht den ersten vier Zeichen des Filenamens.\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('importData', Ili2pgImport) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = fileTree(pathToUnzipFolder) { include '*.itf' }\n    dataset = dataFile\n    datasetSubstring = (0..4).toList()\n    logFile = \"ili2pg.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndataFile\nProperty&lt;Object&gt;\nName der XTF-/ITF-Datei, die gelesen werden soll. Es können auch mehrere Dateien sein. FileCollection oder String.\n nein\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIli2pgImportSchema\nErstellt die Tabellenstruktur in der PostgreSQL-Datenbank anhand eines INTERLIS-Modells.\nDer Parameter iliFile oder models muss gesetzt werden.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('importSchema', Ili2pgImportSchema) {\n    database = [db_uri, db_user, db_pass]\n    models = \"DM01AVSO24\"\n    dbschema = \"gretldemo\"\n    logFile = \"ili2pg.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\nbeautifyEnumDispName\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --beautifyEnumDispName.\n ja\n\n\ncoalesceArray\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceArray.\n ja\n\n\ncoalesceCatalogueRef\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceCatalogueRef.\n ja\n\n\ncoalesceJson\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceJson.\n ja\n\n\ncoalesceMultiLine\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceMultiline.\n ja\n\n\ncoalesceMultiSurface\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --coalesceMultiSurface.\n ja\n\n\ncreateBasketCol\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createBasketCol.\n ja\n\n\ncreateDatasetCol\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createDatasetCol.\n ja\n\n\ncreateDateTimeChecks\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createDateTimeChecks.\n ja\n\n\ncreateEnumColAsItfCode\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createEnumColAsItfCode.\n ja\n\n\ncreateEnumTabs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createEnumTabs.\n ja\n\n\ncreateEnumTabsWithId\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createEnumTabsWithId.\n ja\n\n\ncreateEnumTxtCol\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createEnumTxtCol.\n ja\n\n\ncreateFk\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createFk.\n ja\n\n\ncreateFkIdx\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createFkIdx.\n ja\n\n\ncreateGeomIdx\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createGeomIdx.\n ja\n\n\ncreateImportTabs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createImportTabs.\n ja\n\n\ncreateMetaInfo\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createMetaInfo.\n ja\n\n\ncreateNumChecks\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createNumChecks.\n ja\n\n\ncreateSingleEnumTab\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createSingleEnumTab.\n ja\n\n\ncreateStdCols\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createStdCols.\n ja\n\n\ncreateTextChecks\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createTextChecks.\n ja\n\n\ncreateTidCol\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createTidCol.\n ja\n\n\ncreateTypeConstraint\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createTypeConstraint.\n ja\n\n\ncreateTypeDiscriminator\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createTypeDescriminator.\n ja\n\n\ncreateUnique\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --createUnique.\n ja\n\n\ncreatescript\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --createscript.\n ja\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndefaultSrsAuth\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --defaultSrsAuth.\n ja\n\n\ndefaultSrsCode\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --defaultSrsCode.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableNameOptimization\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableNameOptimization.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\ndropscript\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dropscript.\n ja\n\n\nexpandMultilingual\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --expandMultilingual.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\nidSeqMax\nProperty&lt;Long&gt;\nEntspricht der ili2pg-Option --idSeqMax.\n ja\n\n\nidSeqMin\nProperty&lt;Long&gt;\nEntspricht der ili2pg-Option --idSeqMin.\n ja\n\n\niliFile\nProperty&lt;Object&gt;\nName der ili-Datei, die gelesen werden soll.\n ja\n\n\niliMetaAttrs\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --iliMetaAttrs.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nkeepAreaRef\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --keepAreaRef.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmaxNameLength\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --maxNameLength.\n ja\n\n\nmetaConfig\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --metaConfig.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\nnameByTopic\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --nameByTopic.\n ja\n\n\nnoSmartMapping\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --noSmartMapping.\n ja\n\n\noneGeomPerTable\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --oneGeomPerTable.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nsetupPgExt\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --setupPgExt.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nsmart1Inheritance\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --smart1Inheritance.\n ja\n\n\nsmart2Inheritance\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --smart2Inheritance.\n ja\n\n\nsqlColsAsText\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --sqlColsAsText.\n ja\n\n\nsqlEnableNull\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --sqlEnableNull.\n ja\n\n\nsqlExtRefCols\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --sqlExtRefCols.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\nt_id_Name\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --t_id_Name.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\ntranslation\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --translation.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIli2pgReplace\nErsetzt die Daten in der PostgreSQL-Datenbank anhand eines Datensatz-Identifikators (dataset) mit den Daten aus einer INTERLIS-Transferdatei. Diese Funktion bedingt, dass das Datenbankschema mit der Option createBasketCol erstellt wurde (via Task Ili2pgImportSchema).\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('replaceData', Ili2pgReplace) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900.itf\"\n    dataset = \"254900\"\n    logFile = \"ili2pg.log\"\n}\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('importData', Ili2pgReplace) {\n    database = [db_uri, db_user, db_pass]\n    models = 'DM01AVSO24LV95'\n    dbschema = 'agi_dm01avso24'\n    dataFile = fileTree(dir: dm01SoDir, include: '*.itf')\n    dataset = dataFile\n    datasetSubstring = 0..4\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndataFile\nProperty&lt;Object&gt;\nnull\n nein\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIli2pgUpdate\nAktualisiert die Daten in der PostgreSQL-Datenbank anhand einer INTERLIS-Transferdatei, d.h. neue Objekte werden eingefügt, bestehende Objekte werden aktualisiert und in der Transferdatei nicht mehr vorhandene Objekte werden gelöscht.\nDiese Funktion bedingt, dass das Datenbankschema mit der Option --createBasketCol erstellt wurde (via Task Ili2pgImportSchema), und dass die Klassen und Topics eine stabile OID haben.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('updateData', Ili2pgUpdate) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900.itf\"\n    dataset = \"254900\"\n    logFile = \"ili2pg.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndataFile\nProperty&lt;Object&gt;\nnull\n nein\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIli2pgValidate\nPrüft die Daten ohne diese in eine Datei zu exportieren. Der Task ist erfolgreich, wenn keine Fehler gefunden werden und ist nicht erfolgreich, wenn Fehler gefunden werden. Mit der Option failOnException=false ist der Task erfolgreich, auch wenn Fehler gefunden werden.\nMit dem Parameter --models, --topics, --baskets oder --dataset wird definiert, welche Daten geprüft werden. Parameter --dataset akzeptiert auch eine Liste von Datasets.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('validate', Ili2pgValidate) {\n    database = [db_uri, db_user, db_pass]\n    models = \"SO_AGI_AV_GB_Administrative_Einteilungen_20180613\"\n    modeldir = rootProject.projectDir.toString() + \";http://models.interlis.ch\"\n    dbschema = \"agi_av_gb_admin_einteilungen_fail\"\n    logFile = file(\"fubar.log\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nbaskets\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --baskets.\n ja\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank aus der exportiert werden soll.\n nein\n\n\ndataset\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --dataset.\n ja\n\n\ndatasetSubstring\nListProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --datasetSubstring.\n ja\n\n\ndbschema\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --dbschema.\n ja\n\n\ndeleteData\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --deleteData.\n ja\n\n\ndisableAreaValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableAreaValidation.\n ja\n\n\ndisableRounding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableRounding.\n ja\n\n\ndisableValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --disableValidation.\n ja\n\n\nexportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --exportTid.\n ja\n\n\nfailOnException\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --failOnException.\n ja\n\n\nforceTypeValidation\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --forceTypeValidation.\n ja\n\n\niligml20\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --iligml20.\n ja\n\n\nimportBid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importBid.\n ja\n\n\nimportTid\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --importTid.\n ja\n\n\nlogFile\nProperty&lt;Object&gt;\nEntspricht der ili2pg-Option --logFile.\n ja\n\n\nmodeldir\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --modeldir.\n ja\n\n\nmodels\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --models.\n ja\n\n\npostScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --postScript.\n ja\n\n\npreScript\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --preScript.\n ja\n\n\nproxy\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --proxy.\n ja\n\n\nproxyPort\nProperty&lt;Integer&gt;\nEntspricht der ili2pg-Option --proxyPort.\n ja\n\n\nskipGeometryErrors\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipGeometryErrors.\n ja\n\n\nskipPolygonBuilding\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --skipPolygonBuilding.\n ja\n\n\nstrokeArcs\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --strokeArcs.\n ja\n\n\ntopics\nProperty&lt;String&gt;\nEntspricht der ili2pg-Option --topics.\n ja\n\n\ntrace\nProperty&lt;Boolean&gt;\nEntspricht der ili2pg-Option --trace.\n ja\n\n\nvalidConfigFile\nProperty&lt;File&gt;\nEntspricht der ili2pg-Option --validConfigFile.\n ja\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIliValidator\nPrüft eine INTERLIS-Datei (.itf oder .xtf) gegenüber einem INTERLIS-Modell (.ili). Basiert auf dem ilivalidator.\nBeispiel:\ntasks.register('validate', IliValidator) {\n    dataFiles = [\"Beispiel2a.xtf\"]\n    logFile = \"ilivalidator.log\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nallObjectsAccessible\nBoolean\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n ja\n\n\nconfigFile\nObject\nKonfiguriert die Datenprüfung mit Hilfe einer ini-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\ndataFiles\nObject\nListe der Dateien, die validiert werden sollen. FileCollection oder List. Eine leere Liste ist kein Fehler.\n nein\n\n\ndisableAreaValidation\nBoolean\nSchaltet die AREA-Topologieprüfung aus. Default: false\n ja\n\n\nfailOnError\nBoolean\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n ja\n\n\nforceTypeValidation\nBoolean\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n ja\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n ja\n\n\nmetaConfigFile\nObject\nKonfiguriert den Validator mit Hilfe einer ini-Datei. Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell, gegen das die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Der Name der CSV-Datei.\n ja\n\n\nmultiplicityOff\nBoolean\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n ja\n\n\npluginFolder\nObject\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\nskipPolygonBuilding\nBoolean\nSchaltet die Bildung der Polygone aus (nur ITF). Default: false\n ja\n\n\nvalidationOk\nboolean\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false.\n nein\n\n\nxtflogFile\nObject\nSchreibt die log-Meldungen in eine INTERLIS-2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n ja\n\n\n\nZusatzfunktionen (Custom Functions): Die pluginFolder-Option ist zum jetzigen Zeitpunkt ohne Wirkung. Die Zusatzfunktionen werden als normale Abhängigkeit definiert und in der ilivalidator-Task-Implementierung registriert. Das Laden der Klassen zur Laufzeit in iox-ili hat nicht funktioniert (NoClassDefFoundError…). Der Plugin-Mechanismus von ilivalidator wird momentan ohnehin geändert (“Ahead-Of-Time-tauglich” gemacht).\n\n\nJsonImport\nDaten aus einer Json-Datei in eine Datenbanktabelle importieren. Die gesamte Json-Datei (muss UTF-8 encoded sein) wird als Text in eine Spalte importiert. Ist das Json-Objekt in der Datei ein Top-Level-Array wird für jedes Element des Arrays ein Record in der Datenbanktabelle erzeugt.\nBeispiel:\ntasks.register('importJson', JsonImport) {\n    database = [db_uri, db_user, db_pass]\n    jsonFile = \"data.json\"\n    qualifiedTableName = \"jsonimport.jsonarray\"\n    columnName = \"json_text_col\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ncolumnName\nString\nSpaltenname der Tabelle, in die importiert werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, in die importiert werden soll.\n nein\n\n\nisDeleteAllRows\nBoolean\nInhalt der Tabelle vorgängig löschen?\n ja\n\n\njsonFile\nString\nJSON-Datei, die importiert werden soll.\n nein\n\n\nqualifiedTableName\nString\nQualifizierter Namen der Tabelle (“schema.tabelle”), in die importiert werden soll.\n nein\n\n\n\n\n\nPostgisRasterExport\nExportiert eine PostGIS-Raster-Spalte in eine Raster-Datei mittels SQL-Query. Die SQL-Query darf nur einen Record zurückliefern, d.h. es muss unter Umständen ST_Union() verwendet werden. Es wird angenommen, dass die erste bytea-Spalte des Resultsets die Rasterdaten enthält. Weitere bytea-Spalten werden ignoriert. Das Outputformat und die Formatoptionen müssen in der SQL-Datei (in der Select-Query) angegeben werden, z.B.:\nSELECT\n    1::int AS foo, ST_AsGDALRaster((ST_AsRaster(ST_Buffer(ST_Point(2607880,1228287),10),150, 150)), 'AAIGrid', ARRAY[''], 2056) AS raster\n;\nBeispiel:\ntasks.register('exportTiff', PostgisRasterExport) {\n    database = [db_uri, db_user, db_pass]\n    sqlFile = \"raster.sql\"\n    dataFile = \"export.tif\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nObject\nName der Rasterdatei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, aus der exportiert werden soll.\n nein\n\n\nsqlFile\nString\nName der SQL-Datei aus der das SQL-Statement gelesen und ausgeführt wird.\n nein\n\n\nsqlParameters\nMap&lt;String,String&gt;\nEine Map mit Paaren von Parameter-Name und Parameter-Wert.\n ja\n\n\n\n\n\nPublisher\nStellt für Vektordaten die aktuellsten Geodaten-Dateien bereit und pflegt das Archiv der vorherigen Zeitstände.\nDetails\n\n\nS3Bucket2Bucket\nKopiert Objekte von einem Bucket in einen anderen. Die Buckets müssen in der gleichen Region sein. Die Permissions werden nicht mitkopiert und müssen explizit gesetzt werden.\nBeispiel:\ntasks.register('copyFiles', S3Bucket2Bucket) {\n    accessKey = s3AccessKey\n    secretKey = s3SecretKey\n    sourceBucket = s3SourceBucket\n    targetBucket = s3TargetBucket\n    acl = \"public-read\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\naccessKey\nString\nAccessKey\n nein\n\n\nacl\nString\nAccess Control Layer [private, public-read, public-read-write, authenticated-read, aws-exec-read, bucket-owner-read, bucket-owner-full-control]\n nein\n\n\nendPoint\nString\nS3-Endpunkt. Default: https://s3.eu-central-1.amazonaws.com/\n ja\n\n\nmetaData\nMap&lt;String,String&gt;\nMetadaten des Objektes resp. der Objekte, z.B. [\"lastModified\":\"2020-08-28\"].\n ja\n\n\nregion\nString\nS3-Region. Default: eu-central-1\n ja\n\n\nsecretKey\nString\nSecretKey\n nein\n\n\nsourceBucket\nString\nBucket, aus dem die Objekte kopiert werden.\n nein\n\n\ntargetBucket\nString\nBucket, in den die Objekte kopiert werden.\n nein\n\n\n\n\n\nS3Download\nLädt eine Datei aus einem S3-Bucket herunter.\nBeispiel:\ntasks.register('downloadFile', S3Download) {\n    accessKey = 'abcdefg'\n    secretKey = 'hijklmnopqrstuvwxy'\n    downloadDir = file(\"./path/to/dir/\")\n    bucketName = \"ch.so.ada.denkmalschutz\"\n    key = \"foo.pdf\"\n    endPoint = \"https://s3.eu-central-1.amazonaws.com\" \n    region = \"eu-central-1\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\naccessKey\nString\nAccessKey\n nein\n\n\nbucketName\nString\nName des Buckets, in dem die Datei gespeichert ist.\n nein\n\n\ndownloadDir\nFile\nVerzeichnis, in das die Datei heruntergeladen werden soll.\n nein\n\n\nendPoint\nString\nS3-Endpunkt. Default: https://s3.eu-central-1.amazonaws.com/\n ja\n\n\nkey\nString\nName der Datei.\n nein\n\n\nregion\nString\nS3-Region. Default: eu-central-1\n ja\n\n\nsecretKey\nString\nSecretKey\n nein\n\n\n\n\n\nS3Upload\nLädt ein Dokument (sourceFile) oder alle Dokumente in einem Verzeichnis (sourceDir) in einen S3-Bucket (bucketName) hoch.\nMit dem passenden Content-Typ kann man das Verhalten des Browsers steuern. Default ist ‘application/octect-stream’, was dazu führt, dass die Datei immer heruntergeladen wird. Soll z.B. ein PDF oder ein Bild im Browser direkt angezeigt werden, muss der korrekte Content-Typ gewählt werden.\nBeispiel:\ntasks.register('uploadDirectory', S3Upload) {\n    accessKey = 'abcdefg'\n    secretKey = 'hijklmnopqrstuvwxy'\n    sourceDir = file(\"./docs\")\n    bucketName = \"ch.so.ada.denkmalschutz\"\n    endPoint = \"https://s3.eu-central-1.amazonaws.com\" \n    region = \"eu-central-1\"\n    acl = \"public-read\"\n    contentType = \"application/pdf\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\naccessKey\nString\nAccessKey\n nein\n\n\nacl\nString\nAccess Control Layer [private, public-read, public-read-write, authenticated-read, aws-exec-read, bucket-owner-read, bucket-owner-full-control]\n nein\n\n\nbucketName\nString\nName des Buckets, in dem die Dateien gespeichert werden sollen.\n nein\n\n\ncontentType\nString\nContent-Type\n ja\n\n\nendPoint\nString\nS3-Endpunkt. Default: https://s3.eu-central-1.amazonaws.com/\n ja\n\n\nmetaData\nMap&lt;String,String&gt;\nMetadaten des Objektes resp. der Objekte, z.B. [\"lastModified\":\"2020-08-28\"]\n ja\n\n\nregion\nString\nS3-Region. Default: eu-central-1\n nein\n\n\nsecretKey\nString\nSecretKey\n nein\n\n\nsourceDir\nObject\nVerzeichnis mit den Dateien, die hochgeladen werden sollen.\n ja\n\n\nsourceFile\nObject\nDatei, die hochgeladen werden soll.\n ja\n\n\nsourceFiles\nObject\nFileCollection mit den Dateien, die hochgeladen werden sollen, z.B. fileTree(\"/path/to/directoy/\") { include \"*.itf\" }\n ja\n\n\n\n\n\nShpExport\nDaten aus einer bestehenden Datenbanktabelle werden in eine SHP-Datei exportiert.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('shpexport', ShpExport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"shpexport\"\n    tableName = \"exportdata\"\n    dataFile = \"data.shp\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nObject\nName der SHP-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, aus der exportiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der SHP-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\ntableName\nString\nName der DB-Tabelle, die exportiert werden soll.\n nein\n\n\n\n\n\nShpImport\nDaten aus einer SHP-Datei in eine bestehende Datenbanktabelle importieren.\nDie Tabelle kann weitere Spalten enthalten, die in der SHP-Datei nicht vorkommen. Sie müssen aber NULLable sein, oder einen Default-Wert definiert haben.\nDie Tabelle muss eine Geometriespalte enthalten. Der Name der Geometriespalte kann beliebig gewählt werden.\nDie Gross-/Kleinschreibung der SHP-Spaltennamen wird für die Zuordnung zu den DB-Spalten ignoriert.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('shpimport', ShpImport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"shpimport\"\n    tableName = \"importdata\"\n    dataFile = \"data.shp\"\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndataFile\nObject\nName der SHP-Datei, die erstellt werden soll.\n nein\n\n\ndatabase\nConnector\nDatenbank, aus der exportiert werden soll.\n nein\n\n\nencoding\nString\nZeichencodierung der SHP-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nschemaName\nString\nName des DB-Schemas, in dem die DB-Tabelle ist.\n ja\n\n\ntableName\nString\nName der DB-Tabelle, die exportiert werden soll.\n nein\n\n\n\n\n\nShpValidator\nPrüft eine SHP-Datei gegenüber einem INTERLIS-Modell. Basiert auf dem ilivalidator.\nIm gegebenen Modell wird eine Klasse gesucht, die genau die Attributenamen wie in der Shp-Datei enthält (wobei die Gross-/Kleinschreibung ignoriert wird); die Attributtypen werden ignoriert. Wird keine solche Klasse gefunden, gilt das als Validierungsfehler.\nDie Prüfung von gleichzeitig mehreren Shapefiles führt zu Fehlermeldungen wie OID o3158 of object &lt;Modelname&gt;.&lt;Topicname&gt;.&lt;Klassenname&gt; already exists in .... Beim Öffnen und Lesen eines Shapefiles wird immer der Zähler, der die interne (im Shapefile nicht vorhandene) OID generiert, zurückgesetzt. Somit kann immer nur ein Shapefile pro Task geprüft werden.\nBeispiel:\ntasks.register('validate', ShpValidator) {\n    models = \"ShpModel\"\n    dataFiles = [\"data.shp\"]\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nallObjectsAccessible\nBoolean\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n ja\n\n\nconfigFile\nObject\nKonfiguriert die Datenprüfung mit Hilfe einer ini-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\ndataFiles\nObject\nListe der Dateien, die validiert werden sollen. FileCollection oder List. Eine leere Liste ist kein Fehler.\n nein\n\n\ndisableAreaValidation\nBoolean\nSchaltet die AREA-Topologieprüfung aus. Default: false\n ja\n\n\nencoding\nString\nZeichencodierung der SHP-Datei, z.B. UTF-8. Default: Systemeinstellung\n ja\n\n\nfailOnError\nBoolean\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n ja\n\n\nforceTypeValidation\nBoolean\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n ja\n\n\nlogFile\nObject\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n ja\n\n\nmetaConfigFile\nObject\nKonfiguriert den Validator mit Hilfe einer ini-Datei. Siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n ja\n\n\nmodeldir\nString\nINTERLIS-Modellrepository. String separiert mit Semikolon (analog ili2db, ilivalidator).\n ja\n\n\nmodels\nString\nINTERLIS-Modell, gegen das die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Der Name der CSV-Datei.\n ja\n\n\nmultiplicityOff\nBoolean\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n ja\n\n\npluginFolder\nObject\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n ja\n\n\nproxy\nString\nProxy-Server für den Zugriff auf Modell-Repositories.\n ja\n\n\nproxyPort\nInteger\nProxy-Port für den Zugriff auf Modell-Repositories.\n ja\n\n\nskipPolygonBuilding\nBoolean\nSchaltet die Bildung der Polygone aus (nur ITF). Default: false\n ja\n\n\nvalidationOk\nboolean\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false.\n nein\n\n\nxtflogFile\nObject\nSchreibt die log-Meldungen in eine INTERLIS-2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n ja\n\n\n\n\n\nSqlExecutor\nDer SqlExecutor-Task dient dazu, Datenumbauten auszuführen.\nEr wird im Allgemeinen dann benutzt, wenn\n\nder Datenumbau komplex ist und deshalb nicht im Db2Db-Task erledigt werden kann\noder wenn die Quell-DB keine PostgreSQL-DB ist (weil bei komplexen Queries für den Datenumbau möglicherweise fremdsystemspezifische SQL-Syntax verwendet werden müsste)\noder wenn Quell- und Zielschema in derselben Datenbank liegen\n\nIn den Fällen 1 und 2 werden Stagingtabellen bzw. ein Stagingschema benötigt, in welche der Db2Db-Task die Daten zuerst 1:1 hineinschreibt. Der SqlExecutor-Task liest danach die Daten von dort, baut sie um und schreibt sie dann ins Zielschema. Die Queries für den SqlExecutor-Task können alle in einem einzelnen .sql-File sein oder (z.B. aus Gründen der Strukturierung oder Organisation) auf mehrere .sql-Dateien verteilt sein. Die Reihenfolge der .sql-Dateien ist relevant. Dies bedeutet, dass die SQL-Befehle des zuerst angegebenen .sql-Datei zuerst ausgeführt werden müssen, danach dies SQL-Befehle des an zweiter Stelle angegebenen .sql-Datei, usw.\nDer SqlExecutor-Task muss neben Updates ganzer Tabellen (d.h. Löschen des gesamten Inhalts einer Tabelle und gesamter neuer Stand in die Tabelle schreiben) auch Updates von Teilen von Tabellen zulassen. D.h. es muss z.B. möglich sein, innerhalb einer Tabelle nur die Objekte einer bestimmten Gemeinde zu aktualisieren. Darum ist es möglich innerhalb der .sql-Datei Paramater zu verwenden und diesen Parametern beim Task einen konkreten Wert zuzuweisen. Innerhalb der .sql-Datei werden Paramter mit folgender Syntax verwendet: ${paramName}.\nUnterstützte Datenbanken: PostgreSQL, SQLite, Oracle, Derby und DuckDB.\nBeispiele:\ndef db_uri = 'jdbc:postgresql://localhost:54321/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('executeSomeSql', SqlExecutor) {\n    database = [db_uri, db_user, db_pass]\n    sqlParameters = [dataset:'Olten']\n    sqlFiles = ['demo.sql']\n}\nDamit mit einer einzigen Task-Definition mehrere Datensätze verarbeitet werden können, kann auch eine Liste von Parametern angegeben werden.\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('executeSomeSql', SqlExecutor) {\n    database = [db_uri, db_user, db_pass]\n    sqlParameters = [[dataset:'Olten'],[dataset:'Grenchen']]\n    sqlFiles = ['demo.sql']\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\ndatabase\nListProperty&lt;String&gt;\nDatenbank, in die importiert werden soll.\n nein\n\n\nsqlFiles\nListProperty&lt;String&gt;\nName der SQL-Datei aus der SQL-Statements gelesen und ausgeführt werden.\n nein\n\n\nsqlParameters\nProperty&lt;Object&gt;\nEine Map mit Paaren von Parameter-Name und Parameter-Wert (Map&lt;String,String&gt;). Oder eine Liste mit Paaren von Parameter-Name und Parameter-Wert (List&lt;Map&lt;String,String&gt;&gt;).\n ja\n\n\n\n\n\nXslTransformer (incubating)\nTransformiert eine Datei mittels einer XSL-Transformation ein eine andere Datei. Ist der xslFile-Parameter ein String, wird erwartet, dass die Datei im GRETL-Verzeichnis im Ressourcenordner src/main/resources/xslt-Verzeichnis gespeichert ist. Falls der xslFile-Parameter ein File-Objekt ist, können lokale Dateien verwendet werden.\nBeispiele:\ntasks.register('transform', XslTransformer) {\n    xslFile = \"eCH0132_to_SO_AGI_SGV_Meldungen_20221109.xsl\"\n    xmlFile = file(\"MeldungAnGeometer_G-0111102_20221103_145001.xml\")\n    outDirectory = file(\".\")\n}\ntasks.register('transform', XslTransformer) {\n    xslFile = file(\"path/to/eCH0132_to_SO_AGI_SGV_Meldungen_20221109.xsl\")\n    xmlFile = file(\"MeldungAnGeometer_G-0111102_20221103_145001.xml\")\n    outDirectory = file(\".\")\n}\ntasks.register('transform', XslTransformer) {\n    xslFile = \"eCH0132_to_SO_AGI_SGV_Meldungen_20221109.xsl\"\n    xmlFile = fileTree(\".\").matching {\n        include \"*.xml\"\n    }\n    outDirectory = file(\".\")\n}\n\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\nOptional\n\n\n\n\nfileExtension\nString\nFileextension der Resultatdatei. Default: xtf\n ja\n\n\noutDirectory\nFile\nVerzeichnis, in das die transformierte Datei gespeichert wird. Der Name der transformierten Datei entspricht standardmässig dem Namen der Input-Datei mit Endung .xtf.\n nein\n\n\nxmlFile\nObject\nDatei oder FileTree, die/der transformiert werden soll.\n nein\n\n\nxslFile\nObject\nName der XSLT-Datei, die im src/main/resources/xslt-Verzeichnis liegen muss oder File-Objekt (beliebiger Pfad).\n nein"
  }
]