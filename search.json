[
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Help",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "help.html#gaga",
    "href": "help.html#gaga",
    "title": "Help",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "help.html#bubu",
    "href": "help.html#bubu",
    "title": "Help",
    "section": "bubu",
    "text": "bubu\nLorem Ipsum…."
  },
  {
    "objectID": "development.html",
    "href": "development.html",
    "title": "Development",
    "section": "",
    "text": "tbd"
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "Das Datenmanagement-Tool GRETL ist ein Werkzeug, das für Datenimports, Datenumbauten (Modellumbau) und Datenexports eingesetzt wird. GRETL führt Jobs ausy, wobei ein Job aus mehreren atomaren Tasks besteht. Damit ein Job als vollständig ausgeführt gilt, muss jeder zum Job gehörende Task vollständig ausgeführt worden sein. Schlägt ein Task fehl, gilt auch der Job als fehlgeschlagen.\nEin Job besteht aus einem oder mehreren Tasks, die gemäss einem gerichteten Graphen (Directed Acyclic Graph; DAG) miteinander verknüpft sind.\nEin Job kann aus z.B. aus einer linearen Kette von Tasks bestehen:\nBeispiel: Datenimport aus INTERLIS-Datei – Datenumbau – Datenexport nach Shapefile.\nEin Job kann sich nach einem Task aber auch auf zwei oder mehr verschiedene weitere Tasks verzweigen:\nBeispiel: Datenimport aus INTERLIS-Datei – Datenumbau in Zielschema 1 und ein zweiter Datenumbau in Zielschema 2.\nEs ist auch möglich, dass zuerst zwei oder mehr Tasks unabhängig voneinander ausgeführt werden müssen, bevor ein einzelner weiterer Task ausgeführt wird.\nDie Tasks eines Jobs werden per Konfigurationsfile konfiguriert."
  },
  {
    "objectID": "documentation.html#systemanforderungen",
    "href": "documentation.html#systemanforderungen",
    "title": "Documentation",
    "section": "Systemanforderungen",
    "text": "Systemanforderungen\nUm die aktuelle Version von GRETL auszuführen, muss\n\ndie Java-Laufzeitumgebung (JRE), Version 1.8 oder neuer, und\nGradle, Version 5.1 oder neuer, auf dem System installiert sein.\n\nDie Java-Laufzeitumgebung (JRE) kann z.B. hier gratis bezogen werden.\nDie Gradle-Software kann auf der Website https://gradle.org/ gratis bezogen werden."
  },
  {
    "objectID": "documentation.html#installation",
    "href": "documentation.html#installation",
    "title": "Documentation",
    "section": "Installation",
    "text": "Installation\nGRETL selbst muss als Gradle-Plugin nicht explizit installiert werden, sondern wird dynamisch durch das Internet bezogen.\nUm gretl auszuführen, geben Sie auf der Kommandozeile folgendes Kommando ein (wobei jobfolder der absolute Pfad zu ihrem Verzeichnis mit der Job Konfiguration ist.)\ngradle --project-dir jobfolder\nAlternativ können Sie auch ins Verzeichnis mit der Job Konfiguration wechseln, und da das Kommando gradle ohne Argument verwenden:\ncd jobfolder\ngradle"
  },
  {
    "objectID": "documentation.html#tasks",
    "href": "documentation.html#tasks",
    "title": "Documentation",
    "section": "Tasks",
    "text": "Tasks\n\nAv2ch\nTransformiert eine INTERLIS1-Transferdatei im kantonalen AV-DM01-Modell in das Bundesmodell. Unterstützt werden die Sprachen Deutsch und Italienisch und der Bezugrahmen LV95. Getestet mit Daten aus den Kantonen Solothurn, Glarus und Tessin. Weitere Informationen sind in der Basisbibliothek zu finden: https://github.com/sogis/av2ch.\nDas Bundes-ITF hat denselben Namen wie das Kantons-ITF.\nAufgrund der sehr vielen Logging-Messages einer verwendeten Bibliothek, wird der System.err-Ouput nach dev/null gemappt .\ntask transform(type: Av2ch) {\n    inputFile = file(\"254900.itf\")\n    outputDirectory = file(\"output\")\n}\n\n\n\n\n\n\n\n\nParameter\nDatentyp\nBeschreibung\n\n\n\n\ninputFile\nFile\nName der zu transformierenden ITF-Datei.\n\n\noutputDirectory\nFile\nName des Verzeichnisses in das die zu erstellende Datei geschrieben wird.\n\n\nmodeldir\nString\nINTERLIS-Modellablage. String separiert mit Semikolon (analog ili2db, ilivalidator).\n\n\nzip\nBoolean\nDie zu erstellende Datei wird gezippt (Default: false).\n\n\n\n\n\nAv2geobau\nAv2geobau konvertiert eine Interlis-Transferdatei (itf) in eine DXF-Geobau Datei. Av2geobau funktioniert ohne Datenbank.\nDie ITF-Datei muss dem Modell DM01AVCH24LV95D entsprechen. Die Daten werden nicht validiert.\nDie Datenstruktur der DXF-Datei ist im Prinzip sehr einfach: Die verschiedenen Informationen aus dem Datenmodell DM01 werden in verschiedene DXF-Layer abgebildet, z.B. die begehbaren LFP1 werden in den Layer “01111” abgebildet. Oder die Gebäude in den Layer “01211”.\nDer Datenumbau ist nicht konfigurierbar.\ntask av2geobau(type: Av2geobau){\n    itfFiles = \"ch_254900.itf\"\n    dxfDirectory = \"./out/\"\n}\nEs können auch mehrere Dateien angegeben werden.\ntask av2geobau(type: Av2geobau){\n    itfFiles = fileTree(\".\").matching {\n        include\"*.itf\"\n    }\n    dxfDirectory = \"./out/\"\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\nitfFiles\nITF-Datei, die nach DXF transformiert werden soll. Es können auch mehrere Dateien angegeben werden.\n\n\ndxfDirectory\nVerzeichnis, in das die DXF-Dateien gespeichert werden.\n\n\nmodeldir\nDateipfade, die Modell-Dateien (ili-Dateien) enthalten. Mehrere Pfade können durch Semikolon ‚;‘ getrennt werden. Es sind auch URLs von Modell-Repositories möglich. Default: %ITF_DIR;http://models.interlis.ch/. %ITF_DIR ist ein Platzhalter für das Verzeichnis mit der ITF-Datei.\n\n\nlogFile\nSchreibt die log-Meldungen der Konvertierung in eine Text-Datei.\n\n\nproxy\nProxy Server für den Zugriff auf Modell Repositories\n\n\nproxyPort\nProxy Port für den Zugriff auf Modell Repositories\n\n\nzip\nDie zu erstellende Datei wird gezippt und es werden zusätzliche Dateien (Musterplan, Layerbeschreibung, Hinweise) hinzugefügt (Default: false).\n\n\n\n\n\nCsv2Excel (incubating)\nKonvertiert eine CSV-Datei in eine Excel-Datei (*.xlsx). Datentypen werden anhand eines INTERLIS-Modelles eruiert. Fehlt das Modell, wird alles als Text gespeichert. Die Daten werden vollständig im Speicher vorgehalten. Falls grosse Dateien geschrieben werden müssen, kann das zu Problemen führen. Dann müsste die Apache POI SXSSF Implementierung (Streaming) verwendet werden.\nBeispiel:\ntask convertData(type: Csv2Excel) {\n    csvFile = file(\"./20230124_sap_Gebaeude.csv\")\n    firstLineIsHeader = true\n    valueDelimiter = null\n    valueSeparator = \";\"\n    encoding = \"ISO-8859-1\";\n    models = \"SO_HBA_Gebaeude_20230111\";\n    outputDir = file(\".\");\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ncsvFile\nCSV-Datei, die konvertiert werden soll.\n\n\nfirstLineIsHeader\nDefiniert, ob eine Headerzeile geschrieben werden soll, oder nicht. Default: true\n\n\nvalueDelimiter\nZeichen, das am Anfang und Ende jeden Wertes geschrieben werden soll. Default \"\n\n\nvalueSeparator\nZeichen, das als Trennzeichen zwischen den Werten verwendet werden soll. Default: ,\n\n\nencoding\nZeichencodierung der CSV-Datei, z.B. \"UTF-8\". Default: Systemeinstellung\n\n\nmodels\nINTERLIS-Modell für Definition der Datentypen in der Excel-Datei.\n\n\nmodeldir\nDateipfade, die Modell-Dateien (ili-Dateien) enthalten. Mehrere Pfade können durch Semikolon “;” getrennt werden. Es sind auch URLs von Modell-Repositories möglich.\n\n\noutputDir\nVerzeichnis, in das die Excel-Datei gespeichert wird. Default: Verzeichnis, in dem die CSV-Datei vorliegt.\n\n\n\n\n\nCsv2Parquet (incubating)\nKonvertiert eine CSV-Datei in eine Parquet-Datei. Datentypen werden anhand eines INTERLIS-Modelles eruiert. Fehlt das Modell, wird alles als Text gespeichert.\nBeispiel:\ntask convertData(type: Csv2Parquet) {\n    csvFile = file(\"./20230124_sap_Gebaeude.csv\")\n    firstLineIsHeader = true\n    valueDelimiter = null\n    valueSeparator = \";\"\n    encoding = \"ISO-8859-1\";\n    models = \"SO_HBA_Gebaeude_20230111\";\n    outputDir = file(\".\");\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ncsvFile\nCSV-Datei, die konvertiert werden soll.\n\n\nfirstLineIsHeader\nDefiniert, ob eine Headerzeile geschrieben werden soll, oder nicht. Default: true\n\n\nvalueDelimiter\nZeichen, das am Anfang und Ende jeden Wertes geschrieben werden soll. Default \"\n\n\nvalueSeparator\nZeichen, das als Trennzeichen zwischen den Werten verwendet werden soll. Default: ,\n\n\nencoding\nZeichencodierung der CSV-Datei, z.B. \"UTF-8\". Default: Systemeinstellung\n\n\nmodels\nINTERLIS-Modell für Definition der Datentypen in der Parquet-Datei.\n\n\nmodeldir\nDateipfade, die Modell-Dateien (ili-Dateien) enthalten. Mehrere Pfade können durch Semikolon “;” getrennt werden. Es sind auch URLs von Modell-Repositories möglich.\n\n\noutputDir\nVerzeichnis, in das die Parquet-Datei gespeichert wird. Default: Verzeichnis, in dem die CSV-Datei vorliegt.\n\n\n\n\n\nCsvExport\nDaten aus einer bestehenden Datenbanktabelle werden in eine CSV-Datei exportiert.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask csvexport(type: CsvExport){\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"csvexport\"\n    tableName = \"exportdata\"\n    firstLineIsHeader=true\n    attributes = [ \"t_id\",\"Aint\"]\n    dataFile = \"data.csv\"\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndatabase\nDatenbank aus der exportiert werden soll.\n\n\ndataFile\nName der CSV-Datei, die erstellt werden soll.\n\n\ntableName\nName der DB-Tabelle, die exportiert werden soll\n\n\nschemaName\nName des DB-Schemas, in dem die DB-Tabelle ist.\n\n\nfirstLineIsHeader\nDefiniert, ob eine Headerzeile geschrieben werden soll, oder nicht. Default: true\n\n\nvalueDelimiter\nZeichen, das am Anfang und Ende jeden Wertes geschrieben werden soll. Default \"\n\n\nvalueSeparator\nZeichen, das als Trennzeichen zwischen den Werten verwendet werden soll. Default: ,\n\n\nattributes\nSpalten der DB-Tabelle, die exportiert werden sollen. Definiert die Reihenfolge der Spalten in der CSV-Datei. Default: alle Spalten\n\n\nencoding\nZeichencodierung der CSV-Datei, z.B. \"UTF-8\". Default: Systemeinstellung\n\n\n\nGeometriespalten können nicht exportiert werden.\n\n\nCsvImport\nDaten aus einer CSV-Datei werden in eine bestehende Datenbanktabelle importiert.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask csvimport(type: CsvImport){\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"csvimport\"\n    tableName = \"importdata\"\n    firstLineIsHeader=true\n    dataFile = \"data1.csv\"\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndatabase\nDatenbank in die importiert werden soll\n\n\ndataFile\nName der CSV-Datei, die gelesen werden soll\n\n\ntableName\nName der DB-Tabelle, in die importiert werden soll\n\n\nschemaName\nName des DB-Schemas, in dem die DB-Tabelle ist.\n\n\nfirstLineIsHeader\nDefiniert, ob die CSV-Datei einer Headerzeile hat, oder nicht. Default: true\n\n\nvalueDelimiter\nZeichen, das am Anfang und Ende jeden Wertes vorhanden ist. Default \"\n\n\nvalueSeparator\nZeichen, das als Trennzeichen zwischen den Werten interpretiert werden soll. Default: ,\n\n\nencoding\nZeichencodierung der CSV-Datei, z.B. \"UTF-8\". Default: Systemeinstellung\n\n\nbatchSize\nAnzahl der Records, die pro Batch in die Ziel-Datenbank geschrieben werden (Standard: 5000).\n\n\n\nDie Tabelle kann weitere Spalten enthalten, die in der CSV-Datei nicht vorkommen. Sie müssen aber NULLable sein, oder einen Default-Wert definiert haben.\nGeometriepalten können nicht importiert werden.\nDie Gross-/Kleinschreibung der CSV-Spaltennamen wird für die Zuordnung zu den DB-Spalten ignoriert.\n\n\nCsvValidator\nPrüft eine CSV-Datei gegenüber einem INTERLIS-Modell. Basiert auf dem ilivalidator. Das Datenmodell darf die OID nicht als UUID modellieren (OID AS INTERLIS.UUIDOID).\nBeispiel:\ntask validate(type: CsvValidator){\n    models = \"CsvModel\"\n    firstLineIsHeader=true\n    dataFiles = [\"data1.csv\"]\n}\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndataFiles\nListe der CSV-Dateien, die validiert werden sollen. Eine leere Liste ist kein Fehler.\n\n\nmodels\nINTERLIS-Modell, gegen das die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Der Name der CSV-Datei.\n\n\nmodeldir\nDateipfade, die Modell-Dateien (ili-Dateien) enthalten. Mehrere Pfade können durch Semikolon ‚;‘ getrennt werden. Es sind auch URLs von Modell-Repositories möglich. Default: %XTF_DIR;http://models.interlis.ch/. %XTF_DIR ist ein Platzhalter für das Verzeichnis mit der CSV-Datei.\n\n\nconfigFile\nKonfiguriert die Datenprüfung mit Hilfe einer TOML-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n\n\nforceTypeValidation\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n\n\ndisableAreaValidation\nSchaltet die AREA-Topologieprüfung aus. Default: false\n\n\nmultiplicityOff\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n\n\nallObjectsAccessible\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n\n\nskipPolygonBuilding\nSchaltet die Bildung der Polygone aus (nur ITF). Default: false\n\n\nlogFile\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n\n\nxtflogFile\nSchreibt die log-Meldungen in eine INTERLIS 2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n\n\npluginFolder\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n\n\nproxy\nProxy Server für den Zugriff auf Modell Repositories\n\n\nproxyPort\nProxy Port für den Zugriff auf Modell Repositories\n\n\nfailOnError\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n\n\nvalidationOk\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false\n\n\nfirstLineIsHeader\nDefiniert, ob die CSV-Datei einer Headerzeile hat, oder nicht. Default: true\n\n\nvalueDelimiter\nZeichen, das am Anfang und Ende jeden Wertes vorhanden ist. Default \"\n\n\nvalueSeparator\nZeichen, das als Trennzeichen zwischen den Werten interpretiert werden soll. Default: ,\n\n\nencoding\nZeichencodierung der CSV-Datei, z.B. \"UTF-8\". Default: Systemeinstellung\n\n\n\nFalls die CSV-Datei eine Header-Zeile enthält (mit den Spaltennamen), wird im gegebenen Modell eine Klasse gesucht, welche die Header-Spaltennamen enthält (Gross-/Kleinschreibung sowie optionale “Spalten” der Modell-Klasse werden ignoriert). Wird keine solche Klasse gefunden, gilt das als Validierungsfehler.\nFalls die CSV-Datei keine Header-Zeile enthält (mit den Spaltennamen), wird im gegebenen Modell eine Klasse gesucht, die die selbe Anzahl Attribute hat. Wird keine solche Klasse gefunden, gilt das als Validierungsfehler.\nDie Prüfung von gleichzeitig mehreren CSV-Dateien führt zu Fehlermeldungen wie OID o3158 of object &lt;Modelname&gt;.&lt;Topicname&gt;.&lt;Klassenname&gt; already exists in .... Beim Öffnen und Lesen einer CSV-Datei wird immer der Zähler, der die interne (in der CSV-Datei nicht vorhandene) OID generiert, zurückgesetzt. Somit kann immer nur eine CSV-Datei pro Task geprüft werden.\n\n\nCurl\nSimuliert mit einem HttpClient einige Curl-Befehle.\nBeispiele:\nimport ch.so.agi.gretl.tasks.Curl.MethodType;\n\ntask uploadData(type: Curl) {\n    serverUrl = \"https://geodienste.ch/data_agg/interlis/import\"\n    method = MethodType.POST\n    formData = [\"topic\": \"npl_waldgrenzen\", \"lv95_file\": file(\"./test.xtf.zip\"), \"publish\": \"true\", \"replace_all\": \"true\"]\n    user = \"fooUser\"\n    password = \"barPwd\"\n    expectedStatusCode = 200\n    expectedBody = \"\\\"success\\\":true\"\n}\nimport ch.so.agi.gretl.tasks.Curl.MethodType;\n\ntask uploadData(type: Curl) {\n    serverUrl = \"https://testweb.so.ch/typo3/api/digiplan\"\n    method = MethodType.POST\n    headers = [\"Content-Type\": \"application/xml\", \"Content-Encoding\": \"gzip\"]\n    dataBinary = file(\"./planregister.xml.gz\")\n    user = \"fooUser\"\n    password = \"barPwd\"\n    expectedStatusCode = 202\n}\nimport ch.so.agi.gretl.tasks.Curl.MethodType;\n\ntask downloadData(type: Curl) {\n    serverUrl = \"https://raw.githubusercontent.com/sogis/gretl/master/README.md\"\n    method = MethodType.GET\n    outputFile = file(\"./README.md\")\n    expectedStatusCode = 200\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\nserverUrl\nDie URL des Servers inklusive Pfad und Queryparameter.\n\n\nmethod\nHTTP-Request-Methode. Unterstützt werden GET und POST.\n\n\nexpectedStatusCode\nErwarteter Status Code, der vom Server zurückgeliefert wird.\n\n\nexpectedBody\nErwarteter Text, der vom Server als Body zurückgelieferd wird. (optional)\n\n\nformData\nForm data parameters. Entspricht curl [URL] -F key1=value1 -F file1=@my_file.xtf. (optional)\n\n\ndataBinary\nDatei, die hochgeladen werden soll. Entspricht curl [URL] --data-binary. (optional)\n\n\nheaders\nRequest-Header. Entspricht curl [URL] -H ... -H .... (optional)\n\n\nuser\nBenutzername. Wird zusammen mit password in einen Authorization-Header umgewandelt. Entspricht curl [URL] -u user:password. (optional)\n\n\npassword\nPasswort. Wird zusammen mit user in einen Authorization-Header umgewandelt. Entspricht curl [URL] -u user:password. (optional)\n\n\noutputFile\nDatei, in die der Output gespeichert wird. Entspricht curl [URL] -o. (optional)\n\n\n\n\n\nDatabaseDocumentExport (Experimental)\nSpeichert Dokumente, deren URL in einer Spalte einer Datenbanktabelle gespeichert sind, in einem lokalen Verzeichnis. Zukünftig und bei Bedarf kann der Task so erweitert werden, dass auch BLOBs aus der Datenbank gespeichert werden können.\nRedirect von HTTP nach HTTPS funktionieren nicht. Dies korrekterweise (?) wegen der verwendeten Java-Bibliothek.\nWegen der vom Kanton Solothurn eingesetzten self-signed Zertifikate muss ein unschöner Handstand gemacht werden. Leider kann dieser Usecase schlecht getestet werden, da die Links nur in der privaten Zone verfügbar sind und die zudem noch häufig ändern können. Manuell getestet wurde es jedoch.\nAls Dateiname wird der letzte Teil des URL-Pfades verwendet, z.B. https://artplus.verw.rootso.org/MpWeb-apSolothurnDenkmal/download/2W8v0qRZQBC0ahDnZGut3Q?mode=gis wird mit den Prefix und Extension zu ada_2W8v0qRZQBC0ahDnZGut3Q.pdf.\nEs wird DISTINCT ON (&lt;documentColumn&gt;) und ein Filter WHERE &lt;documentColumn&gt; IS NOT NULL verwendet.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask exportDocuments(type: DatabaseDocumentExport){\n    database = [db_uri, db_user, db_pass]\n    qualifiedTableName = \"ada_denkmalschutz.fachapplikation_rechtsvorschrift_link\"\n    documentColumn = \"multimedia_link\"\n    targetDir = file(\".\")\n    fileNamePrefix = \"ada_\"\n    fileNameExtension = \"pdf\"\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndatabase\nDatenbank aus der die Dokumente exportiert werden sollen.\n\n\nqualifiedTableName\nQualifizierter Tabellenname\n\n\ndocumentColumn\nDB-Tabellenspalte mit dem Dokument resp. der URL zum Dokument.\n\n\ntargetDir\nVerzeichnis in das die Dokumente exportiert werden sollen.\n\n\nfileNamePrefix\nPrefix für Dateinamen (optional)\n\n\nfileNameExtension\nDateinamen-Extension (optional)\n\n\n\n\n\nDb2Db\nDies ist prinzipiell ein 1:1-Datenkopie, d.h. es findet kein Datenumbau statt, die Quell- und die Ziel-Tabelle hat jeweils identische Attribute. Es werden auf Seite Quelle in der Regel also simple SELECT-Queries ausgeführt und die Resultate dieser Queries in Tabellen der Ziel-DB eingefügt. Unter bestimmten Bedingungen (insbesondere wenn es sich um einen wenig komplexen Datenumbau handelt), kann dieser Task aber auch zum Datenumbau benutzt werden.\nDie Queries können auf mehrere .sql-Dateien verteilt werden, d.h. der Task muss die Queries mehrerer .sql-Dateien zu einer Transaktion kombinieren können. Jede .sql-Datei gibt genau eine Resultset (RAM-Tabelle) zurück. Das Resultset wird in die konfigurierte Zieltabelle geschrieben. Die Beziehungen sind: Eine bis mehrere Quelltabellen ergeben ein Resultset; das Resultset entspricht bezüglich den Attributen genau der Zieltabelle und wird 1:1 in diese geschrieben. Der Db2Db-Task verarbeitet innerhalb einer Transaktion 1-n Resultsets und wird entsprechend auch mit 1-n SQL-Dateien konfiguriert.\nDie Reihenfolge der .sql-Dateien ist relevant. Dies bedeutet, dass die SQL-Befehle der zuerst angegebenen .sql-Datei zuerst ausgeführt werden müssen, danach die SQL-Befehle der an zweiter Stelle angegebenen .sql-Datei, usw.\nEs ist auch möglich, in den .sql-Dateien mehr als nur ein SELECT-Query zu formulieren, z.B. ein vorgängiges DELETE.\nAlle SELECT-Statements werden in einer Transaktion ausgeführt werden, damit ein konsistenter Datenstand gelesen wird. Alle INSERT-Statements werden in einer Transaktion ausgeführt werden, damit bei einem Fehler der bisherige Datenstand bestehen bleibt und also kein unvollständiger Import zurückgelassen wird.\nDamit dieselbe .sql-Datei für verschiedene Datensätze benutzt werden kann, ist es möglich innerhalb der .sql-Datei Parameter zu verwenden und diesen Parametern beim Task einen konkreten Wert zuzuweisen. Innerhalb der .sql-Datei werden Paramter mit folgender Syntax verwendet: ${paramName}.\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask transferSomeData(type: Db2Db) {\n    sourceDb = [db_uri, db_user, db_pass]\n    targetDb = ['jdbc:sqlite:gretldemo.sqlite',null,null]\n    sqlParameters = [dataset:'Olten']\n    transferSets = [\n        new TransferSet('some.sql', 'albums_dest', true)\n    ];\n}\nDamit mit einer einzigen Task-Definition mehrere Datensätze verarbeitet werden können, kann auch eine Liste von Parametern angegeben werden.\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask transferSomeData(type: Db2Db) {\n    sourceDb = [db_uri, db_user, db_pass]\n    targetDb = ['jdbc:sqlite:gretldemo.sqlite',null,null]\n    sqlParameters = [[dataset:'Olten'],[dataset:'Grenchen']]\n    transferSets = [\n        new TransferSet('some.sql', 'albums_dest', true)\n    ];\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\nsourceDb\nDatenbank aus der gelesen werden soll\n\n\ntargetDb\nDatenbank in die geschrieben werden soll\n\n\ntransferSets\nEine Liste von TransferSets.\n\n\nsqlParameters\nEine Map mit Paaren von Parameter-Name und Parameter-Wert (Map&lt;String,String&gt;). Oder eine Liste mit Paaren von Parameter-Name und Parameter-Wert (List&lt;Map&lt;String,String&gt;&gt;).\n\n\nbatchSize\nAnzahl der Records, die pro Batch in die Ziel-Datenbank geschrieben werden (Standard: 5000). Für sehr grosse Tabellen muss ein kleinerer Wert gewählt werden.\n\n\nfetchSize\nAnzahl der Records, die auf einmal vom Datenbank-Cursor von der Quell-Datenbank zurückgeliefert werden (Standard: 5000). Für sehr grosse Tabellen muss ein kleinerer Wert gewählt werden.\n\n\n\nEine TransferSet ist\n\neine SQL-Datei (mit SQL-Anweisungen zum Lesen der Daten aus der sourceDb),\ndem Namen der Ziel-Tabelle in der targetDb, und\nder Angabe ob in der Ziel-Tabelle vor dem INSERT zuerst alle Records gelöscht werden sollen.\neinem optionalen vierten Parameter, der verwendet werden kann um den zu erzeugenden SQL-Insert-String zu beeinflussen, u.a. um einen WKT-Geometrie-String in eine PostGIS-Geometrie umzuwandeln\n\nBeispiel, Umwandlung Rechtswert/Hochwertspalten in eine PostGIS-Geometrie (siehe auch Gretl-Job afu_onlinerisk_transfer der eine Punktgeometriespalten aus einer Nicht-Postgis-DB übernimmt):\nnew TransferSet('untersuchungseinheit.sql', 'afu_qrcat_v1.onlinerisk_untersuchungseinheit', true, (String[])[\"geom:wkt:2056\"])\n“geom” ist der Geometrie-Spalten-Name der verwendet wird.\nDazugehöriger Auszug aus SQL-Datei zur Erzeugung des WKT-Strings mit Hilfe von concatenation:\n'Point(' || ue.koordinate_x::text || ' ' || ue.koordinate_y::text || ')' AS geom\nUnterstützte Datenbanken: PostgreSQL, SQLite und Oracle. Der Oracle-JDBC-Treiber muss jedoch selber installiert werden (Ausgenommen vom Docker-Image).\n\n\nFtpDelete\nLöscht Daten auf einem FTP-Server.\nBeispiel, löscht alle Daten in einem Verzeichnis:\ntask ftpdelete(type: FtpDelete) {\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir = \"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile = fileTree(pathToTempFolder) { include '*.zip' } \n    //remoteFile = \"*.zip\"\n}\nUm bestimmte Dateien zu löschen:\ntask ftpdownload(type: FtpDownload){\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir = \"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile = \"*.zip\"\n}\nUm heruntergeladene Daten zu löschen:\ntask ftpdownload(type: FtpDownload){\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir = \"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile = fileTree(pathToDownloadFolder) { include '*.zip' } \n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\nserver\nName des Servers (ohne ftp://)\n\n\nuser\nBenutzername auf dem Server\n\n\npassword\nPasswort für den Zugriff auf dem Server\n\n\nremoteDir\nVerzeichnis auf dem Server\n\n\nremoteFile\nDateiname oder Liste der Dateinamen auf dem Server (kann auch ein Muster sein (* oder ?)). Ohne diesen Parameter werden alle Dateien aus dem Remoteverzeichnis gelöscht.\n\n\nsystemType\nUNIX oder WINDOWS. Default ist UNIX.\n\n\nfileSeparator\nDefault ist ‘/’. (Falls systemType Windows ist, ist der Default ‘\\’.\n\n\npassiveMode\nAktiv oder Passiv Verbindungsmodus. Default ist Passiv (true)\n\n\ncontrolKeepAliveTimeout\nTimeout bis ein NOOP über den Kontroll-Kanal versendet wird. Default ist 300s (=5 Minuten)\n\n\n\n\n\nFtpDownload\nLädt alle Dateien aus dem definierten Verzeichnis des Servers in ein lokales Verzeichnis herunter.\nBeispiel:\ntask ftpdownload(type: FtpDownload){\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    localDir= \"downloads\"\n    remoteDir= \"\"\n}\nUm eine bestimmte Datei herunterzuladen:\ntask ftpdownload(type: FtpDownload){\n    server=\"ftp.infogrips.ch\"\n    user= \"Hans\"\n    password= \"dummy\"\n    systemType=\"WINDOWS\"\n    localDir= \"downloads\"\n    remoteDir=\"\\\\dm01avso24lv95\\\\itf\"\n    remoteFile=\"240100.zip\"\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\nserver\nName des Servers (ohne ftp://)\n\n\nuser\nBenutzername auf dem Server\n\n\npassword\nPasswort für den Zugriff auf dem Server\n\n\nlocalDir\nLokales Verzeichnis indem die Dateien gespeichert werden\n\n\nremoteDir\nVerzeichnis auf dem Server\n\n\nremoteFile\nDateiname oder Liste der Dateinamen auf dem Server (kann auch ein Muster sein (* oder ?)). Ohne diesen Parameter werden alle Dateien aus dem Remoteverzeichnis heruntergeladen.\n\n\nsystemType\nUNIX oder WINDOWS. Default ist UNIX.\n\n\nfileType\nASCII oder BINARY. Default ist ASCII.\n\n\nfileSeparator\nDefault ist ‘/’. (Falls systemType Windows ist, ist der Default ‘\\’.\n\n\npassiveMode\nAktiv oder Passiv Verbindungsmodus. Default ist Passiv (true)\n\n\ncontrolKeepAliveTimeout\nTimeout bis ein NOOP über den Kontroll-Kanal versendet wird. Default ist 300s (=5 Minuten)\n\n\n\n\n\nFtpList\nLiefert eine Liste der Dateien aus dem definierten Verzeichnis des Servers.\nBeispiel:\ntask ftplist(type: FtpList){\n    server= \"ftp.server.org\"\n    user= \"Hans\"\n    password= \"dummy\"\n    remoteDir= \"\"\n    doLast {\n        println files\n    }\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\nserver\nName des Servers (ohne ftp://)\n\n\nuser\nBenutzername auf dem Server\n\n\npassword\nPasswort für den Zugriff auf dem Server\n\n\nremoteDir\nVerzeichnis auf dem Server\n\n\nfiles\nListe der Dateinamen auf dem Server\n\n\nsystemType\nUNIX oder WINDOWS. Default ist UNIX.\n\n\nfileSeparator\nDefault ist ‘/’. (Falls systemType Windows ist, ist der Default ‘\\’.\n\n\npassiveMode\nAktiv oder Passiv Verbindungsmodus. Default ist Passiv (true)\n\n\ncontrolKeepAliveTimeout\nTimeout bis ein NOOP über den Kontroll-Kanal versendet wird. Default ist 300s (=5 Minuten)\n\n\n\n\n\nGpkg2Dxf\nExportiert alle Tabellen einer GeoPackage-Datei in DXF-Dateien. Als Input wird eine von ili2gpkg erzeugte GeoPackage-Datei benötigt.\nEs werden alle INTERLIS-Klassen exportier (SELECT tablename FROM T_ILI2DB_TABLE_PROP WHERE setting = 'CLASS'). Der eigentliche SELECT-Befehl ist komplizierter weil für das Layern der einzelnen DXF-Objekte das INTERLIS-Metaattribut !!@dxflayer=\"true\" ausgelesen wird. Gibt es kein solches Metaattribut wird alles in den gleichen DXF-Layer (default) geschrieben.\nEncoding: Die DXF-Dateien sind ISO-8859-1 encodiert.\nAchtung: Task sollte verbessert werden (siehe E-Mail Claude im Rahmen des Publisher-Projektes).\nBeispiel:\ntask gpkg2dxf(type: Gpkg2Dxf) {\n    dataFile = file(\"data.gpkg\")\n    outputDir = file(\"./out/\")\n}\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndataFile\nGeoPackage-Datei, die nach DXF transformiert werden soll.\n\n\noutputDir\nVerzeichnis, in das die DXF-Dateien gespeichert werden.\n\n\n\n\n\nGpkgExport\nDaten aus einer bestehenden Datenbanktabelle werden in eine GeoPackage-Datei exportiert.\nBeispiele:\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask gpkgexport(type: GpkgExport){\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"gpkgexport\"\n    srcTableName = \"exportdata\"\n    dataFile = \"data.gpkg\"\n    dstTableName = \"exportdata\"\n}\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask gpkgexport(type: GpkgExport) {\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"gpkgexport\"\n    srcTableName = [\"exportTable1\", \"exportTable2\"]\n    dataFile = \"data.gpkg\"\n    dstTableName = [\"exportTable1\", \"exportTable2\"]\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndatabase\nDatenbank aus der exportiert werden soll\n\n\ndataFile\nName der GeoPackage-Datei, die erstellt werden soll\n\n\nsrcTableName\nName der DB-Tabelle(n), die exportiert werden soll(en). String oder List.\n\n\nschemaName\nName des DB-Schemas, in dem die DB-Tabelle ist.\n\n\ndstTableName\nName der Tabelle(n) in der GeoPackage-Datei. String oder List.\n\n\nbatchSize\nAnzahl der Records, die pro Batch in die Ziel-Datenbank (GeoPackage) geschrieben werden (Standard: 5000).\n\n\nfetchSize\nAnzahl der Records, die pro Fetch aus der Quell-Datenbank gelesen werden (Standard: 5000).\n\n\n\n\n\nGpkgImport\nDaten aus einer GeoPackage-Datei in eine bestehende Datenbanktabelle importieren.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask gpkgimport(type: GpkgImport){\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"gpkgimport\"\n    srcTableName = \"Point\"\n    dstTableName = \"importdata\"\n    dataFile = \"point.gpkg\"\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndatabase\nDatenbank in die importiert werden soll\n\n\ndataFile\nName der GeoPackage-Datei, die gelesen werden soll\n\n\nsrcTableName\nName der GeoPackage-Tabelle, die importiert werden soll\n\n\nschemaName\nName des DB-Schemas, in dem die DB-Tabelle ist.\n\n\ndstTableName\nName der DB-Tabelle, in die importiert werden soll\n\n\nencoding\nZeichencodierung der SHP-Datei, z.B. \"UTF-8\". Default: Systemeinstellung\n\n\nbatchSize\nAnzahl der Records, die pro Batch in die Ziel-Datenbank geschrieben werden (Standard: 5000).\n\n\nfetchSize\nAnzahl der Records, die pro Fetch aus der Quell-Datenbank gelesen werden (Standard: 5000).\n\n\n\nDie Tabelle kann weitere Spalten enthalten, die in der GeoPackage-Datei nicht vorkommen. Sie müssen aber NULLable sein, oder einen Default-Wert definiert haben.\nDie Gross-/Kleinschreibung der GeoPckage-Spaltennamen wird für die Zuordnung zu den DB-Spalten ignoriert.\n\n\nGpkg2Shp\nExportiert alle Tabellen einer GeoPackage-Datei in Shapefiles. Als Input wird eine von ili2gpkg erzeugte GeoPackage-Datei benötigt.\nEs werden alle INTERLIS-Klassen exportiert (SELECT tablename FROM T_ILI2DB_TABLE_PROP WHERE setting = 'CLASS'). Je nach, bei der Erstellung der GeoPackage-Datei, verwendeten Parametern, muss die Query angepasst werden. Es muss jedoch darauf geachtet werden, dass es nur eine Query gibt (für alle Datensätze). Für den vorgesehenen Anwendungsfall (sehr einfache, flache Modelle) dürfte das kein Problem darstellen.\nEncoding: Die Shapefiles sind neu UTF-8 encodiert. Standard ist ISO-8859-1, scheint aber v.a. in QGIS nicht standardmässig zu funktionieren (keine Umlaute).\nAchtung: Task sollte verbessert werden (siehe E-Mail Claude im Rahmen des Publisher-Projektes).\nBeispiel:\ntask gpkg2shp(type: Gpkg2Shp) {\n    dataFile = file(\"data.gpkg\")\n    outputDir = file(\"./out/\")\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndataFile\nGeoPackage-Datei, die nach Shapefile transformiert werden soll.\n\n\noutputDir\nVerzeichnis, in das die Shapefile gespeichert werden.\n\n\n\n\n\nGpkgValidator\nPrüft eine GeoPackage-Datei gegenüber einem INTERLIS-Modell. Basiert auf dem ilivalidator.\nBeispiel:\ntask validate(type: GpkgValidator){\n    models = \"GpkgModel\"\n    dataFiles = [\"attributes.gpkg\"]\n    tableName = \"Attributes\"\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndataFiles\nListe der GeoPackage-Dateien, die validiert werden sollen. Eine leere Liste ist kein Fehler.\n\n\ntableName\nName der Tabelle in den GeoPackage-Dateien.\n\n\nmodels\nINTERLIS-Modell, gegen das die die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Der Name der CSV-Datei.\n\n\nmodeldir\nDateipfade, die Modell-Dateien (ili-Dateien) enthalten. Mehrere Pfade können durch Semikolon ‚;‘ getrennt werden. Es sind auch URLs von Modell-Repositories möglich. Default: %XTF_DIR;http://models.interlis.ch/. %XTF_DIR ist ein Platzhalter für das Verzeichnis mit der SHP-Datei.\n\n\nconfigFile\nKonfiguriert die Datenprüfung mit Hilfe einer TOML-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n\n\nforceTypeValidation\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n\n\ndisableAreaValidation\nSchaltet die AREA Topologieprüfung aus. Default: false\n\n\nmultiplicityOff\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n\n\nallObjectsAccessible\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n\n\nlogFile\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n\n\nxtflogFile\nSchreibt die log-Meldungen in eine INTERLIS 2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n\n\npluginFolder\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n\n\nproxy\nProxy Server für den Zugriff auf Modell Repositories\n\n\nproxyPort\nProxy Port für den Zugriff auf Modell Repositories\n\n\nfailOnError\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n\n\nvalidationOk\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false\n\n\n\n\n\nGzip\nGzipped eine einzelne Datei. Es gibt einen eingebauten Tar-Task, der - nomen est omen - aber immer zuerst eine Tar-Datei erstellt.\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndataFile\nDatei, die gezipped werden soll.\n\n\ngzipFile\nOutput-Datei\n\n\n\nBeispiel:\ntask compressFile(type: Gzip) {\n    dataFile = file(\"./planregister.xml\");\n    gzipFile = file(\"./planregister.xml.gz\");\n}\n\n\nIli2gpkgImport\nImportiert Daten aus einer INTERLIS-Transferdatei in eine GeoPackage-Datei.\nDie Tabellen werden implizit auch angelegt, falls sie noch nicht vorhanden sind. Falls die Tabellen in der Datenbank schon vorhanden sind, können sie zusätzliche Spalten enthalten (z.B. bfsnr, datum etc.), welche beim Import leer bleiben.\nFalls beim Import ein Datensatz-Identifikator (dataset) definiert wird, darf dieser Datensatz-Identifikator in der Datenbank noch nicht vorhanden sein.\nUm die bestehenden (früher importierten) Daten zu ersetzen, kann der Task Ili2gpkgReplace (not yet implemented) verwendet werden.\nDie Option --doSchemaImport wird automatisch gesetzt.\nBeispiel:\ntask importData(type: Ili2gpkgImport) {\n    models = \"SO_AGI_AV_GB_Administrative_Einteilungen_20180613\"\n    dataFile = file(\"data.xtf\");\n    dbfile = file(\"data.gpkg\")    \n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndbfile\nGeoPackage-Datei in die importiert werden soll\n\n\ndataFile\nName der XTF-/ITF-Datei, die gelesen werden soll. Es können auch mehrere Dateien sein.\n\n\nproxy\nEntspricht der ili2gpkg Option --proxy\n\n\nproxyPort\nEntspricht der ili2gpkg-Option --proxyPort\n\n\nmodeldir\nEntspricht der ili2gpkg-Option --modeldir\n\n\nmodels\nEntspricht der ili2gpkg-Option --models\n\n\ndataset\nEntspricht der ili2gpkg-Option --dataset\n\n\nbaskets\nEntspricht der ili2gpkg-Option --baskets\n\n\ntopics\nEntspricht der ili2gpkg-Option --topics\n\n\npreScript\nEntspricht der ili2gpkg-Option --preScript\n\n\npostScript\nEntspricht der ili2gpkg-Option --postScript\n\n\ndeleteData\nEntspricht der ili2gpkg-Option --deleteData\n\n\nlogFile\nEntspricht der ili2gpkg-Option --logFile\n\n\nvalidConfigFile\nEntspricht der ili2gpkg-Option --validConfigFile\n\n\ndisableValidation\nEntspricht der ili2gpkg-Option --disableValidation\n\n\ndisableAreaValidation\nEntspricht der ili2gpkg-Option --disableAreaValidation\n\n\nforceTypeValidation\nEntspricht der ili2gpkg-Option --forceTypeValidation\n\n\nstrokeArcs\nEntspricht der ili2gpkg-Option --strokeArcs\n\n\nskipPolygonBuilding\nEntspricht der ili2gpkg-Option --skipPolygonBuilding\n\n\nskipGeometryErrors\nEntspricht der ili2gpkg-Option --skipGeometryErrors\n\n\niligml20\nEntspricht der ili2gpkg-Option --iligml20\n\n\ncoalesceJson\nEntspricht der ili2gpkg-Option --coalesceJson\n\n\nnameByTopic\nEntspricht der ili2gpkg-Option --nameByTopic\n\n\ndefaultSrsCode\nEntspricht der ili2gpkg-Option --defaultSrsCode\n\n\ncreateEnumTabs\nEntspricht der ili2gpkg-Option --createEnumTabs\n\n\ncreateMetaInfo\nEntspricht der ili2gpkg-Option --createMetaInfo\n\n\n\nFür die Beschreibung der einzenen ili2gpkg Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIli2pgExport\nExportiert Daten aus der PostgreSQL-Datenbank in eine INTERLIS-Transferdatei.\nMit dem Parameter models, topics, baskets oder dataset wird definiert, welche Daten exportiert werden.\nOb die Daten im INTERLIS 1-, INTERLIS 2- oder GML-Format geschrieben werden, ergibt sich aus der Dateinamenserweiterung der Ausgabedatei. Für eine INTERLIS 1-Transferdatei muss die Erweiterung .itf verwendet werden. Für eine GML-Transferdatei muss die Erweiterung .gml verwendet werden.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask exportData(type: Ili2pgExport){\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900-out.itf\"\n    dataset = \"254900\"\n    logFile = \"ili2pg.log\"\n}\nDamit mit einer einzigen Task-Definition mehrere Datensätze verarbeitet werden können, kann auch eine Liste von Dateinamen und Datensätzen angegeben werden.\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask exportData(type: Ili2pgExport){\n    database = [db_uri, db_user, db_pass]\n    dataFile = [\"lv03_254900-out.itf\",\"lv03_255000-out.itf\"]\n    dataset = [\"254900\",\"255000\"]\n    logFile = \"ili2pg.log\"\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndatabase\nDatenbank aus der exportiert werden soll\n\n\ndataFile\nName der XTF-/ITF-Datei, die erstellt werden soll\n\n\ndbschema\nEntspricht der ili2pg-Option --dbschema\n\n\nproxy\nEntspricht der ili2pg-Option --proxy\n\n\nproxyPort\nEntspricht der ili2pg-Option --proxyPort\n\n\nmodeldir\nEntspricht der ili2pg-Option --modeldir\n\n\nmodels\nEntspricht der ili2pg-Option --models\n\n\ndataset\nEntspricht der ili2pg-Option --dataset\n\n\nbaskets\nEntspricht der ili2pg-Option --baskets\n\n\ntopics\nEntspricht der ili2pg-Option --topics\n\n\npreScript\nEntspricht der ili2pg-Option --preScript\n\n\npostScript\nEntspricht der ili2pg-Option --postScript\n\n\ndeleteData\nEntspricht der ili2pg-Option --deleteData\n\n\nlogFile\nEntspricht der ili2pg-Option --logFile\n\n\nvalidConfigFile\nEntspricht der ili2pg-Option --validConfigFile\n\n\ndisableValidation\nEntspricht der ili2pg-Option --disableValidation\n\n\ndisableAreaValidation\nEntspricht der ili2pg-Option --disableAreaValidation\n\n\nforceTypeValidation\nEntspricht der ili2pg-Option --forceTypeValidation\n\n\nstrokeArcs\nEntspricht der ili2pg-Option --strokeArcs\n\n\nskipPolygonBuilding\nEntspricht der ili2pg-Option --skipPolygonBuilding\n\n\nskipGeometryErrors\nEntspricht der ili2pg-Option --skipGeometryErrors\n\n\nexport3\nEntspricht der ili2pg-Option --export3\n\n\niligml20\nEntspricht der ili2pg-Option --iligml20\n\n\ndisableRounding\nEntspricht der ili2pg-Option --disableRounding\n\n\nfailOnException\nTask wirft Exception, falls ili2db-Prozess fehlerhaft ist (= Ili2dbException). Default: true.\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIli2pgImport\nImportiert Daten aus einer INTERLIS-Transferdatei in die PostgreSQL-Datenbank.\nDie Tabellen werden implizit auch angelegt, falls sie noch nicht vorhanden sind. Falls die Tabellen in der Datenbank schon vorhanden sind, können sie zusätzliche Spalten enthalten (z.B. bfsnr, datum etc.), welche beim Import leer bleiben.\nFalls beim Import ein Datensatz-Identifikator (dataset) definiert wird, darf dieser Datensatz-Identifikator in der Datenbank noch nicht vorhanden sein.\nFalls man mehrere Dateien importieren will, diese jedoch erst zur Laufzeit eruiert werden können, muss der Parameter dataFile eine Gradle FileCollection resp. eine implementierende Klasse (z.B. FileTree) sein. Gleiches gilt für den dataset-Parameter. Als einzelner Wert für das Dataset wird in diesem Fall der Name der Datei ohne Extension und ohne Pfad verwendet. Leider kann nicht bereits in der Task-Definition aus dem Filetree eine Liste gemacht werden, z.B. fileTree(pathToUnzipFolder) { include '*.itf' }.files.name. Diese Liste ist leer.\nUm die bestehenden (früher importierten) Daten zu ersetzen, kann der Task Ili2pgReplace verwendet werden.\nIli2pgImport unterstützt den Import von Daten in einem ilidata-Repository. Der Dateinamen (dataFile) entspricht dem Identifikator der Datei im Repository. Dem Identifikator muss ilidata: vorangestellt werden.\nBeispiel 1:\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask importData(type: Ili2pgImport){\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900.itf\"\n    logFile = \"ili2pg.log\"\n}\nBeispiel 2:\nImport der AV-Daten. In der t_datasetname-Spalte soll die BFS-Nummer stehen. Die BFS-Nummer entspricht den ersten vier Zeichen des Filenamens.\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask importData(type: Ili2pgImport){\n    database = [db_uri, db_user, db_pass]\n    dataFile = fileTree(pathToUnzipFolder) { include '*.itf' }\n    dataset = dataFile\n    datasetSubstring = 0..4\n    logFile = \"ili2pg.log\"\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndatabase\nDatenbank in die importiert werden soll\n\n\ndataFile\nName der XTF-/ITF-Datei, die gelesen werden soll. Es können auch mehrere Dateien sein.\n\n\ndbschema\nEntspricht der ili2pg-Option --dbschema\n\n\nproxy\nEntspricht der ili2pg-Option --proxy\n\n\nproxyPort\nEntspricht der ili2pg-Option --proxyPort\n\n\nmodeldir\nEntspricht der ili2pg-Option --modeldir\n\n\nmodels\nEntspricht der ili2pg-Option --models\n\n\ndataset\nEntspricht der ili2pg-Option --dataset\n\n\ndatasetSubstring\nRange für das Extrahieren eines Substrings von Datasets\n\n\nbaskets\nEntspricht der ili2pg-Option --baskets\n\n\ntopics\nEntspricht der ili2pg-Option --topics\n\n\npreScript\nEntspricht der ili2pg-Option --preScript\n\n\npostScript\nEntspricht der ili2pg-Option --postScript\n\n\ndeleteData\nEntspricht der ili2pg-Option --deleteData\n\n\nlogFile\nEntspricht der ili2pg-Option --logFile\n\n\nimportTid\nEntspricht der ili2pg-Option --importTid\n\n\nimportBid\nEntspricht der lii2pg Option –importBid\n\n\nvalidConfigFile\nEntspricht der ili2pg-Option --validConfigFile\n\n\ndisableValidation\nEntspricht der ili2pg-Option --disableValidation\n\n\ndisableAreaValidation\nEntspricht der ili2pg-Option --disableAreaValidation\n\n\nforceTypeValidation\nEntspricht der ili2pg-Option --forceTypeValidation\n\n\nstrokeArcs\nEntspricht der ili2pg-Option --strokeArcs\n\n\nskipPolygonBuilding\nEntspricht der ili2pg-Option --skipPolygonBuilding\n\n\nskipGeometryErrors\nEntspricht der ili2pg-Option --skipGeometryErrors\n\n\niligml20\nEntspricht der ili2pg-Option --iligml20\n\n\ndisableRounding\nEntspricht der ili2pg-Option --disableRounding\n\n\nfailOnException\nTask wirft Exception, falls ili2db-Prozess fehlerhaft ist (= Ili2dbException). Default: true.\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIli2pgImportSchema\nErstellt die Tabellenstruktur in der PostgreSQL-Datenbank anhand eines INTERLIS-Modells.\nDer Parameter iliFile oder models muss gesetzt werden.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask importSchema(type: Ili2pgImportSchema){\n    database = [db_uri, db_user, db_pass]\n    models = \"DM01AVSO24\"\n    dbschema = \"gretldemo\"\n    logFile = \"ili2pg.log\"\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndatabase\nDatenbank in die importiert werden soll\n\n\niliFile\nName der ili-Datei die gelesen werden soll\n\n\nmodels\nName des ili-Modells, das gelesen werden soll\n\n\ndbschema\nEntspricht der ili2pg-Option --dbschema\n\n\nproxy\nEntspricht der ili2pg-Option --proxy\n\n\nproxyPort\nEntspricht der ili2pg-Option --proxyPort\n\n\nmodeldir\nEntspricht der ili2pg-Option --modeldir\n\n\ndataset\nEntspricht der ili2pg-Option --dataset\n\n\nbaskets\nEntspricht der ili2pg-Option --baskets\n\n\ntopics\nEntspricht der ili2pg-Option --topics\n\n\npreScript\nEntspricht der ili2pg-Option --preScript\n\n\npostScript\nEntspricht der ili2pg-Option --postScript\n\n\nlogFile\nEntspricht der ili2pg-Option --logFile\n\n\nstrokeArcs\nEntspricht der ili2pg-Option --strokeArcs\n\n\noneGeomPerTable\nEntspricht der ili2pg-Option --oneGeomPerTable\n\n\nsetupPgExt\nEntspricht der ili2pg-Option --setupPgExt\n\n\ndropscript\nEntspricht der ili2pg-Option --dropscript\n\n\ncreatescript\nEntspricht der ili2pg-Option --createscript\n\n\ndefaultSrsAuth\nEntspricht der ili2pg-Option --defaultSrsAuth\n\n\ndefaultSrsCode\nEntspricht der ili2pg-Option --defaultSrsCode\n\n\ncreateSingleEnumTab\nEntspricht der ili2pg-Option --createSingleEnumTab\n\n\ncreateEnumTabs\nEntspricht der ili2pg-Option --createEnumTabs\n\n\ncreateEnumTxtCol\nEntspricht der ili2pg-Option --createEnumTxtCol\n\n\ncreateEnumColAsItfCode\nEntspricht der ili2pg-Option --createEnumColAsItfCode\n\n\nbeautifyEnumDispName\nEntspricht der ili2pg-Option --beautifyEnumDispName\n\n\nnoSmartMapping\nEntspricht der ili2pg-Option --noSmartMapping\n\n\nsmart1Inheritance\nEntspricht der ili2pg-Option --smart1Inheritance\n\n\nsmart2Inheritance\nEntspricht der ili2pg-Option --smart2Inheritance\n\n\ncoalesceCatalogueRef\nEntspricht der ili2pg-Option --coalesceCatalogueRef\n\n\ncoalesceMultiSurface\nEntspricht der ili2pg-Option --coalesceMultiSurface\n\n\ncoalesceMultiLine\nEntspricht der ili2pg-Option --coalesceMultiLine\n\n\nexpandMultilingual\nEntspricht der ili2pg-Option --expandMultilingual\n\n\ncoalesceJson\nEntspricht der ili2pg-Option --coalesceJson\n\n\ncreateFk\nEntspricht der ili2pg-Option --createFk\n\n\ncreateFkIdx\nEntspricht der ili2pg-Option --createFkIdx\n\n\ncreateUnique\nEntspricht der ili2pg-Option --createUnique\n\n\ncreateNumChecks\nEntspricht der ili2pg-Option --createNumChecks\n\n\ncreateStdCols\nEntspricht der ili2pg-Option --createStdCols\n\n\nt_id_Name\nEntspricht der ili2pg-Option --t_id_Name\n\n\nidSeqMin\nEntspricht der ili2pg-Option --idSeqMin\n\n\nidSeqMax\nEntspricht der ili2pg-Option --idSeqMax\n\n\ncreateTypeDiscriminator\nEntspricht der ili2pg-Option --createTypeDiscriminator\n\n\ncreateGeomIdx\nEntspricht der ili2pg-Option --createGeomIdx\n\n\ndisableNameOptimization\nEntspricht der ili2pg-Option --disableNameOptimization\n\n\nnameByTopic\nEntspricht der ili2pg-Option --nameByTopic\n\n\nmaxNameLength\nEntspricht der ili2pg-Option --maxNameLength\n\n\nsqlEnableNull\nEntspricht der ili2pg-Option --sqlEnableNull\n\n\nsqlColsAsText\nEntspricht der ili2pg-Option --sqlColsAsText\n\n\nsqlExtRefCols\nEntspricht der ili2pg-Option --sqlExtRefCols\n\n\nkeepAreaRef\nEntspricht der ili2pg-Option --keepAreaRef\n\n\ncreateTidCol\nEntspricht der ili2pg-Option --createTidCol\n\n\ncreateBasketCol\nEntspricht der ili2pg-Option --createBasketCol\n\n\ncreateDatasetCol\nEntspricht der ili2pg-Option --createDatasetCol\n\n\nver4_translation\nEntspricht der ili2pg-Option --ver4_translation\n\n\ntranslation\nEntspricht der ili2pg-Option --translation\n\n\ncreateMetaInfo\nEntspricht der ili2pg-Option --createMetaInfo\n\n\nfailOnException\nTask wirft Exception, falls ili2db-Prozess fehlerhaft ist (= Ili2dbException). Default: true.\n\n\n\nFür die Beschreibung der einzenen ili2pg-Optionen: https://github.com/claeis/ili2db/blob/master/docs/ili2db.rst#aufruf-syntax\n\n\nIli2pgReplace\nErsetzt die Daten in der PostgreSQL-Datenbank anhand eines Datensatz-Identifikators (dataset) mit den Daten aus einer INTERLIS-Transferdatei. Diese Funktion bedingt, dass das Datenbankschema mit der Option createBasketCol erstellt wurde (via Task Ili2pgImportSchema).\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask replaceData(type: Ili2pgReplace) {\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900.itf\"\n    dataset = \"254900\"\n    logFile = \"ili2pg.log\"\n}\nDie Parameter sind analog wie bei Ili2pgImport. Ili2pgReplace unterstützt den Import von Daten in einem ilidata-Repository. Der Dateinamen (dataFile) entspricht dem Identifikator der Datei im Repository. Dem Identifikator muss ilidata: vorangestellt werden.\n\n\nIli2pgDelete\nLöscht einen Datensatz in der PostgreSQL-Datenbank anhand eines Datensatz-Identifikators. Diese Funktion bedingt, dass das Datenbankschema mit der Option createBasketCol erstellt wurde (via Task Ili2pgImportSchema). Der Parameter failOnException muss false sein, ansonsten bricht der Job ab.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask deleteDataset(type: Ili2pgDelete){\n    database = [db_uri, db_user, db_pass]\n    models = \"DM01AVSO24LV95\"\n    dbschema = \"dm01\"\n    dataset = \"kammersrohr\"\n}\nEs können auch mehrere Datensätze pro Task gelöscht werden:\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask deleteDataset(type: Ili2pgDelete){\n    database = [db_uri, db_user, db_pass]\n    dbschema = \"dm01\"\n    dataset = [\"Olten\",\"Grenchen\"]\n}\n\n\nIli2pgUpdate\nAktualisiert die Daten in der PostgreSQL-Datenbank anhand einer INTERLIS-Transferdatei, d.h. neue Objekte werden eingefügt, bestehende Objekte werden aktualisiert und in der Transferdatei nicht mehr vorhandene Objekte werden gelöscht.\nDiese Funktion bedingt, dass das Datenbankschema mit der Option createBasketCol erstellt wurde (via Task Ili2pgImportSchema), und dass die Klassen und Topics eine stabile OID haben.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask updateData(type: Ili2pgUpdate){\n    database = [db_uri, db_user, db_pass]\n    dataFile = \"lv03_254900.itf\"\n    dataset = \"254900\"\n    logFile = \"ili2pg.log\"\n}\nDie Parameter sind analog wie bei Ili2pgImport.\n\n\nIli2pgValidate\nPrüft die Daten ohne diese in eine Datei zu exportieren. Der Task ist erfolgreich, wenn keine Fehler gefunden werden und ist nicht erfolgreich, wenn Fehler gefunden werden. Mit der Option failOnException=false ist der Task erfolgreich, auch wenn Fehler gefunden werden.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntasks.register('validate', Ili2pgValidate) {\n    database = [db_uri, db_user, db_pass]\n    models = \"SO_AGI_AV_GB_Administrative_Einteilungen_20180613\"\n    modeldir = rootProject.projectDir.toString() + \";http://models.interlis.ch\"\n    dbschema = \"agi_av_gb_admin_einteilungen_fail\"\n    logFile = file(\"fubar.log\")\n}\n\n\nIliValidator\nPrüft eine INTERLIS-Datei (.itf oder .xtf) gegenüber einem INTERLIS-Modell (.ili). Basiert auf dem ilivalidator.\nBeispiel:\ntask validate(type: IliValidator){\n    dataFiles = [\"Beispiel2a.xtf\"]\n    logFile = \"ilivalidator.log\"\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndataFiles\nListe der XTF- oder ITF-Dateien, die validiert werden sollen. Eine leere Liste ist kein Fehler.\n\n\nmodels\nINTERLIS-Modell, gegen das die die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Wird anhand der dataFiles ermittelt.\n\n\nmodeldir\nDateipfade, die Modell-Dateien (ili-Dateien) enthalten. Mehrere Pfade können durch Semikolon ‚;‘ getrennt werden. Es sind auch URLs von Modell-Repositories möglich. Default: %XTF_DIR;http://models.interlis.ch/. %XTF_DIR ist ein Platzhalter für das Verzeichnis mit der CSV-Datei.\n\n\nconfigFile\nKonfiguriert die Datenprüfung mit Hilfe einer TOML-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n\n\nforceTypeValidation\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n\n\ndisableAreaValidation\nSchaltet die AREA Topologieprüfung aus. Default: false\n\n\nmultiplicityOff\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n\n\nallObjectsAccessible\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n\n\nskipPolygonBuilding\nSchaltet die Bildung der Polygone aus (nur ITF). Default: false\n\n\nlogFile\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n\n\nxtflogFile\nSchreibt die log-Meldungen in eine INTERLIS 2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n\n\npluginFolder\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n\n\nproxy\nProxy Server für den Zugriff auf Modell Repositories\n\n\nproxyPort\nProxy Port für den Zugriff auf Modell Repositories\n\n\nfailOnError\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n\n\nvalidationOk\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false\n\n\n\nZusatzfunktionen (Custom Functions): Die pluginFolder-Option ist zum jetzigen Zeitpunkt ohne Wirkung. Die Zusatzfunktionen werden als normale Abhängigkeit definiert und in der ilivalidator-Task-Implementierung registriert. Das Laden der Klassen zur Laufzeit in iox-ili hat nicht funktioniert (NoClassDefFoundError…). Der Plugin-Mechanismus von ilivalidator wird momentan ohnehin geändert (“Ahead-Of-Time-tauglich” gemacht).\n\n\nJsonImport\nDaten aus einer Json-Datei in eine Datenbanktabelle importieren. Die gesamte Json-Datei (muss UTF-8 encoded sein) wird als Text in eine Spalte importiert. Ist das Json-Objekt in der Datei ein Top-Level-Array wird für jedes Element des Arrays ein Record in der Datenbanktabelle erzeugt.\nBeispiel:\ntask importJson(type: JsonImport){\n    database = [db_uri, db_user, db_pass]\n    jsonFile = \"data.json\"\n    qualifiedTableName = \"jsonimport.jsonarray\"\n    columnName = \"json_text_col\"\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndatabase\nDatenbank in die importiert werden soll\n\n\njsonFile\nJson-Datei, die importiert werden soll\n\n\nqualifiedTableName\nQualifizierter Tabellennamen (“schema.tabelle”) in die importiert werden soll\n\n\ncolumnName\nSpaltenname der Tabelle in die importiert werden soll\n\n\ndeleteAllRows\nInhalt der Tabelle vorgängig löschen?\n\n\n\n\n\nMetaPublisher (incubating)\nDer MetaPublisher dient zum Publizieren von Metadaten/-dateien. Er erstellt eine INTERLIS-Transferdatei mit Metadaten pro Themenpublikation für die Datensuche, das Datenblatt (die HTML-Datei detaillierten Meta-Infos), falls nötig eine GeoJSON-Datei und die Geocat-XML-Datei. Die GeoJSON-Datei ist entweder statisch (für Raster o.ä.) oder dynamisch (z.B. amtliche Vermessung). Bei einer dynamischen GeoJSON-Datei wird das Datum nachgeführt.\nPro Themenpublikation (also pro Publisher-Task) kann resp. muss es auch einen MetaPublisher-Task geben. Informationen zum Thema sind in einer TOML-Datei gespeichert. Diese dient auch zum Überschreiben von Klassen- und Attributbeschreibungen, die aus der Modelldatei gelesen werden. Die GeoJSON-Datei muss im gleichen Verzeichnis wie die TOML-Datei vorliegen.\nMit dem MetaPublisher können auch Daten resp. Datenthemen publiziert werden, die nicht im SIMI erfasst sind und/oder nicht als Kartendienst publiziert werden (z.B. ÖREB-Daten).\nBeispiel einer TOML-Datei:\n[\"meta\"]\nidentifier = \"ch.so.afu.abbaustellen\"\ntitle = \"Abbaustellen\"\ndescription = \"\"\"\nDie Abbaustellen umfassen die Flächen folgender Objekte: &lt;br/&gt;\n&lt;ul&gt;\n&lt;li&gt;sämtliche grösseren Abbaugebiete (Kiesgruben, Kalksteinbrüche sowie Tongruben), für welche ein Gestaltungsplan vorliegt. Die dargestellten Flächen umfassen jeweils den gesamten Perimeter der genehmigten Gestaltungspläne, und nicht einzelne Abbauetappen.&lt;/li&gt;\n&lt;li&gt;Kleinabbaustellen. Es handelt sich üblicherweise um kleinere, gemeindeeigene Mergelgruben, in welchen Material für den Bau und Unterhalt von Wald- und Flurwegen abgebaut wird. Kleinabbaustellen erfordern keinen Gestaltungsplan. Die dargestellten Flächen umfassen hier jeweils den auf Stufe Bau-, bzw. Abbaubewilligung genehmigten Perimeter.&lt;/li&gt;\n&lt;li&gt;alle künftigen Erweiterungs- und Ersatzstandorte, welche im kantonalen Richtplan (Kap. E-3.1 bis E-3.4) enthalten sind.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;br&gt;\nDie Flächen wurden von verschiedenen Planvorlagen und bestehenden Flächendaten mit unterschiedlichem Massstab digitalisiert, bzw. übernommen.\n\"\"\"\nkeywords = \"unverschmutztes Aushubmaterial\"\nsynonyms = \"Abbaugebiet,Kiesabbau,Kiesgrube,Steinbruch,Kalksteinbruch,Kalksteinbrüche,Tongrube,Kleinabbaustelle,Mergelgrube\"\nmodel = \"SO_AFU_ABBAUSTELLEN_Publikation_20221103\"\nowner = \"ch.so.afu\"\nservicer = \"ch.so.agi\"\nlicence = \"https://files.geo.so.ch/nutzungsbedingungen.html\"\nfurtherInformation = \"https://so.ch/verwaltung/bau-und-justizdepartement/amt-fuer-umwelt/boden-untergrund-geologie/rohstoffe-abbau/\"\nformats = [\"XTF\", \"GPKG\", \"SHP\", \"DXF\"]\n\n[\"SO_AFU_ABBAUSTELLEN_Publikation_20221103.Abbaustelle.Abbaustelle\"]\ntitle = \"Standorte Abbaustellen\"\ndescription = \"Umfasst die Flächenobjekte der Abbaugebiete, Kleinabbaustellen und künftigen Standorte. Er enthält u.a. Angaben zum abgebauten Material und zur Abstimmungskategorie im Richtplan.\"\nDie Struktur, die möglichen Attribute im TOML und die Auswertelogik können sich noch ändern.\nÄmter (“owner” und “servicer”) und Formate werden im TOML nur verlinkt. Die Daten liegen in einer XTF-Datein als Resource in GRETL vor. Das hat heute den Nachteil, dass bei einer Änderung eine neue GRETL-Version installiert werden muss. Zukünft (Idee “Themenrepo”) wird das anders gelöst.\nEs gibt Attribute (“identifier” und “model”), die redundant sind, wenn es auch einen Publisher-Task gibt. Weil es aber auch MetaPublisher-Tasks ohne einen Publisher-Task geben kann, ist das bis auf weiteres so. Vielleicht könnte man diese beiden Attribute als Task-Property exponieren und sie somit von aussen (über)steuern, wenn es sie gibt.\nMomentan werden die erzeugen Meta-Dateien im jeweiligen Ordner der Themenpublikation im meta-Verzeichnis gespeichert und zusätzlich (damit es für die Datensuche leichter fällt) in einem übergeordneten config-Verzeichnis.\nDie Informationen zu den Klassen/Tabellen und Attributen wird in erster Linie aus dem Datenmodell gelesen. Das Datenmodell muss in einem INTERLIS-Repo vorliegen oder im gleichen Verzeichnis wie die TOML-Datei gespeichert sein. Der Parser ist (noch) sehr einfach und es wird vor allem der Fokus auf flache Datenmodelle gelegte (keine Beziehungen). Bei Klassen wird das Metaattribut title ausgelesen und für den “schönen” Titel der Klasse verwendet. Für die Beschreibung der Klasse wird der Blockkommentar ausgewertet. Beide Informationen können im TOML-File überschrieben werden (siehe Beispiel). Bei Attributen gibt es kein Metaattribut, sondern der Name des Attributes und der Blockkommentar.\nDas “gretlEnvironmement”-Property wird verwendet, um zu entscheiden in welche Geocat-Umgebung die Daten hochgeladen werden. Alles ausser production landet in der Geocat-Testumgebung. Die “files”- und “data”-URL, die in der XTF- und HTML-Datei (“Datenblatt”) landen, werden ebenfalls darüber gesteuert. Die umgebungsabhängigen URL sind hardodiert im MetaPublisherStep-Code.\nMan kann die Herstellung der Klassen-/Tabellen-Metainformationen unterdrücken, in dem man in der Section [\"config\"] das Attribut printClassDescription auf false setzt. Dies ist zwingend notwendig für Rasterthemen und sinnvoll für Edit-Modelle.\nDas Publikationsdatum kann in der TOML-Datei definiert werden. Fehlt es, wird das aktuelle Datum verwendet.\nRegionen: Datenthemen, die in Regionen (Subunits, Datasets) unterteilt sind, benötigen eine GeoJSON-Datei, die Informationen über die einzelnen Regionen enthält (Geometrie, Name, Identifier, Nachführungs- resp. Publikationsdatum). Die GeoJSON-Datei wird vor der Datensuche-Anwendung direkt verwendet. Die GeoJSON-Datei ist entweder statisch oder dynamisch. Die statische GeoJSON-Datei muss nicht nachgeführt werden, bei dynamischen muss in der Regel das Publikationsdatum nachgeführt werden (z.B. amtliche Vermessung oder Nutzungsplanung). Im dynamischen Fall wird geprüft, ob die Datei bereits in der Datenablage publiziert wurde. Ist dies nicht der Fall, wird sie dorthin kopiert. Anschliessend werden die Publikationsdaten nachgeführt. Im statischen Fall wird ebenfalls geprüft, ob sie bereits bereitgestellt wird und gegebenenfalls dorthin kopiert. Ein Rückfluss in das Github-Repo findet nicht statt. D.h. die Datei im Repo ist eher als GeoJSON-Template zu betrachten und nicht als Daten-Master (im dynamischen Fall). Aus der GeoJSON-Datei werden beim Herstellen der XTF-Datei (für die Datensuche) Informationen gelesen.\nDie verschiedenen Dateien, die während im Task hergestellt werden, werden zuerst immer lokal gespeichert und erst am ganz am Schluss (wenn alle Dateien vorhanden sind), werden sie an ihren Zielort kopiert.\nVollständige Dokumentation muss noch erfolgen.\nSiehe auch die TOML-Beispiele im Quellcode.\nBeispiele:\ntasks.register('publishMetaFile', MetaPublisher) {\n    metaConfigFile = file(\"meta.toml\")\n    target = [\"sftp://foo.bar.ch\", \"user\", \"pwd\"]      \n    geocatTarget = [Paths.get(project.buildDir.getAbsolutePath(), \"geocat\").toFile()]\n}\ntasks.register('publishMetaFiles', MetaPublisher) {\n    dependsOn 'publishFiles'\n    metaConfigFile = file(\"meta-dm01_so.toml\")\n    target = [project.buildDir]  \n    regions = publishFiles.publishedRegions\n}\nIm zweiten Beispiel werden die Regionen aus dem vorausgegangenen Publisher-Task als Input verwendet.\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\nmetaConfigFile\nToml-Datei mit Metainformationen zur Themenpublikation\n\n\ntarget\nZielverzeichnis der Metadateien (sftp oder Filesystem)\n\n\nregions\nListe (resp. ListProperty) mit den durch den PublisherTask publizierten Regionen.\n\n\ngeocatTarget\nZielverzeichnis für Geocat-Output (sftp oder Filesystem)\n\n\n\n\n\nOgdMetaPublisher (incubating)\nPubliziert die Metadaten eines OGD-Datensatzes. Im Gegensatz zum “MetaPublisher” ist es weniger als Publisher (mit viel Konvention), sondern eher ein Generator, der aus dem Toml-File und der INTERLIS-Modelldatei die Metainfo-Datei erstellt. Beim Resultat handelt es sich um eine INTERLIS-Transferdatei (mit eigenem OgdMeta-Modell).\nMit dem “MetaPublisher” teilt er sich einiges an Copy/Paste-Code. Man könnte gewissen Aspekte wohl zusammenlegen (z.B. Infos aus INTERLIS-Modell lesen).\nDie INTERLIS-Modelldatei muss entweder in einem (bekannten) Repo sein oder im Verzeichnis der Toml-Datei vorliegen.\nBeispiel:\ntask publishMeta(type: OgdMetaPublisher) {\n    configFile = file(\"./ch.so.hba.kantonale_gebaeude.toml\")\n    outputDir = file(\".\")\n}\n\n\n\nParameter\nBeschreibung\n\n\n\n\nconfigFile\nToml-Datei mit Metainformationen zum Datensatz.\n\n\noutputDir\nVerzeichnis, in das die Metainfo-Datei gespeichert wird.\n\n\n\n\n\nPostgisRasterExport\nExportiert eine PostGIS-Raster-Spalte in eine Raster-Datei mittels SQL-Query. Die SQL-Query darf nur einen Record zurückliefern, d.h. es muss unter Umständen ST_Union() verwendet werden. Es angenommen, dass die erste bytea-Spalte des Resultsets die Rasterdaten enthält. Weitere bytea-Spalten werden ignoriert.\nBeispiel:\ntask exportTiff(type: PostgisRasterExport) {\n    database = [db_uri, db_user, db_pass]\n    sqlFile = \"raster.sql\"\n    dataFile = \"export.tif\"\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndatabase\nDatenbank aus der exportiert werden soll.\n\n\nsqlFile\nName der SQL-Datei aus das SQL-Statement gelesen und ausgeführt wird.\n\n\ndataFile\nName der Rasterdatei, die erstellt werden soll.\n\n\n\n\n\nPublisher\nStellt für Vektordaten die aktuellsten Geodaten-Dateien bereit und pflegt das Archiv der vorherigen Zeitstände.\nDetails\n\n\nS3Download\nLädt eine Datei aus einem S3-Bucket herunter.\nBeispiel:\ntask downloadFile(type: S3Download) {\n    accessKey = abcdefg\n    secretKey = hijklmnopqrstuvwxy\n    downloadDir = file(\"./path/to/dir/\")\n    bucketName = \"ch.so.ada.denkmalschutz\"\n    key = \"foo.pdf\"\n    endPoint = \"https://s3.eu-central-1.amazonaws.com\" \n    region = \"eu-central-1\"\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\naccessKey\nAccessKey\n\n\nsecretKey\nSecretKey\n\n\ndownloadDir\nVerzeichnis in das die Datei heruntergeladen werden soll.\n\n\nbucketName\nName des Buckets, in dem die Datei gespeichert ist.\n\n\nkey\nName der Datei\n\n\nendPoint\nS3-Endpunkt (default: https://s3.eu-central-1.amazonaws.com/)\n\n\nregion\nS3-Region (default: eu-central-1).\n\n\n\n\n\nS3Upload\nLädt ein Dokument (sourceFile) oder alle Dokumente in einem Verzeichnis (sourceDir) in einen S3-Bucket (bucketName) hoch.\nMit dem passenden Content-Typ kann man das Verhalten des Browsers steuern. Default ist ‘application/octect-stream’, was dazu führt, dass die Datei immer heruntergeladen wird. Soll z.B. ein PDF oder ein Bild im Browser direkt angezeigt werden, muss der korrekte Content-Typ gewählt werden.\nBeispiel:\ntask uploadDirectory(type: S3Upload) {\n    accessKey = abcdefg\n    secretKey = hijklmnopqrstuvwxy\n    sourceDir = file(\"./docs\")\n    bucketName = \"ch.so.ada.denkmalschutz\"\n    endPoint = \"https://s3.eu-central-1.amazonaws.com\" \n    region = \"eu-central-1\"\n    acl = \"public-read\"\n    contentType = \"application/pdf\"\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\naccessKey\nAccessKey\n\n\nsecretKey\nSecretKey\n\n\nsourceDir\nVerzeichnis mit den Dateien, die hochgeladen werden sollen.\n\n\nsourceFile\nDatei, die hochgeladen werden soll.\n\n\nsourceFiles\nFileCollection mit den Dateien, die hochgeladen werden sollen, z.B. fileTree(\"/path/to/directoy/\") { include \"*.itf\" }\n\n\nbucketName\nName des Buckets, in dem die Dateien gespeichert werden sollen.\n\n\nendPoint\nS3-Endpunkt (default: https://s3.eu-central-1.amazonaws.com/)\n\n\nregion\nS3-Region (default: eu-central-1).\n\n\nacl\nAccess Control Layer [private, public-read, public-read-write, authenticated-read, aws-exec-read, bucket-owner-read, bucket-owner-full-control]\n\n\ncontentType\nContent-Type\n\n\nmetaData\nMetadaten des Objektes resp. der Objekte, z.B. [\"lastModified\":\"2020-08-28\"].\n\n\n\n\n\nS3Bucket2Bucket\nKopiert Objekte von einem Bucket in einen anderen. Die Buckets müssen in der gleichen Region sein. Die Permissions werden nicht mitkopiert und müssen explizit gesetzt werden.\nBeispiel:\ntask copyFiles(type: S3Bucket2Bucket, dependsOn:'directoryupload') {\n    accessKey = s3AccessKey\n    secretKey = s3SecretKey\n    sourceBucket = s3SourceBucket\n    targetBucket = s3TargetBucket\n    acl = \"public-read\"\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\naccessKey\nAccessKey\n\n\nsecretKey\nSecretKey\n\n\nsourceBucket\nBucket aus dem die Objekte kopiert werden.\n\n\ntargetBucket\nBucket in den die Objekte kopiert werden.\n\n\nacl\nAccess Control Layer [private, public-read, public-read-write, authenticated-read, aws-exec-read, bucket-owner-read, bucket-owner-full-control]\n\n\nmetaData\nMetadaten des Objektes resp. der Objekte, z.B. [\"lastModified\":\"2020-08-28\"].\n\n\n\n\n\nShpExport\nDaten aus einer bestehenden Datenbanktabelle werden in eine Shp-Datei exportiert.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask shpexport(type: ShpExport){\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"shpexport\"\n    tableName = \"exportdata\"\n    dataFile = \"data.shp\"\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndatabase\nDatenbank aus der exportiert werden soll\n\n\ndataFile\nName der SHP Datei, die erstellt werden soll\n\n\ntableName\nName der DB-Tabelle, die exportiert werden soll\n\n\nschemaName\nName des DB-Schemas, in dem die DB-Tabelle ist.\n\n\nfirstLineIsHeader\nDefiniert, ob eine Headerzeile geschrieben werden soll, oder nicht. Default: true\n\n\nencoding\nZeichencodierung der SHP-Datei, z.B. \"UTF-8\". Default: Systemeinstellung\n\n\n\nDie Tabelle darf eine Geometriespalte enthalten.\n\n\nShpImport\nDaten aus einer Shp-Datei in eine bestehende Datenbanktabelle importieren.\nBeispiel:\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask shpimport(type: ShpImport){\n    database = [db_uri, db_user, db_pass]\n    schemaName = \"shpimport\"\n    tableName = \"importdata\"\n    dataFile = \"data.shp\"\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndatabase\nDatenbank in die importiert werden soll\n\n\ndataFile\nName der SHP-Datei, die gelesen werden soll\n\n\ntableName\nName der DB-Tabelle, in die importiert werden soll\n\n\nschemaName\nName des DB-Schemas, in dem die DB-Tabelle ist.\n\n\nencoding\nZeichencodierung der SHP-Datei, z.B. \"UTF-8\". Default: Systemeinstellung\n\n\nbatchSize\nAnzahl der Records, die pro Batch in die Ziel-Datenbank geschrieben werden (Standard: 5000).\n\n\n\nDie Tabelle kann weitere Spalten enthalten, die in der Shp-Datei nicht vorkommen. Sie müssen aber NULLable sein, oder einen Default-Wert definiert haben.\nDie Tabelle muss eine Geometriespalte enthalten. Der Name der Geometriespalte kann beliebig gewählt werden.\nDie Gross-/Kleinschreibung der Shp-Spaltennamen wird für die Zuordnung zu den DB-Spalten ignoriert.\n\n\nShpValidator\nPrüft eine SHP-Datei gegenüber einem INTERLIS-Modell. Basiert auf dem ilivalidator.\nBeispiel:\ntask validate(type: ShpValidator){\n    models = \"ShpModel\"\n    dataFiles = [\"data.shp\"]\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndataFiles\nListe der SHP-Dateien, die validiert werden sollen. Eine leere Liste ist kein Fehler.\n\n\nmodels\nINTERLIS-Modell, gegen das die Dateien geprüft werden sollen (mehrere Modellnamen durch Semikolon trennen). Default: Der Name der SHP-Datei.\n\n\nmodeldir\nDateipfade, die Modell-Dateien (ili-Dateien) enthalten. Mehrere Pfade können durch Semikolon ‚;‘ getrennt werden. Es sind auch URLs von Modell-Repositories möglich. Default: %XTF_DIR;http://models.interlis.ch/. %XTF_DIR ist ein Platzhalter für das Verzeichnis mit der SHP-Datei.\n\n\nconfigFile\nKonfiguriert die Datenprüfung mit Hilfe einer TOML-Datei (um z.B. die Prüfung von einzelnen Constraints auszuschalten). siehe https://github.com/claeis/ilivalidator/blob/master/docs/ilivalidator.rst#konfiguration\n\n\nforceTypeValidation\nIgnoriert die Konfiguration der Typprüfung aus der TOML-Datei, d.h. es kann nur die Multiplizität aufgeweicht werden. Default: false\n\n\ndisableAreaValidation\nSchaltet die AREA Topologieprüfung aus. Default: false\n\n\nmultiplicityOff\nSchaltet die Prüfung der Multiplizität generell aus. Default: false\n\n\nallObjectsAccessible\nMit der Option nimmt der Validator an, dass er Zugriff auf alle Objekte hat. D.h. es wird z.B. auch die Multiplizität von Beziehungen auf externe Objekte geprüft. Default: false\n\n\nlogFile\nSchreibt die log-Meldungen der Validierung in eine Text-Datei.\n\n\nxtflogFile\nSchreibt die log-Meldungen in eine INTERLIS 2-Datei. Die Datei result.xtf entspricht dem Modell IliVErrors.\n\n\npluginFolder\nVerzeichnis mit JAR-Dateien, die Zusatzfunktionen enthalten.\n\n\nproxy\nProxy Server für den Zugriff auf Modell Repositories\n\n\nproxyPort\nProxy Port für den Zugriff auf Modell Repositories\n\n\nfailOnError\nSteuert, ob der Task bei einem Validierungsfehler fehlschlägt. Default: true\n\n\nvalidationOk\nOUTPUT: Ergebnis der Validierung. Nur falls failOnError=false\n\n\nencoding\nZeichencodierung der SHP-Datei, z.B. \"UTF-8\". Default: Systemeinstellung\n\n\n\nIm gegebenen Modell wird eine Klasse gesucht, die genau die Attributenamen wie in der Shp-Datei enthält (wobei die Gross- Kleinschreibung ignoriert wird); die Attributtypen werden ignoriert. Wird keine solche Klasse gefunden, gilt das als Validierungsfehler.\nDie Prüfung von gleichzeitig mehreren Shapefiles führt zu Fehlermeldungen wie OID o3158 of object &lt;Modelname&gt;.&lt;Topicname&gt;.&lt;Klassenname&gt; already exists in .... Beim Öffnen und Lesen eines Shapefiles wird immer der Zähler, der die interne (im Shapefile nicht vorhandene) OID generiert, zurückgesetzt. Somit kann immer nur ein Shapefile pro Task geprüft werden.\n\n\nSqlExecutor\nDer SqlExecutor-Task dient dazu, Datenumbauten auszuführen.\nEr wird im Allgemeinen dann benutzt, wenn\n\nder Datenumbau komplex ist und deshalb nicht im Db2Db-Task erledigt werden kann\noder wenn die Quell-DB keine PostgreSQL-DB ist (weil bei komplexen Queries für den Datenumbau möglicherweise fremdsystemspezifische SQL-Syntax verwendet werden müsste)\noder wenn Quell- und Zielschema in derselben Datenbank liegen\n\nIn den Fällen 1 und 2 werden Stagingtabellen bzw. ein Stagingschema benötigt, in welche der Db2Db-Task die Daten zuerst 1:1 hineinschreibt. Der SqlExecutor-Task liest danach die Daten von dort, baut sie um und schreibt sie dann ins Zielschema. Die Queries für den SqlExecutor-Task können alle in einem einzelnen .sql-File sein oder (z.B. aus Gründen der Strukturierung oder Organisation) auf mehrere .sql-Dateien verteilt sein. Die Reihenfolge der .sql-Dateien ist relevant. Dies bedeutet, dass die SQL-Befehle des zuerst angegebenen .sql-Datei zuerst ausgeführt werden müssen, danach dies SQL-Befehle des an zweiter Stelle angegebenen .sql-Datei, usw.\nDer SqlExecutor-Task muss neben Updates ganzer Tabellen (d.h. Löschen des gesamten Inhalts einer Tabelle und gesamter neuer Stand in die Tabelle schreiben) auch Updates von Teilen von Tabellen zulassen. D.h. es muss z.B. möglich sein, innerhalb einer Tabelle nur die Objekte einer bestimmten Gemeinde zu aktualisieren. Darum ist es möglich innerhalb der .sql-Datei Paramater zu verwenden und diesen Parametern beim Task einen konkreten Wert zuzuweisen. Innerhalb der .sql-Datei werden Paramter mit folgender Syntax verwendet: ${paramName}.\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask executeSomeSql(type: SqlExecutor){\n    database = [db_uri, db_user, db_pass]\n    sqlParameters = [dataset:'Olten']\n    sqlFiles = ['demo.sql']\n}\nDamit mit einer einzigen Task-Definition mehrere Datensätze verarbeitet werden können, kann auch eine Liste von Parametern angegeben werden.\ndef db_uri = 'jdbc:postgresql://localhost/gretldemo'\ndef db_user = \"dmluser\"\ndef db_pass = \"dmluser\"\n\ntask executeSomeSql(type: SqlExecutor){\n    database = [db_uri, db_user, db_pass]\n    sqlParameters = [[dataset:'Olten'],[dataset:'Grenchen']]\n    sqlFiles = ['demo.sql']\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndatabase\nDatenbank in die importiert werden soll\n\n\nsqlFiles\nName der SQL-Datei aus der SQL-Statements gelesen und ausgeführt werden\n\n\nsqlParameters\nEine Map mit Paaren von Parameter-Name und Parameter-Wert (Map&lt;String,String&gt;). Oder eine Liste mit Paaren von Parameter-Name und Parameter-Wert (List&lt;Map&lt;String,String&gt;&gt;).\n\n\n\nUnterstützte Datenbanken: PostgreSQL, SQLite und Oracle. Der Oracle-JDBC-Treiber muss jedoch selber installiert werden (Ausgenommen vom Docker-Image).\n\n\nXslTransformer (incubating)\nTransformiert eine Datei mittels einer XSL-Transformation ein eine andere Datei. Ist der xslFile-Parameter ein String, wird erwartet, dass die Datei im GRETL-Verzeichnis im Ressourcenordner src/main/resources/xslt-Verzeichnis gespeichert ist. Falls der xslFile-Parameter ein File-Objekt ist, können lokale Dateien verwendet werden.\ntask transform(type: XslTransformer) {\n    xslFile = \"eCH0132_to_SO_AGI_SGV_Meldungen_20221109.xsl\"\n    xmlFile = file(\"MeldungAnGeometer_G-0111102_20221103_145001.xml\")\n    outDirectory = file(\".\")\n}\ntask transform(type: XslTransformer) {\n    xslFile = file(\"path/to/eCH0132_to_SO_AGI_SGV_Meldungen_20221109.xsl\")\n    xmlFile = file(\"MeldungAnGeometer_G-0111102_20221103_145001.xml\")\n    outDirectory = file(\".\")\n}\ntask transform(type: XslTransformer) {\n    xslFile = \"eCH0132_to_SO_AGI_SGV_Meldungen_20221109.xsl\"\n    xmlFile = fileTree(\".\").matching {\n        include \"*.xml\"\n    }\n    outDirectory = file(\".\")\n}\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\nxslFile\nName der XSLT-Datei, die im src/main/resources/xslt-Verzeichnis liegen muss oder File-Objekt (beliebier Pfad).\n\n\nxmlFile\nDatei oder FileTree, die/der transformiert werden soll.\n\n\noutDirectory\nVerzeichnis, in das die transformierte Datei gespeichert wird. Der Name der transformierten Datei entspricht dem Namen der Inpuzt-Datei mit Endung .xtf."
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "Get Started",
    "section": "",
    "text": "Let’s start with a simple example: Download, unzip and validate an INTERLIS transfer file:"
  },
  {
    "objectID": "getting_started.html#java-flavour",
    "href": "getting_started.html#java-flavour",
    "title": "Get Started",
    "section": "Java flavour",
    "text": "Java flavour\nYou need Java 1.8 and Gradle &gt;= 5.1.1 (and &lt; 6) installed on your machine.\nCreate a build.gradle file and paste the following code into the file:\nimport java.nio.file.Paths\nimport ch.so.agi.gretl.tasks.*\nimport ch.so.agi.gretl.api.*\nimport de.undercouch.gradle.tasks.download.Download\n\nbuildscript {\n    repositories {\n        maven { url \"https://jars.interlis.ch\" }\n        maven { url \"https://repo.osgeo.org/repository/release/\" }\n        maven { url \"https://plugins.gradle.org/m2/\" }\n        maven { url \"https://s01.oss.sonatype.org/service/local/repositories/releases/content/\" }\n        maven { url \"https://s01.oss.sonatype.org/service/local/repositories/snapshots/content/\" }\n        mavenCentral()\n    }\n}\n\nplugins {\n  id \"de.undercouch.download\" version \"4.1.2\"\n  id \"ch.so.agi.gretl\" version \"2.3.426\"\n}\n\ndefaultTasks 'validateData'\n\ntasks.register('downloadFile', Download) {\n    src \"https://files.geo.so.ch/ch.so.agi.av.dm01_ch/aktuell/2549.ch.so.agi.av.dm01_ch.itf.zip\"\n    dest file(\"2549.ch.so.agi.av.dm01_ch.itf.zip\")\n    overwrite true\n}\n\ntasks.register('unzipFile', Copy) {\n    dependsOn 'downloadFile'\n    from zipTree(Paths.get(\"2549.ch.so.agi.av.dm01_ch.itf.zip\"))\n    into file(\".\")\n    include \"**/*.itf\"\n}\n\ntasks.register('validateData', IliValidator) {\n    dependsOn 'unzipFile'\n    dataFiles = [\"2549.ch.so.agi.av.dm01_ch.itf\"]\n}\nTo run your first GRETL job just type gradle in the terminal - in the same directory where you saved the build.gradle file - and hit enter. On the first run, it will download a lot of dependencies (external libraries). Be patient and after downloading everything it will actually run the job and should finish with BUILD SUCCESSFUL.\nIf you want some more logging output, use gradle -i and you should see e.g. the well known output from ilivalidator."
  },
  {
    "objectID": "getting_started.html#docker-flavour",
    "href": "getting_started.html#docker-flavour",
    "title": "Get Started",
    "section": "Docker flavour",
    "text": "Docker flavour\nYou can build your very own image or just use docker.io/sogis/gretl:latest. Since the image makes use of an init.gradle file, the build.gradle file looks a little bit different:\nimport java.nio.file.Paths\nimport ch.so.agi.gretl.tasks.*\nimport ch.so.agi.gretl.api.*\nimport de.undercouch.gradle.tasks.download.Download\n\napply plugin: 'ch.so.agi.gretl'\napply plugin: 'de.undercouch.download'\n\ndefaultTasks 'validateData'\n\ntasks.register('downloadFile', Download) {\n    src \"https://files.geo.so.ch/ch.so.agi.av.dm01_ch/aktuell/2549.ch.so.agi.av.dm01_ch.itf.zip\"\n    dest file(\"2549.ch.so.agi.av.dm01_ch.itf.zip\")\n    overwrite true\n}\n\ntasks.register('unzipFile', Copy) {\n    dependsOn 'downloadFile'\n    from zipTree(Paths.get(\"2549.ch.so.agi.av.dm01_ch.itf.zip\"))\n    into file(\".\")\n    include \"**/*.itf\"\n}\n\ntasks.register('validateData', IliValidator) {\n    dependsOn 'unzipFile'\n    dataFiles = [\"2549.ch.so.agi.av.dm01_ch.itf\"]\n}\nRun the following Docker command in the directory where the build.gradle file is stored:\ndocker run -i --rm --name gretl --entrypoint=\"/bin/sh\" -v $PWD:/home/gradle/project sogis/gretl:latest -c 'gretl'\nThe most important part is the bind mount of the working directory ($PWD) to /home/gradle/project/ in the image. The image expects the build.gradle file in this specific directory.\nIf you want it a less verbose - especially if the run command gets more and more complex by adding more options - you can use e.g. a shell script."
  },
  {
    "objectID": "publisher.html",
    "href": "publisher.html",
    "title": "Publisher",
    "section": "",
    "text": "GRETL-Task, welcher für Vektordaten die aktuellen Geodaten-Dateien bereitstellt und das Archiv der vorherigen Zeitstände pflegt."
  },
  {
    "objectID": "publisher.html#einbindung-in-einen-typischen-gretl-publikationsjob",
    "href": "publisher.html#einbindung-in-einen-typischen-gretl-publikationsjob",
    "title": "Publisher",
    "section": "Einbindung in einen typischen GRETL-Publikationsjob",
    "text": "Einbindung in einen typischen GRETL-Publikationsjob\nIn den heute vorliegenden Publikationsjobs werden häufig die Daten vom relational aufgebauten Edit-Schema mittels SQL-Queries “flachgewalzt” und ins Pub-Schema kopiert. Die Schema-Struktur wird automatisch mittels ili2pg aus Edit- und Pub-Modell generiert.\nFür den Datenbezug wird das build.gradle mit zwei Aufrufen des Publisher-Task ergänzt:\ndefaultTasks 'pubProduct'\n\ntask pubEdit(type: Publisher){\n    dataIdent = \"ch.so.avt.verkehrszaehlstellen.edit\"\n    ...\n}\n\ntask transferVZS(type: Db2Db, dependsOn: pubEdit){\n    ...\n}\n\ntask pubProduct(type: Publisher, dependsOn: transferDenkmal){\n    dataIdent = \"ch.so.avt.verkehrszaehlstellen\"\n    ...\n}\nBei Problemen mit der Datenqualität der Originaldaten schlägt der Task “pubEdit” fehl. Der Job bricht mit Fehler ab, bevor die Daten irgendwo landen."
  },
  {
    "objectID": "publisher.html#ablauf",
    "href": "publisher.html#ablauf",
    "title": "Publisher",
    "section": "Ablauf",
    "text": "Ablauf\nDer Publisher arbeitet die folgenden Hauptschritte ab:\n\nVerstecktes Verzeichnis für den Datenstand via FTPS erstellen (.yyyy-MM-dd-UUID/). Kein Abbruch, falls das Verzeichnis vorhanden ist.\nXTFs in Verzeichnis ablegen.\n\nFür Datenthemen mit Quelle=Datenbank: XTF-Transferdateien exportieren.\n\nMit ili2pg das xtf erzeugen\nPrüfung des xtf gegen das Modell. Abbruch bei fatalen Fehlern\nPrüf-Bericht (und evtl. Prüf-Konfiguartion) muss auch mit in die ZIP Datei\nZIP-Datei publizieren\n\nFür Datenthemen mit Quelle=XTF: XTF in Verzeichnis kopieren.\n\nPrüfung des xtf gegen das Modell. Abbruch bei fatalen Fehlern\nPrüf-Bericht (und evtl. Prüf-Konfiguartion) muss auch mit in die ZIP Datei\nZIP-Datei publizieren\n\n\n\nAus dem Publikations-xtf die Benutzerformate (Geopackage, Shapefile, Dxf) ableiten und ablegen.\nMetadaten sammeln und im Unterordner meta/ ablegen.\n\nPublikationsdatum\nili-Dateien\nBeipackzettel (HTML via REST-API vom SIMI-Service beziehen)\n\nNeue Ordnernamen setzen.\n\n“aktuell” umbenennen auf Ordnername gemäss Datum in publishdate.json und verschieben in “hist”.\nVerstecktes Verzeichnis umbenennen auf aktuell.\nBenutzerformate in “hist” löschen\n\nPublikationsdatum via REST-API in den KGDI-Metadaten nachführen\nHistorische Stände ausdünnen."
  },
  {
    "objectID": "publisher.html#ordnerstruktur-im-ziel-verzeichnis",
    "href": "publisher.html#ordnerstruktur-im-ziel-verzeichnis",
    "title": "Publisher",
    "section": "Ordnerstruktur im Ziel-Verzeichnis",
    "text": "Ordnerstruktur im Ziel-Verzeichnis\n\nGängiger Fall: Zwei Modelle, keine Regionen\nPublikation in den beiden Datenbereitstellungen ch.so.avt.verkehrszaehlstellen und ch.so.avt.verkehrszaehlstellen.edit\nNamenskonvention für die Dateien: [Datenbereitstellungs-Identifier].[Format-Identifier].zip\ndata-Verzeichnis:\n\n\nch.so.avt.verkehrszaehlstellen/\n\naktuell/\n\nch.so.avt.verkehrszaehlstellen.dxf.zip\n\nTabelle1.dxf\nTabelle2.dxf\n….\nvalidation.log\nvalidation.ini\n\nch.so.avt.verkehrszaehlstellen.gpkg.zip\n\nch.so.avt.verkehrszaehlstellen.gpkg\nvalidation.log\nvalidation.ini\n\nch.so.avt.verkehrszaehlstellen.shp.zip\n\nTabelle1.prj\nTabelle1.shp\nTabelle1.shx\nTabelle2.dbf\nTabelle2.prj\nTabelle2.shp\nTabelle2.shx\n….\nvalidation.log\nvalidation.ini\n\nch.so.avt.verkehrszaehlstellen.xtf.zip\n\nch.so.avt.verkehrszaehlstellen.xtf\nvalidation.log\nvalidation.ini\n\nmeta/\n\nSO_AVT_Verkehrszaehlstellen_Publikation_20190206.ili\npublishdate.json\n\ndatenbeschreibung.html\n\n\nhist/\n\n2021-04-12/ – intern identisch aufgebaut wie Ordner aktuell/ aber ohne Benutzerformate\n\nch.so.avt.verkehrszaehlstellen.xtf.zip\n\nch.so.avt.verkehrszaehlstellen.xtf\nvalidation.log\nvalidation.ini\n\nmeta/\n\nSO_AVT_Verkehrszaehlstellen_Publikation_20190206.ili\npublishdate.json\n\ndatenbeschreibung.html\n\n\n2021-03-14/\n…\n\n\nch.so.avt.verkehrszaehlstellen.edit/\n\naktuell/\n\nch.so.avt.verkehrszaehlstellen.edit.xtf.zip\n\nch.so.avt.verkehrszaehlstellen.edit.xtf\nvalidation.log\nvalidation.ini\n\nmeta/\n\nSO_AVT_Verkehrszaehlstellen_20190206.ili\n\npublishdate.json\n\ndatenbeschreibung.html\n\n\nhist/\n\n…\n\n\n\n\n\n\nAbbildung von Regionen-Einteilungen\nDie Regionen werden als Präfix der Dateien abgebildet. Die Ordnerstruktur bleibt gleich. Aufbau Dateiname:\n[Regionen-Identifier].[Datenbereitstellungs-Identifier].[Format-Identifier].zip\nBeispiel AV (Regionen-Identifier ist die BFS-NR):\ndata-Verzeichnis:\n\n\nch.so.agi.av.mopublic/\n\naktuell/\n\n2501.ch.so.agi.av.mopublic.dxf.zip\n2501.ch.so.agi.av.mopublic.gpkg.zip\n2501.ch.so.agi.av.mopublic.shp.zip\n2501.ch.so.agi.av.mopublic.xtf.zip\n2502.ch.so.agi.av.mopublic.dxf.zip\n2502.ch.so.agi.av.mopublic.gpkg.zip\n2502.ch.so.agi.av.mopublic.shp.zip\n2502.ch.so.agi.av.mopublic.xtf.zip\n…\nmeta/\n\n…\n\n\n\nhist/\n\n…"
  },
  {
    "objectID": "publisher.html#xtf---xtf",
    "href": "publisher.html#xtf---xtf",
    "title": "Publisher",
    "section": "XTF -> XTF",
    "text": "XTF -&gt; XTF\nFalls die Daten bereits als XTF-/ITF-Datei vorliegen, muss die Quelldatei angegeben werden.\ntask publishFile(type: Publisher){\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    sourcePath = file(\"/path/file.xtf\")\n}\nDie Daten können alternativ zu SFTP in ein lokales Verzeichnis publiziert werden:\ntask publishFile(type: Publisher){\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [file(\"/out\")]  \n    sourcePath = file(\"/path/file.xtf\")\n}"
  },
  {
    "objectID": "publisher.html#db---xtf",
    "href": "publisher.html#db---xtf",
    "title": "Publisher",
    "section": "DB -> XTF",
    "text": "DB -&gt; XTF\nFalls die Daten in einer ili2db konformen PostgreSQL Datenbank vorliegen, muss die Datenbank angegeben werden und welche Daten (Parameter dataset, modelsToPublish, regions, region) aus der Datenbank exportiert werden sollen.\ntask publishFromDb(type: Publisher){\n    dataIdent = \"ch.so.agi.vermessung\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    database = [\"uri\",\"user\",\"password\"]\n    dbSchema \"av\"\n    dataset = \"dataset\"\n}\nNur bei einfachen Modellen (falls das DB Schema ohne createBasketCol erstellt worden ist), kann der Export alternativ mit dem Parameter modelsToPublish mit Angabe des INTERLIS-Modellnamens erfolgen:\ntask publishFromDb(type: Publisher){\n    dataIdent = \"ch.so.agi.vermessung\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    database = [\"uri\",\"user\",\"password\"]\n    dbSchema \"av\"\n    modelsToPublish = \"DM01AVCH24LV95D\"\n}"
  },
  {
    "objectID": "publisher.html#regionen",
    "href": "publisher.html#regionen",
    "title": "Publisher",
    "section": "Regionen",
    "text": "Regionen\nFalls die Daten bereits als XTF-/ITF-Dateien vorliegen, muss zusätzlich zu einer möglichen Quelldatei (sourcePath) das Dateinamens-Muster (ohne Nameserweiterung (.xtf oder .itf) der Regionen (region) angegeben werden.\ntask publishFile(type: Publisher){\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    sourcePath = file(\"/transferfiles/file.xtf\")\n    region = \"[0-9][0-9][0-9][0-9]\"  // regex; ersetzt den filename im sourcePath\n}\nDer sourcePath ist wie bei der Verarbeitung eines XTF (ohne Regionen) ein Datei- und nicht ein Ordner-Pfad. Mittels Parameter “region” werden aus allem im Ordner “transferfiles” enthaltenen Transfer-Dateien die zu verarbeitenden selektiert.\nFalls die Daten in einer ili2db konformen PostgreSQL Datenbank vorliegen, muss das Muster der Datensatz-Namen (dataset) angegeben werden (= ein Datensatz pro Region (region)).\ntask publishFromDb(type: Publisher){\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    database = [\"uri\",\"user\",\"password\"]\n    dbSchema \"av\"\n    region = \"[0-9][0-9][0-9][0-9]\"  // regex; ersetzt das dataset\n}\nVerarbeitet werden alle Datensätze, deren Dateiname (Quelle Transferdatei) oder dataset Name (Quelle DB) auf das in Parameter “region” definierte Muster (regular Expression) zutreffen.\n\nBeispiele für die Verwendung von Regionen\nEs können eindeutige Namen oder auch regular expressions verwendet werden.\nExport mit region aus lokaler DB (Nutzungsplanung mit 3 Datasets: 2580, 2581, 2582)\ntask publishFromDb2(type: Publisher){\n    dataIdent = \"ch.so.arp.nutzungsplanung.publishFromDb2\"\n    database = [dbUriPub, dbUserPub, dbPwdPub]\n    dbSchema = \"arp_nutzungsplanung_pub_v1\"\n    target = [project.buildDir]\n\n    region = \"[2][5][8][0]\"         //exportiert Dataset 2580\n    region = \"2580\"                 //exportiert Dataset 2580\n    region = 2580                   //exportiert Dataset 2580\n    region = \"[0-9][0-9][0-9][0-9]\" //exportiert alle 3 Datasets\n    region = \".*\"                   //exportiert alle 3 Datasets\n    userFormats = true\n    kgdiService = [\"http://localhost:8080/app/rest\",\"admin\",\"admin\"]\n}\n4 xtf-Files: a2581.xtf, c2582.xtf, b2583.xtf, d2584.xtf, lokal im Job-Verzeichnis\ntask publishFile2(type: Publisher){\n    dataIdent = \"publishFile2\"\n    sourcePath = file(\"a2581.xtf\") // Angabe zum Ablageort eines der zu publizierenden Files\n    target = [project.buildDir]\n    \n    region = \"[a-d][0-9][0-9][0-9][0-9]\"  //alle 4 Files werden publiziert\n    region = \"[2][5][8][4]\"               //d2584.xtf wird publiziert\n    kgdiService = [\"http://localhost:8080/app/rest\",\"admin\",\"admin\"]\n}"
  },
  {
    "objectID": "publisher.html#verkettung-von-publishern",
    "href": "publisher.html#verkettung-von-publishern",
    "title": "Publisher",
    "section": "Verkettung von Publishern",
    "text": "Verkettung von Publishern\nDamit nachfolgende Tasks die Liste der tatsächlich publizierten Regionen auswerten können, kann der Parameter publishedRegions des Tasks Publisher verwendet werden.\ntask publishFile(type: Publisher){\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    sourcePath = file(\"/path/file.xtf\")\n    region = \"[0-9][0-9][0-9][0-9]\"\n}\n\ntask printPublishedRegions(dependsOn: publishFile){\n    doLast() {\n        println publishFile.publishedRegions\n    }\n}\nDer Publisher lässt sich somit auch über die zu publizierenden Regionen verketten.\ntask publishFile0(type: Publisher){\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [project.buildDir]\n    sourcePath = file(\"../../../../src/test/resources/data/publisher/files/av_test.itf\")\n    modeldir= file(\"../../../../src/test/resources/data/publisher/ili\")\n    region=\"[0-9][0-9][0-9][0-9]\"\n}\n\ntask publishFile1(type: Publisher){\n    dataIdent = \"ch.so.agi.vermessung.pub\"\n    target = [project.buildDir]\n    sourcePath = file(\"../../../../src/test/resources/data/publisher/files/av_test.itf\")\n    modeldir= file(\"../../../../src/test/resources/data/publisher/ili\")\n    regions=publishFile0.publishedRegions\n}\nEnthält das aktuelle Verzeichnis schon Daten (von Regionen), werden diese (wie sonst auch historisiert), und danach mit den neuen Regionen ergänzt. Der Parameter publishedRegions enthält nur die neu publizierten Regionen (und nicht alle publizierten Regionen). Auch an den KGDI-Service werden nur die neu publizierten Regionen notifiziert (und nicht alle publizierten Regionen). Die Dateien im meta Unterverzeichnis werden neu erstellt."
  },
  {
    "objectID": "publisher.html#validierung",
    "href": "publisher.html#validierung",
    "title": "Publisher",
    "section": "Validierung",
    "text": "Validierung\nDie Validierung kann mit einer ilivalidator Konfigurationsdatei konfiguriert werden.\ntask publishFile(type: Publisher){\n    ...\n    validationConfig =  \"validationConfig.ini\"\n}"
  },
  {
    "objectID": "publisher.html#benutzer-formate-gpkg-dxf-shp",
    "href": "publisher.html#benutzer-formate-gpkg-dxf-shp",
    "title": "Publisher",
    "section": "Benutzer-Formate (GPKG, DXF, SHP)",
    "text": "Benutzer-Formate (GPKG, DXF, SHP)\nOptional können Benutzerformate (Geopackage, Shapefile, Dxf) erstellt werden. Die Daten müssen in einer entsprechend flachen Struktur vorliegen.\ntask publishFromDb(type: Publisher){\n    dataIdent = \"ch.so.agi.vermessung\"\n    target = [ \"sftp://ftp.server.ch/data\", \"user\", \"password\" ]\n    database = [\"uri\",\"user\",\"password\"]\n    dbSchema \"av\"\n    dataset = \"dataset\"\n    userFormats = true\n}\n\ntask publishFromFile(type: Publisher){\n    dataIdent = \"ch.so.agi.vermessung.edit\"\n    target = [file(\"/out\")]  \n    sourcePath = file(\"/path/file.xtf\")\n    userFormats = true\n}\nFalls das Datenmodell der Quelldaten DM01AVCH24LV95D ist, wird das DXF automatisch in der Geobau-Struktur erstellt und kein Geopackage und kein Shapefile erstellt.\nDie Benutzerformate werden beim Verschieben in den Archiv-Ordner entfernt."
  },
  {
    "objectID": "publisher.html#kgdi-service",
    "href": "publisher.html#kgdi-service",
    "title": "Publisher",
    "section": "KGDI-Service",
    "text": "KGDI-Service\nDer Service wird benutzt, um:\n\nden Beipackzettel (im Unterordner meta) zu erstellen/beziehen\ndas Publikationsdatum in den Metadaten nachzuführen\n\ntask publishFile(type: Publisher){\n    ...\n    kgdiTokenService = [\"http://api.kgdi.ch/metadata\",\"superuser\",\"superpwd\"]\n    kgdiService = [\"http://api.kgdi.ch/metadata\",\"user\",\"pwd\"]\n}\n\nBeipackzettel beziehen\nHTTP GET: endpoint+\"/doc?dataident=\"+dataIdent+\"&published=\"+versionTag\n\n\nPublikationsdatum nachführen\nHTTP PUT: endpoint+\"/pubsignal\",request\nDer Request-Body “Ohne Regionen”:\n{\n    \"dataIdent\": \"ch.so.afu.gewaesserschutz\",\n    \"published\": \"2021-12-23T14:54:49.050062\",\n    \"publishedBaskets\": [{\n        \"model\": \"SO_AGI_MOpublic_20201009\",\n        \"topic\": \"Bodenbedeckung\",\n        \"basket\": \"oltenBID\"\n    }, {\n        \"model\": \"DM01\",\n        \"topic\": \"Liegenschaften\",\n        \"basket\": \"wangenBID\"\n    }]\n}\nDer Request-Body “Mit Regionen”:\n{\n    \"dataIdent\": \"ch.so.afu.gewaesserschutz\",\n    \"published\": \"2021-12-23T14:54:49.050062\",\n    \"publishedRegions\": [{\n        \"region\": \"olten\",\n        \"publishedBaskets\": [{\n            \"model\": \"SO_AGI_MOpublic_20201009\",\n            \"topic\": \"Bodenbedeckung\",\n            \"basket\": \"oltenBID\"\n        }]\n    }, {\n        \"region\": \"wangen\",\n        \"publishedBaskets\": [{\n            \"model\": \"SO_AGI_MOpublic_20201009\",\n            \"topic\": \"Bodenbedeckung\",\n            \"basket\": \"wangenBID\"\n        }]\n    }]\n}"
  },
  {
    "objectID": "publisher.html#archiv-aufräumen",
    "href": "publisher.html#archiv-aufräumen",
    "title": "Publisher",
    "section": "Archiv aufräumen",
    "text": "Archiv aufräumen\ntask publishFile(type: Publisher){\n    ...\n    grooming = \"grooming.json\"\n}\nIn der Datei grooming.json wird konfiguriert, wie ausgedünnt wird.\n{\n    \"grooming\": {\n    \"daily\": {\n        \"from\": 0,\n        \"to\": 1\n    },\n    \"weekly\": {\n        \"from\": 1,\n        \"to\": 4\n    },\n    \"monthly\": {\n        \"from\": 4,\n        \"to\": 52\n    },\n    \"yearly\": {\n        \"from\": 52,\n        \"to\": null\n    }\n    }\n}\nDie to Angabe muss mit der from Angabe der nächsthöheren Stufe identisch sein (also z.B daily.to=weekly.from). Alle from und to Angaben sind in Tagen. Einzelne Stufen können weggelassen werden. Bei der niedrigsten Stufe (i.d.R. daily) muss from=0 sein. Bei der höchsten Stufe (i.d.R. yearly) kann to definiert oder null sein. Falls to definiert ist, wird der älteste Stand beim Erreichen des to Alters gelöscht. Falls bei der höchsten Stufe to=null, wird der älteste Stand nicht gelöscht."
  },
  {
    "objectID": "publisher.html#parameter",
    "href": "publisher.html#parameter",
    "title": "Publisher",
    "section": "Parameter",
    "text": "Parameter\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\ndataIdent\nIdentifikator der Daten z.B. “ch.so.agi.vermessung.edit”\n\n\ntarget\nZielverzeichnis z.B. [ “sftp://ftp.server.ch/data”, “user”, “password” ] oder einfach ein Pfad [file(“/out”)]\n\n\nsourcePath\nQuelldatei z.B. file(“/path/file.xtf”)\n\n\ndatabase\nDatenbank mit Quelldaten z.B. [“uri”,“user”,“password”]. Alternative zu sourcePath\n\n\ndbSchema\nSchema in der Datenbank z.B. “av”\n\n\ndataset\nili2db-Datasetname der Quelldaten “dataset” (Das ili2db-Schema muss also immer mit –createBasketCol erstellt werden)\n\n\nmodelsToPublish\nInterlis-Modellnamen der Quelldaten in der DB (Nur für “einfache” Modelle, deren ili2db-Schema ohne –createBasketCol erstellt werden kann)\n\n\nregion\nMuster (Regular Expression) der Dateinamen oder Datasetnamen, falls die Publikation Regionen-weise erfolgt z.B. “[0-9][0-9][0-9][0-9]”. Alternative zum Parameter regions,Bei Quelle “Datei” ist die Angabe einer “stellvertretenden” Transferdatei mittels “sourcePath” zwingend. Bsp.: Bei sourcePath “file(”/transferfiles/dummy.xtf”)” werden alle im Ordner “transferfiles” enthaltenen Transferdateien mit dem Muster verglichen und bei “match” selektiert und verarbeitet.\n\n\nregions\nListe der zu publizierenden Regionen (Dateinamen oder Datasetnamen), falls die Publikation Regionen-weise erfolgen soll. Alternative zum Parameter region\n\n\npublishedRegions\nListe der effektiv publizierten Regionen\n\n\nvalidationConfig\nKonfiguration für die Validierung (eine ilivalidator-config-Datei) z.B. “validationConfig.ini”\n\n\nuserFormats\nBenutzerformat (Geopackage, Shapefile, Dxf) erstellen. Default ist false\n\n\nkgdiTokenService\nEndpunkt des Authentifizierung-Services, z.B. [“http://api.kgdi.ch/metadata”,“user”,“pwd”]. Publisher ergänzt die URL mit /v2/oauth/token.\n\n\nkgdiService\nEndpunkt des SIMI-Services für die Rückmeldung des Publikationsdatums und die Erstellung des Beipackzettels, z.B. [“http://api.kgdi.ch/metadata”,“user”,“pwd”]. Publisher ergänzt die URL fallabhängig mit /pubsignal respektive /doc.\n\n\ngrooming\nKonfiguration für die Ausdünnung z.B. “grooming.json”. Ohne Angabe wird nicht aufgeräumt.\n\n\nexportModels\nDas Export-Modell, indem die Daten exportiert werden. Der Parameter wird nur bei der Ausdünnung benötigt. Als Export-Modelle sind Basis-Modelle zulässig.\n\n\nmodeldir\nDateipfade, die Modell-Dateien (ili-Dateien) enthalten. Mehrere Pfade können durch Semikolon ‚;‘ getrennt werden. Es sind auch URLs von Modell-Repositories möglich. Default: %ITF_DIR;http://models.interlis.ch/. %ITF_DIR ist ein Platzhalter für das Verzeichnis mit der ITF-Datei.\n\n\nproxy\nProxy Server für den Zugriff auf Modell Repositories\n\n\nproxyPort\nProxy Port für den Zugriff auf Modell Repositories"
  },
  {
    "objectID": "deployment.html",
    "href": "deployment.html",
    "title": "Deployment",
    "section": "",
    "text": "GRETL kann unterschiedlich deployed werden. Die ETL-Prozesse können somit auf unterschiedlichsten Runtimes ausgeführt werden."
  },
  {
    "objectID": "deployment.html#lokal",
    "href": "deployment.html#lokal",
    "title": "Deployment",
    "section": "Lokal",
    "text": "Lokal\nSiehe Beispiele im “Get Started” Kapitel."
  },
  {
    "objectID": "deployment.html#jenkins",
    "href": "deployment.html#jenkins",
    "title": "Deployment",
    "section": "Jenkins",
    "text": "Jenkins"
  },
  {
    "objectID": "deployment.html#github-actions",
    "href": "deployment.html#github-actions",
    "title": "Deployment",
    "section": "Github Actions",
    "text": "Github Actions\nWill man einen GRETL-Job als Github Action ausführen, funktioniert folgendes:\nIn einem Github Repository erstellt man eine build.gradle-Datei.\nimport java.nio.file.Paths\nimport ch.so.agi.gretl.tasks.*\nimport ch.so.agi.gretl.api.*\nimport de.undercouch.gradle.tasks.download.Download\n\napply plugin: 'ch.so.agi.gretl'\napply plugin: 'de.undercouch.download'\n\ndefaultTasks 'validateData'\n\ntasks.register('downloadFile', Download) {\n    src \"https://files.geo.so.ch/ch.so.agi.av.dm01_ch/aktuell/2549.ch.so.agi.av.dm01_ch.itf.zip\"\n    dest file(\"2549.ch.so.agi.av.dm01_ch.itf.zip\")\n    overwrite true\n}\n\ntasks.register('unzipFile', Copy) {\n    dependsOn 'downloadFile'\n    from zipTree(Paths.get(\"2549.ch.so.agi.av.dm01_ch.itf.zip\"))\n    into file(\".\")\n    include \"**/*.itf\"\n}\n\ntasks.register('validateData', IliValidator) {\n    dependsOn 'unzipFile'\n    dataFiles = [\"2549.ch.so.agi.av.dm01_ch.itf\"]\n}\nEs muss ein Github Action Workflow erstellt werden:\nname: mein_erster_gretljob\n\non:\n  push\n\njobs:  \n\n  build:\n\n    runs-on: ubuntu-latest\n\n1    container:\n      image: sogis/gretl:latest\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Run GRETL job\n        run: |\n          gradle -b build.gradle --init-script /home/gradle/init.gradle --no-daemon\n\n1\n\nMit der Anweisung container wird der Action mitgeteilt, dass die folgenden Steps im GRETL Dockerimage ausgeführt werden.\n\n\nEine leicht komplizierter Workflow könnte so aussehen:\nname: mein_zweiter_gretljob\n\non:\n1  workflow_dispatch:\n2    inputs:\n      directory:\n        description: 'directory?'\n        required: false\n      fileName:\n        description: 'file name?'\n        required: true\n\njobs:  \n\n  build:\n\n3    env:\n      ORG_GRADLE_PROJECT_awsAccessKeyAgi: ${{secrets.AWS_ACCESS_KEY_ID}}\n      ORG_GRADLE_PROJECT_awsSecretAccessKeyAgi: ${{secrets.AWS_SECRET_ACCESS_KEY}}\n\n    runs-on: ubuntu-latest\n\n    container:\n      image: sogis/gretl:latest\n\n4    services:\n      postgis:\n        image: postgis/postgis:14-3.3\n        env:\n          POSTGRES_PASSWORD: gretl\n          POSTGRES_USER: gretl\n          POSTGRES_DB: edit\n        options: &gt;-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Run GRETL job\n        run: |\n          gradle -b build.gradle --init-script /home/gradle/init.gradle --no-daemon\n5        env:\n          ORG_GRADLE_PROJECT_directory: ${{ github.event.inputs.directory }}\n          ORG_GRADLE_PROJECT_fileName: ${{ github.event.inputs.fileName }}\n          ORG_GRADLE_PROJECT_dbUriEdit: jdbc:postgresql://postgis:5432/edit\n          ORG_GRADLE_PROJECT_dbUserEdit: gretl\n          ORG_GRADLE_PROJECT_dbPwdEdit: gretl\n\n6      - name: Upload results\n        uses: actions/upload-artifact@v3\n        with:\n          name: results\n          path: |\n            out-**.xtf\n            *.log\n\n1\n\nDie Action wird entweder manuell gestartet oder durch einen API-Call.\n\n2\n\nEs müssen zwei Input-Parameter übergeben werden: directory und fileName. Letzterer ist zwingend.\n\n3\n\nDer GRETL-Job kommunziert mit AWS-S3 und benötigt Credentials. Diese werden Gradle als ENV-Variablen bekannt gemacht.\n\n4\n\nEs wird eine PostGIS-Datenbank als Dockercontainer gestartet, die wir für den ETL-Prozess benötigen.\n\n5\n\nWeitere Umgebungsvariablen für den GRETL-Job.\n\n6\n\nDas Resultat des GRETL-Jobs (eine XTF-Datei) und Logfiles werden als Artefakt hochgeladen und stehen dem Benutzer zum Download zur Verfügung."
  },
  {
    "objectID": "deployment.html#gitlab-pipelines",
    "href": "deployment.html#gitlab-pipelines",
    "title": "Deployment",
    "section": "Gitlab Pipelines",
    "text": "Gitlab Pipelines"
  }
]